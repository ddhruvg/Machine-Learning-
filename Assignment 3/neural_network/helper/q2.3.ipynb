{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "756da69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30a4ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_data(type=\"train\"):\n",
    "    data = []\n",
    "    label = []\n",
    "    for i in range(1, 37):\n",
    "        X = []\n",
    "        y = []\n",
    "        target = f\"{i:02d}\"  \n",
    "        train_data_path = f\"../../data/q2/{type}/{target}\"\n",
    "        \n",
    "        for img_name in os.listdir(train_data_path):\n",
    "            img_path = os.path.join(train_data_path, img_name)\n",
    "            \n",
    "            \n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            \n",
    "            img = cv2.resize(img, (32, 32))\n",
    "            img = img / 255.0 \n",
    "            img_flatten = img.flatten()\n",
    "            X.append(img_flatten)\n",
    "            y.append(i)\n",
    "\n",
    "        data.extend(X)\n",
    "        label.extend(y)\n",
    "\n",
    "    data = np.array(data)\n",
    "    label = np.array(label)\n",
    "    return data, label\n",
    "\n",
    "X_train , y_train = get_data(\"train\")\n",
    "X_test , y_test = get_data(\"test\")\n",
    "y_train  = np.eye(36)[y_train - 1]\n",
    "y_test  = np.eye(36)[y_test - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5567e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self,in_dimension , out_dimension , learning_rate = 0.001 , is_sigmoid = True,is_output = False):\n",
    "        self.net_grad = None\n",
    "        self.lr = learning_rate\n",
    "        self.O = None\n",
    "        limit = np.sqrt(6 / (in_dimension + out_dimension))\n",
    "        self.w = np.random.uniform(-limit, limit, (in_dimension + 1, out_dimension))\n",
    "\n",
    "        self.output_function = self.__sigmoid if is_sigmoid else self.__softmax\n",
    "        self.is_output = is_output\n",
    "\n",
    "    def __sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __softmax(self,x):\n",
    "        exp_x = np.exp(x - np.max(x , axis =1 , keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis = 1 , keepdims=True)\n",
    "\n",
    "    def __sigmoid_grad(self,x):\n",
    "        return x * (1-x)    \n",
    "\n",
    "    def next(self,X ):\n",
    "        X = np.hstack([np.ones((X.shape[0] , 1)),X])\n",
    "        self.net_grad = np.dot(X, self.w)\n",
    "        self.O = self.output_function(self.net_grad)\n",
    "        return self.O\n",
    "\n",
    "    def _prev(self, dO , O_down):\n",
    "        if not self.is_output:\n",
    "            new_grad = dO * self.__sigmoid_grad(self.O)\n",
    "            dw = np.dot(np.hstack([np.ones((O_down.shape[0], 1)), O_down]).T , new_grad) / O_down.shape[0]\n",
    "            d_O_down = np.dot(new_grad , self.w.T)\n",
    "            dO_prev =   d_O_down[:,1:]\n",
    "            self.w = self.w - self.lr * dw\n",
    "            return  dO_prev\n",
    "        \n",
    "            \n",
    "    \n",
    "\n",
    "class FinalLayer :\n",
    "    def __init__(self,in_dim , out_dim , lr = 0.001 ,is_softmax = True) :\n",
    "        self.lr = lr\n",
    "        limit = np.sqrt(6 / (in_dim + out_dim))\n",
    "        self.w = np.random.uniform(-limit, limit, (in_dim + 1, out_dim))\n",
    "\n",
    "        self.net_grad = None\n",
    "        self.O = None\n",
    "        self.output_function = self.__softmax if is_softmax else self.__sigmoid\n",
    "\n",
    "    def __sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __softmax(self,x):\n",
    "        exp_x = np.exp(x - np.max(x , axis =1 , keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis = 1 , keepdims=True)\n",
    "\n",
    "    def __sigmoid_grad(self,x):\n",
    "        return x * (1-x)  \n",
    "\n",
    "    def next(self , X) :\n",
    "        X = np.hstack([np.ones((X.shape[0] , 1)),X ])    \n",
    "        self.net_grad = np.dot(X, self.w)\n",
    "        self.O = self.output_function(self.net_grad)\n",
    "        return self.O\n",
    "    \n",
    "    def _prev(self, y_true , O_down)  :\n",
    "        new_grad = self.O - y_true\n",
    "        dw = np.dot(np.hstack([np.ones((O_down.shape[0], 1)), O_down]).T , new_grad) / y_true.shape[0]\n",
    "        d_O_down = np.dot(new_grad , self.w.T)\n",
    "        dO_prev =   d_O_down[:,1:]\n",
    "        self.w = self.w - self.lr * dw\n",
    "        return dO_prev\n",
    "    \n",
    "class NeuralNetwork:\n",
    "    def __init__(self, M = 32 , n = 3072 , HiddenLayer = [512 , 265 ]  , target_class = 36 , lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.layers = []\n",
    "        self.Batch_size = M\n",
    "        prev_dimn = n\n",
    "        for h in HiddenLayer:\n",
    "            layer = Layer(prev_dimn , h , is_sigmoid = True , learning_rate = self.lr)\n",
    "            self.layers.append(layer)\n",
    "            prev_dimn = h\n",
    "        self.output_layer = FinalLayer(prev_dimn , target_class , is_softmax = True , lr = self.lr)\n",
    "\n",
    "    def next(self,X) : \n",
    "        O_down = X \n",
    "        for layer in self.layers :\n",
    "            O_down = layer.next(O_down)\n",
    "\n",
    "        predictions = self.output_layer.next(O_down)\n",
    "        return predictions\n",
    "    \n",
    "    def corss_entropy_loss(self, y_true , y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        return - np.sum(y_true * np.log(y_pred + 1e-10 )) / m\n",
    "     \n",
    "    def _prev(self,Y , X) : \n",
    "        O = X \n",
    "        stored_outputs = [X]\n",
    "        for layer in self.layers :\n",
    "            O = layer.next(O)\n",
    "            stored_outputs.append(O)\n",
    "        predictions = self.output_layer.next(O)\n",
    "\n",
    "       \n",
    "\n",
    "        do_down = self.output_layer._prev(Y , stored_outputs[-1])\n",
    "        for i in range(len(self.layers)-1 , -1 , -1):\n",
    "            do_down = self.layers[i]._prev(do_down , stored_outputs[i])\n",
    "\n",
    "        return predictions  \n",
    "\n",
    "    def predict(self,X) :\n",
    "        predictions = self.next(X)\n",
    "        return np.argmax(predictions , axis = 1)   \n",
    "    \n",
    "    def fit(self , X , Y , epochs = 10 ):\n",
    "        for epoch in range(epochs) : \n",
    "            comb = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[comb]\n",
    "            Y_shuffled = Y[comb]\n",
    "            J = 0\n",
    "\n",
    "            for i in range(0, X.shape[0] , self.Batch_size):\n",
    "                X_batch = X_shuffled[i : i + self.Batch_size]\n",
    "                Y_batch = Y_shuffled[i : i + self.Batch_size]\n",
    "                predictions = self._prev(Y_batch , X_batch)\n",
    "                loss = self.corss_entropy_loss(Y_batch , predictions)\n",
    "                J += loss\n",
    "           \n",
    "            print(f\"Epoch {epoch + 1} / {epochs} , Loss : {J}\")\n",
    "\n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea2c7d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 20 , Loss : 2169.539596130592\n",
      "Epoch 2 / 20 , Loss : 1709.0519445343086\n",
      "Epoch 3 / 20 , Loss : 1409.3300034000856\n",
      "Epoch 4 / 20 , Loss : 1227.7618031046363\n",
      "Epoch 5 / 20 , Loss : 1113.3505808659609\n",
      "Epoch 6 / 20 , Loss : 1035.6529618051557\n",
      "Epoch 7 / 20 , Loss : 979.3547647713399\n",
      "Epoch 8 / 20 , Loss : 936.8484610928751\n",
      "Epoch 9 / 20 , Loss : 902.8982841636717\n",
      "Epoch 10 / 20 , Loss : 875.4175325984816\n",
      "Epoch 11 / 20 , Loss : 852.1567037173903\n",
      "Epoch 12 / 20 , Loss : 831.8331640480155\n",
      "Epoch 13 / 20 , Loss : 814.5035089393419\n",
      "Epoch 14 / 20 , Loss : 798.9628084071877\n",
      "Epoch 15 / 20 , Loss : 784.8655204732895\n",
      "Epoch 16 / 20 , Loss : 771.3079723340414\n",
      "Epoch 17 / 20 , Loss : 759.6291988057023\n",
      "Epoch 18 / 20 , Loss : 748.6184164177948\n",
      "Epoch 19 / 20 , Loss : 738.2549367152521\n",
      "Epoch 20 / 20 , Loss : 728.6544648062104\n",
      "Train Accuracy:  72.42592592592592\n",
      "Test Accuracy:  69.5\n",
      "f1 Score for hidden layer size [512] :  0.6941844940930983\n",
      "Precision for hidden layer size [512] :  0.6941844940930983\n",
      "Recall for hidden layer size [512] :  0.6941844940930983\n",
      "---------------------------------------------------\n",
      "Train f1 Score for hidden layer size [512] :  0.7234131381196114\n",
      "Train Precision for hidden layer size [512] :  0.7234131381196114\n",
      "Train Recall for hidden layer size [512] :  0.7234131381196114\n",
      "===================================================\n",
      "Epoch 1 / 20 , Loss : 2407.3962507746473\n",
      "Epoch 2 / 20 , Loss : 2346.7443126036546\n",
      "Epoch 3 / 20 , Loss : 2271.626060353421\n",
      "Epoch 4 / 20 , Loss : 2163.504579286783\n",
      "Epoch 5 / 20 , Loss : 2025.0172775487617\n",
      "Epoch 6 / 20 , Loss : 1878.1945338869755\n",
      "Epoch 7 / 20 , Loss : 1738.1750805601555\n",
      "Epoch 8 / 20 , Loss : 1611.3378033165195\n",
      "Epoch 9 / 20 , Loss : 1498.0927543036357\n",
      "Epoch 10 / 20 , Loss : 1400.191817535482\n",
      "Epoch 11 / 20 , Loss : 1316.5288855403887\n",
      "Epoch 12 / 20 , Loss : 1246.5089463022377\n",
      "Epoch 13 / 20 , Loss : 1187.336805064543\n",
      "Epoch 14 / 20 , Loss : 1138.551067797703\n",
      "Epoch 15 / 20 , Loss : 1096.9577019995397\n",
      "Epoch 16 / 20 , Loss : 1062.1801426456288\n",
      "Epoch 17 / 20 , Loss : 1032.0710666948748\n",
      "Epoch 18 / 20 , Loss : 1006.1082247957496\n",
      "Epoch 19 / 20 , Loss : 982.9332184415105\n",
      "Epoch 20 / 20 , Loss : 962.11716767669\n",
      "Train Accuracy:  63.4537037037037\n",
      "Test Accuracy:  62.19444444444444\n",
      "f1 Score for hidden layer size [512, 256] :  0.6173552854721335\n",
      "Precision for hidden layer size [512, 256] :  0.6173552854721335\n",
      "Recall for hidden layer size [512, 256] :  0.6173552854721335\n",
      "---------------------------------------------------\n",
      "Train f1 Score for hidden layer size [512, 256] :  0.6297153056506983\n",
      "Train Precision for hidden layer size [512, 256] :  0.6297153056506983\n",
      "Train Recall for hidden layer size [512, 256] :  0.6297153056506983\n",
      "===================================================\n",
      "Epoch 1 / 20 , Loss : 2430.0458219839124\n",
      "Epoch 2 / 20 , Loss : 2416.37104309872\n",
      "Epoch 3 / 20 , Loss : 2412.8623676996644\n",
      "Epoch 4 / 20 , Loss : 2409.1488329720223\n",
      "Epoch 5 / 20 , Loss : 2404.5381580251374\n",
      "Epoch 6 / 20 , Loss : 2399.368626801169\n",
      "Epoch 7 / 20 , Loss : 2392.8372307823065\n",
      "Epoch 8 / 20 , Loss : 2384.642669086092\n",
      "Epoch 9 / 20 , Loss : 2373.471718742777\n",
      "Epoch 10 / 20 , Loss : 2357.9018098409592\n",
      "Epoch 11 / 20 , Loss : 2335.778859315839\n",
      "Epoch 12 / 20 , Loss : 2301.8087634924464\n",
      "Epoch 13 / 20 , Loss : 2252.098551534916\n",
      "Epoch 14 / 20 , Loss : 2180.4986538179587\n",
      "Epoch 15 / 20 , Loss : 2094.0175626386845\n",
      "Epoch 16 / 20 , Loss : 2007.238504590187\n",
      "Epoch 17 / 20 , Loss : 1930.5689104209162\n",
      "Epoch 18 / 20 , Loss : 1865.2665440079745\n",
      "Epoch 19 / 20 , Loss : 1806.2046553146836\n",
      "Epoch 20 / 20 , Loss : 1749.847886408411\n",
      "Train Accuracy:  32.958333333333336\n",
      "Test Accuracy:  31.796296296296294\n",
      "f1 Score for hidden layer size [512, 256, 128] :  0.2733815676824903\n",
      "Precision for hidden layer size [512, 256, 128] :  0.2733815676824903\n",
      "Recall for hidden layer size [512, 256, 128] :  0.2733815676824903\n",
      "---------------------------------------------------\n",
      "Train f1 Score for hidden layer size [512, 256, 128] :  0.28735206793443874\n",
      "Train Precision for hidden layer size [512, 256, 128] :  0.28735206793443874\n",
      "Train Recall for hidden layer size [512, 256, 128] :  0.28735206793443874\n",
      "===================================================\n",
      "Epoch 1 / 20 , Loss : 2434.3068843468227\n",
      "Epoch 2 / 20 , Loss : 2420.0879087384815\n",
      "Epoch 3 / 20 , Loss : 2419.847235692459\n",
      "Epoch 4 / 20 , Loss : 2419.783261031576\n",
      "Epoch 5 / 20 , Loss : 2419.571753007323\n",
      "Epoch 6 / 20 , Loss : 2419.326983786481\n",
      "Epoch 7 / 20 , Loss : 2419.0290534777228\n",
      "Epoch 8 / 20 , Loss : 2419.017004226152\n",
      "Epoch 9 / 20 , Loss : 2418.667446965683\n",
      "Epoch 10 / 20 , Loss : 2418.624407841909\n",
      "Epoch 11 / 20 , Loss : 2418.319631315941\n",
      "Epoch 12 / 20 , Loss : 2417.9517898008316\n",
      "Epoch 13 / 20 , Loss : 2417.598555627367\n",
      "Epoch 14 / 20 , Loss : 2417.501218553921\n",
      "Epoch 15 / 20 , Loss : 2417.242066789659\n",
      "Epoch 16 / 20 , Loss : 2416.6505035606374\n",
      "Epoch 17 / 20 , Loss : 2416.7089506016614\n",
      "Epoch 18 / 20 , Loss : 2416.309858144469\n",
      "Epoch 19 / 20 , Loss : 2415.928874071174\n",
      "Epoch 20 / 20 , Loss : 2415.4244986084536\n",
      "Train Accuracy:  4.152777777777778\n",
      "Test Accuracy:  3.8796296296296293\n",
      "f1 Score for hidden layer size [512, 256, 128, 64] :  0.006717833117545087\n",
      "Precision for hidden layer size [512, 256, 128, 64] :  0.006717833117545087\n",
      "Recall for hidden layer size [512, 256, 128, 64] :  0.006717833117545087\n",
      "---------------------------------------------------\n",
      "Train f1 Score for hidden layer size [512, 256, 128, 64] :  0.008253521710266122\n",
      "Train Precision for hidden layer size [512, 256, 128, 64] :  0.008253521710266122\n",
      "Train Recall for hidden layer size [512, 256, 128, 64] :  0.008253521710266122\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_length = [[512],[512,256],[512,256,128],[512,256,128,64],]\n",
    "\n",
    "for h_len in hidden_layer_length:\n",
    "    model = NeuralNetwork(M = 32 , n = 3072 , HiddenLayer = h_len  , target_class = 36 , lr = 0.01) \n",
    "    model.fit(X_train , y_train , epochs = 20)\n",
    "    y_test_prediction = model.predict(X_test)\n",
    "    y_train_prediction = model.predict(X_train)\n",
    "    print(\"Train Accuracy: \", np.mean(y_train_prediction == np.argmax(y_train , axis = 1)) * 100)\n",
    "    print(\"Test Accuracy: \", np.mean(y_test_prediction == np.argmax(y_test , axis = 1)) * 100)\n",
    "    \n",
    "    def results_metrics(y_true, y_pred):\n",
    "        f1 = f1_score(np.argmax(y_true , axis = 1) , y_pred , average='macro')\n",
    "        precision = f1_score(np.argmax(y_true , axis = 1) , y_pred , average='macro', zero_division=0)\n",
    "        recall = f1_score(np.argmax(y_true , axis = 1) , y_pred , average='macro', zero_division=0)\n",
    "        return f1, precision, recall\n",
    "\n",
    "    f1, precision, recall = results_metrics(y_test, y_test_prediction)\n",
    "    print(f\"f1 Score for hidden layer size {h_len} : \", f1)\n",
    "    print(f\"Precision for hidden layer size {h_len} : \", precision)\n",
    "    print(f\"Recall for hidden layer size {h_len} : \", recall)\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "    f1, precision, recall = results_metrics(y_train, y_train_prediction)\n",
    "    print(f\"Train f1 Score for hidden layer size {h_len} : \", f1)\n",
    "    print(f\"Train Precision for hidden layer size {h_len} : \", precision)\n",
    "    print(f\"Train Recall for hidden layer size {h_len} : \", recall)\n",
    "    print(\"===================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824dcf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "104d8d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Hidden layer architecture: [512] ===\n",
      "Epoch 1 / 20 , Loss : 2164.1136174359053\n",
      "Epoch 2 / 20 , Loss : 1711.9924094252192\n",
      "Epoch 3 / 20 , Loss : 1414.111750329867\n",
      "Epoch 4 / 20 , Loss : 1231.2960954362711\n",
      "Epoch 5 / 20 , Loss : 1115.8349969274125\n",
      "Epoch 6 / 20 , Loss : 1037.164340707464\n",
      "Epoch 7 / 20 , Loss : 980.6332803195786\n",
      "Epoch 8 / 20 , Loss : 937.7839732793944\n",
      "Epoch 9 / 20 , Loss : 903.459233938744\n",
      "Epoch 10 / 20 , Loss : 875.709899595438\n",
      "Epoch 11 / 20 , Loss : 852.3532527347066\n",
      "Epoch 12 / 20 , Loss : 832.2435396461796\n",
      "Epoch 13 / 20 , Loss : 814.693108325888\n",
      "Epoch 14 / 20 , Loss : 799.1164898660185\n",
      "Epoch 15 / 20 , Loss : 785.2315817981989\n",
      "Epoch 16 / 20 , Loss : 771.8034040728877\n",
      "Epoch 17 / 20 , Loss : 760.2397868031684\n",
      "Epoch 18 / 20 , Loss : 748.9250420419403\n",
      "Epoch 19 / 20 , Loss : 738.8023382491006\n",
      "Epoch 20 / 20 , Loss : 728.6686700413475\n",
      "\n",
      "Accuracy:\n",
      "Train Accuracy: 72.20%\n",
      "Test Accuracy: 69.52%\n",
      "\n",
      "Per-Class Metrics (Test):\n",
      " Class     F1  Precision  Recall\n",
      "     1 0.7809     0.7720  0.7900\n",
      "     2 0.6244     0.6431  0.6067\n",
      "     3 0.7119     0.7241  0.7000\n",
      "     4 0.5850     0.5737  0.5967\n",
      "     5 0.6298     0.6547  0.6067\n",
      "     6 0.7427     0.7261  0.7600\n",
      "     7 0.6387     0.6703  0.6100\n",
      "     8 0.7869     0.7742  0.8000\n",
      "     9 0.8446     0.8562  0.8333\n",
      "    10 0.8345     0.8459  0.8233\n",
      "    11 0.8754     0.8613  0.8900\n",
      "    12 0.7961     0.7857  0.8067\n",
      "    13 0.6818     0.7169  0.6500\n",
      "    14 0.7929     0.7704  0.8167\n",
      "    15 0.7164     0.6972  0.7367\n",
      "    16 0.7346     0.6839  0.7933\n",
      "    17 0.6072     0.6254  0.5900\n",
      "    18 0.6775     0.7373  0.6267\n",
      "    19 0.6769     0.6910  0.6633\n",
      "    20 0.6586     0.6821  0.6367\n",
      "    21 0.6093     0.5162  0.7433\n",
      "    22 0.8114     0.8197  0.8033\n",
      "    23 0.6629     0.7675  0.5833\n",
      "    24 0.7138     0.7594  0.6733\n",
      "    25 0.6149     0.6098  0.6200\n",
      "    26 0.4708     0.5411  0.4167\n",
      "    27 0.7516     0.7281  0.7767\n",
      "    28 0.7980     0.8131  0.7833\n",
      "    29 0.5244     0.5731  0.4833\n",
      "    30 0.6370     0.5733  0.7167\n",
      "    31 0.6965     0.6476  0.7533\n",
      "    32 0.5377     0.5528  0.5233\n",
      "    33 0.6634     0.6506  0.6767\n",
      "    34 0.7367     0.6953  0.7833\n",
      "    35 0.6273     0.6348  0.6200\n",
      "    36 0.7321     0.7309  0.7333\n",
      "\n",
      "Average (Macro) Metrics (Test):\n",
      "F1: 0.6940, Precision: 0.6974, Recall: 0.6952\n",
      "---------------------------------------------------\n",
      "\n",
      "Per-Class Metrics (Train):\n",
      " Class     F1  Precision  Recall\n",
      "     1 0.8033     0.7967  0.8100\n",
      "     2 0.6386     0.6601  0.6183\n",
      "     3 0.7523     0.7633  0.7417\n",
      "     4 0.6384     0.6489  0.6283\n",
      "     5 0.6517     0.7084  0.6033\n",
      "     6 0.7631     0.7484  0.7783\n",
      "     7 0.7153     0.7367  0.6950\n",
      "     8 0.8212     0.8311  0.8117\n",
      "     9 0.8684     0.8912  0.8467\n",
      "    10 0.8063     0.8043  0.8083\n",
      "    11 0.8430     0.8192  0.8683\n",
      "    12 0.8133     0.8133  0.8133\n",
      "    13 0.7094     0.7088  0.7100\n",
      "    14 0.8299     0.8185  0.8417\n",
      "    15 0.7214     0.6978  0.7467\n",
      "    16 0.7406     0.7147  0.7683\n",
      "    17 0.6328     0.6536  0.6133\n",
      "    18 0.6937     0.7373  0.6550\n",
      "    19 0.7162     0.7260  0.7067\n",
      "    20 0.7379     0.7391  0.7367\n",
      "    21 0.6338     0.5330  0.7817\n",
      "    22 0.8330     0.8519  0.8150\n",
      "    23 0.6856     0.7908  0.6050\n",
      "    24 0.7320     0.7553  0.7100\n",
      "    25 0.6405     0.6282  0.6533\n",
      "    26 0.4930     0.5605  0.4400\n",
      "    27 0.7964     0.7727  0.8217\n",
      "    28 0.8302     0.8254  0.8350\n",
      "    29 0.5746     0.5919  0.5583\n",
      "    30 0.6755     0.6047  0.7650\n",
      "    31 0.7119     0.6830  0.7433\n",
      "    32 0.5819     0.6240  0.5450\n",
      "    33 0.6699     0.6540  0.6867\n",
      "    34 0.7700     0.7540  0.7867\n",
      "    35 0.6644     0.6776  0.6517\n",
      "    36 0.7705     0.7504  0.7917\n",
      "\n",
      "Average (Macro) Metrics (Train):\n",
      "F1: 0.7211, Precision: 0.7243, Recall: 0.7220\n",
      "===================================================\n",
      "\n",
      "=== Hidden layer architecture: [512, 256] ===\n",
      "Epoch 1 / 20 , Loss : 2407.2627644430163\n",
      "Epoch 2 / 20 , Loss : 2344.263151359034\n",
      "Epoch 3 / 20 , Loss : 2267.5266967479556\n",
      "Epoch 4 / 20 , Loss : 2158.2506104240542\n",
      "Epoch 5 / 20 , Loss : 2018.2902162096711\n",
      "Epoch 6 / 20 , Loss : 1867.7113239982111\n",
      "Epoch 7 / 20 , Loss : 1725.4195628232856\n",
      "Epoch 8 / 20 , Loss : 1596.5907845176785\n",
      "Epoch 9 / 20 , Loss : 1485.0027423922943\n",
      "Epoch 10 / 20 , Loss : 1387.9333430019753\n",
      "Epoch 11 / 20 , Loss : 1307.0462802040652\n",
      "Epoch 12 / 20 , Loss : 1238.4155016780244\n",
      "Epoch 13 / 20 , Loss : 1181.1965087011436\n",
      "Epoch 14 / 20 , Loss : 1132.6766703254068\n",
      "Epoch 15 / 20 , Loss : 1091.4192824861354\n",
      "Epoch 16 / 20 , Loss : 1056.2754654366581\n",
      "Epoch 17 / 20 , Loss : 1025.7826404930502\n",
      "Epoch 18 / 20 , Loss : 999.3803546318878\n",
      "Epoch 19 / 20 , Loss : 975.8822895035622\n",
      "Epoch 20 / 20 , Loss : 955.1966201961659\n",
      "\n",
      "Accuracy:\n",
      "Train Accuracy: 63.83%\n",
      "Test Accuracy: 62.64%\n",
      "\n",
      "Per-Class Metrics (Test):\n",
      " Class     F1  Precision  Recall\n",
      "     1 0.7198     0.6657  0.7833\n",
      "     2 0.5226     0.6828  0.4233\n",
      "     3 0.6774     0.6533  0.7033\n",
      "     4 0.5291     0.4887  0.5767\n",
      "     5 0.5675     0.5825  0.5533\n",
      "     6 0.7030     0.6780  0.7300\n",
      "     7 0.5473     0.5658  0.5300\n",
      "     8 0.7298     0.6831  0.7833\n",
      "     9 0.7776     0.7289  0.8333\n",
      "    10 0.7783     0.7462  0.8133\n",
      "    11 0.7922     0.7722  0.8133\n",
      "    12 0.7422     0.7314  0.7533\n",
      "    13 0.5521     0.6560  0.4767\n",
      "    14 0.6927     0.6294  0.7700\n",
      "    15 0.6431     0.6497  0.6367\n",
      "    16 0.6686     0.5888  0.7733\n",
      "    17 0.5073     0.4952  0.5200\n",
      "    18 0.5476     0.6558  0.4700\n",
      "    19 0.6284     0.6370  0.6200\n",
      "    20 0.6200     0.6531  0.5900\n",
      "    21 0.5705     0.5677  0.5733\n",
      "    22 0.7531     0.7088  0.8033\n",
      "    23 0.5974     0.6586  0.5467\n",
      "    24 0.6620     0.6934  0.6333\n",
      "    25 0.4806     0.5741  0.4133\n",
      "    26 0.3553     0.4428  0.2967\n",
      "    27 0.7044     0.6834  0.7267\n",
      "    28 0.7161     0.7826  0.6600\n",
      "    29 0.4247     0.4366  0.4133\n",
      "    30 0.6362     0.6230  0.6500\n",
      "    31 0.6516     0.5665  0.7667\n",
      "    32 0.4348     0.5022  0.3833\n",
      "    33 0.5487     0.4986  0.6100\n",
      "    34 0.6425     0.5868  0.7100\n",
      "    35 0.5650     0.5884  0.5433\n",
      "    36 0.6711     0.6757  0.6667\n",
      "\n",
      "Average (Macro) Metrics (Test):\n",
      "F1: 0.6211, Precision: 0.6259, Recall: 0.6264\n",
      "---------------------------------------------------\n",
      "\n",
      "Per-Class Metrics (Train):\n",
      " Class     F1  Precision  Recall\n",
      "     1 0.7529     0.7072  0.8050\n",
      "     2 0.5058     0.6928  0.3983\n",
      "     3 0.6631     0.6169  0.7167\n",
      "     4 0.5113     0.4789  0.5483\n",
      "     5 0.5400     0.5577  0.5233\n",
      "     6 0.7067     0.6722  0.7450\n",
      "     7 0.6050     0.6226  0.5883\n",
      "     8 0.7621     0.7406  0.7850\n",
      "     9 0.7895     0.7542  0.8283\n",
      "    10 0.7443     0.7010  0.7933\n",
      "    11 0.7814     0.7519  0.8133\n",
      "    12 0.7526     0.7330  0.7733\n",
      "    13 0.6072     0.6776  0.5500\n",
      "    14 0.7153     0.6756  0.7600\n",
      "    15 0.6420     0.6341  0.6500\n",
      "    16 0.6315     0.5763  0.6983\n",
      "    17 0.4918     0.4839  0.5000\n",
      "    18 0.5453     0.6461  0.4717\n",
      "    19 0.6293     0.6408  0.6183\n",
      "    20 0.6810     0.6771  0.6850\n",
      "    21 0.5740     0.5603  0.5883\n",
      "    22 0.7697     0.7239  0.8217\n",
      "    23 0.6249     0.6750  0.5817\n",
      "    24 0.6509     0.6870  0.6183\n",
      "    25 0.5326     0.6144  0.4700\n",
      "    26 0.3067     0.4055  0.2467\n",
      "    27 0.7427     0.7034  0.7867\n",
      "    28 0.7546     0.7866  0.7250\n",
      "    29 0.4812     0.4727  0.4900\n",
      "    30 0.6129     0.5997  0.6267\n",
      "    31 0.6553     0.5891  0.7383\n",
      "    32 0.4701     0.5571  0.4067\n",
      "    33 0.5732     0.5406  0.6100\n",
      "    34 0.6801     0.6340  0.7333\n",
      "    35 0.5803     0.6205  0.5450\n",
      "    36 0.7168     0.6965  0.7383\n",
      "\n",
      "Average (Macro) Metrics (Train):\n",
      "F1: 0.6329, Precision: 0.6363, Recall: 0.6383\n",
      "===================================================\n",
      "\n",
      "=== Hidden layer architecture: [512, 256, 128] ===\n",
      "Epoch 1 / 20 , Loss : 2429.3070539447444\n",
      "Epoch 2 / 20 , Loss : 2417.0365911092545\n",
      "Epoch 3 / 20 , Loss : 2413.2436284842697\n",
      "Epoch 4 / 20 , Loss : 2409.40375221507\n",
      "Epoch 5 / 20 , Loss : 2405.1083016031603\n",
      "Epoch 6 / 20 , Loss : 2399.6560970245646\n",
      "Epoch 7 / 20 , Loss : 2392.370372677537\n",
      "Epoch 8 / 20 , Loss : 2383.2051453133513\n",
      "Epoch 9 / 20 , Loss : 2370.1281830581825\n",
      "Epoch 10 / 20 , Loss : 2350.1686568506534\n",
      "Epoch 11 / 20 , Loss : 2319.6696586193707\n",
      "Epoch 12 / 20 , Loss : 2274.5990704412307\n",
      "Epoch 13 / 20 , Loss : 2216.9643338802857\n",
      "Epoch 14 / 20 , Loss : 2157.7123393764846\n",
      "Epoch 15 / 20 , Loss : 2100.487578426027\n",
      "Epoch 16 / 20 , Loss : 2043.5462474821106\n",
      "Epoch 17 / 20 , Loss : 1984.0471068325141\n",
      "Epoch 18 / 20 , Loss : 1923.816775838831\n",
      "Epoch 19 / 20 , Loss : 1868.1035921994915\n",
      "Epoch 20 / 20 , Loss : 1816.9790637746291\n",
      "\n",
      "Accuracy:\n",
      "Train Accuracy: 29.75%\n",
      "Test Accuracy: 28.66%\n",
      "\n",
      "Per-Class Metrics (Test):\n",
      " Class     F1  Precision  Recall\n",
      "     1 0.4420     0.6689  0.3300\n",
      "     2 0.1133     0.1635  0.0867\n",
      "     3 0.1316     0.2737  0.0867\n",
      "     4 0.2163     0.2311  0.2033\n",
      "     5 0.0475     0.2162  0.0267\n",
      "     6 0.4178     0.4977  0.3600\n",
      "     7 0.2864     0.1898  0.5833\n",
      "     8 0.4476     0.7833  0.3133\n",
      "     9 0.4182     0.3426  0.5367\n",
      "    10 0.4725     0.3281  0.8433\n",
      "    11 0.3995     0.3297  0.5067\n",
      "    12 0.2998     0.2964  0.3033\n",
      "    13 0.2098     0.3488  0.1500\n",
      "    14 0.3783     0.2630  0.6733\n",
      "    15 0.2130     0.3485  0.1533\n",
      "    16 0.2094     0.2727  0.1700\n",
      "    17 0.0065     0.1250  0.0033\n",
      "    18 0.2462     0.2000  0.3200\n",
      "    19 0.3120     0.2016  0.6900\n",
      "    20 0.3356     0.3451  0.3267\n",
      "    21 0.1857     0.1625  0.2167\n",
      "    22 0.3911     0.4028  0.3800\n",
      "    23 0.1798     0.1521  0.2200\n",
      "    24 0.4391     0.4523  0.4267\n",
      "    25 0.0633     0.1519  0.0400\n",
      "    26 0.0000     0.0000  0.0000\n",
      "    27 0.0725     0.3871  0.0400\n",
      "    28 0.4028     0.3085  0.5800\n",
      "    29 0.0544     0.2903  0.0300\n",
      "    30 0.2522     0.2657  0.2400\n",
      "    31 0.3363     0.3060  0.3733\n",
      "    32 0.0066     1.0000  0.0033\n",
      "    33 0.2474     0.3333  0.1967\n",
      "    34 0.2474     0.2259  0.2733\n",
      "    35 0.2952     0.4351  0.2233\n",
      "    36 0.3620     0.3262  0.4067\n",
      "\n",
      "Average (Macro) Metrics (Test):\n",
      "F1: 0.2539, Precision: 0.3229, Recall: 0.2866\n",
      "---------------------------------------------------\n",
      "\n",
      "Per-Class Metrics (Train):\n",
      " Class     F1  Precision  Recall\n",
      "     1 0.4479     0.6826  0.3333\n",
      "     2 0.1581     0.2202  0.1233\n",
      "     3 0.1416     0.2932  0.0933\n",
      "     4 0.2170     0.2264  0.2083\n",
      "     5 0.0503     0.2237  0.0283\n",
      "     6 0.4349     0.5273  0.3700\n",
      "     7 0.2689     0.1781  0.5483\n",
      "     8 0.4906     0.8387  0.3467\n",
      "     9 0.4601     0.3712  0.6050\n",
      "    10 0.4667     0.3267  0.8167\n",
      "    11 0.4113     0.3354  0.5317\n",
      "    12 0.3189     0.3130  0.3250\n",
      "    13 0.2150     0.3509  0.1550\n",
      "    14 0.4067     0.2816  0.7317\n",
      "    15 0.2404     0.4008  0.1717\n",
      "    16 0.2152     0.2931  0.1700\n",
      "    17 0.0195     0.4286  0.0100\n",
      "    18 0.2519     0.2031  0.3317\n",
      "    19 0.3203     0.2078  0.6983\n",
      "    20 0.3782     0.3798  0.3767\n",
      "    21 0.2199     0.1914  0.2583\n",
      "    22 0.4275     0.4352  0.4200\n",
      "    23 0.1990     0.1717  0.2367\n",
      "    24 0.4215     0.4394  0.4050\n",
      "    25 0.0883     0.2000  0.0567\n",
      "    26 0.0000     0.0000  0.0000\n",
      "    27 0.0470     0.3947  0.0250\n",
      "    28 0.4408     0.3314  0.6583\n",
      "    29 0.0611     0.3636  0.0333\n",
      "    30 0.2051     0.2251  0.1883\n",
      "    31 0.3272     0.3023  0.3567\n",
      "    32 0.0000     0.0000  0.0000\n",
      "    33 0.2939     0.3834  0.2383\n",
      "    34 0.2539     0.2396  0.2700\n",
      "    35 0.2457     0.4030  0.1767\n",
      "    36 0.3517     0.3079  0.4100\n",
      "\n",
      "Average (Macro) Metrics (Train):\n",
      "F1: 0.2638, Precision: 0.3186, Recall: 0.2975\n",
      "===================================================\n",
      "\n",
      "=== Hidden layer architecture: [512, 256, 128, 64] ===\n",
      "Epoch 1 / 20 , Loss : 2430.249264037338\n",
      "Epoch 2 / 20 , Loss : 2420.0278458210273\n",
      "Epoch 3 / 20 , Loss : 2419.8420945550247\n",
      "Epoch 4 / 20 , Loss : 2419.4633952516124\n",
      "Epoch 5 / 20 , Loss : 2419.3049990969726\n",
      "Epoch 6 / 20 , Loss : 2419.2616830553784\n",
      "Epoch 7 / 20 , Loss : 2418.8307810118645\n",
      "Epoch 8 / 20 , Loss : 2418.7201620104584\n",
      "Epoch 9 / 20 , Loss : 2418.5889616160403\n",
      "Epoch 10 / 20 , Loss : 2418.242886761199\n",
      "Epoch 11 / 20 , Loss : 2417.833974789929\n",
      "Epoch 12 / 20 , Loss : 2417.5297691322403\n",
      "Epoch 13 / 20 , Loss : 2417.3435783522737\n",
      "Epoch 14 / 20 , Loss : 2417.02392785744\n",
      "Epoch 15 / 20 , Loss : 2416.5826365523367\n",
      "Epoch 16 / 20 , Loss : 2416.35288685496\n",
      "Epoch 17 / 20 , Loss : 2415.781036624818\n",
      "Epoch 18 / 20 , Loss : 2415.3350125557376\n",
      "Epoch 19 / 20 , Loss : 2414.9722137331532\n",
      "Epoch 20 / 20 , Loss : 2414.4234001262976\n",
      "\n",
      "Accuracy:\n",
      "Train Accuracy: 3.25%\n",
      "Test Accuracy: 3.27%\n",
      "\n",
      "Per-Class Metrics (Test):\n",
      " Class     F1  Precision  Recall\n",
      "     1 0.0000     0.0000  0.0000\n",
      "     2 0.0000     0.0000  0.0000\n",
      "     3 0.0000     0.0000  0.0000\n",
      "     4 0.0000     0.0000  0.0000\n",
      "     5 0.0000     0.0000  0.0000\n",
      "     6 0.0000     0.0000  0.0000\n",
      "     7 0.0000     0.0000  0.0000\n",
      "     8 0.0000     0.0000  0.0000\n",
      "     9 0.0000     0.0000  0.0000\n",
      "    10 0.0000     0.0000  0.0000\n",
      "    11 0.0000     0.0000  0.0000\n",
      "    12 0.0620     0.0320  1.0000\n",
      "    13 0.0000     0.0000  0.0000\n",
      "    14 0.0000     0.0000  0.0000\n",
      "    15 0.0000     0.0000  0.0000\n",
      "    16 0.0000     0.0000  0.0000\n",
      "    17 0.0000     0.0000  0.0000\n",
      "    18 0.0000     0.0000  0.0000\n",
      "    19 0.0000     0.0000  0.0000\n",
      "    20 0.0000     0.0000  0.0000\n",
      "    21 0.0000     0.0000  0.0000\n",
      "    22 0.0000     0.0000  0.0000\n",
      "    23 0.0000     0.0000  0.0000\n",
      "    24 0.0066     0.5000  0.0033\n",
      "    25 0.0605     0.0366  0.1733\n",
      "    26 0.0000     0.0000  0.0000\n",
      "    27 0.0000     0.0000  0.0000\n",
      "    28 0.0000     0.0000  0.0000\n",
      "    29 0.0000     0.0000  0.0000\n",
      "    30 0.0000     0.0000  0.0000\n",
      "    31 0.0000     0.0000  0.0000\n",
      "    32 0.0000     0.0000  0.0000\n",
      "    33 0.0000     0.0000  0.0000\n",
      "    34 0.0000     0.0000  0.0000\n",
      "    35 0.0000     0.0000  0.0000\n",
      "    36 0.0000     0.0000  0.0000\n",
      "\n",
      "Average (Macro) Metrics (Test):\n",
      "F1: 0.0036, Precision: 0.0158, Recall: 0.0327\n",
      "---------------------------------------------------\n",
      "\n",
      "Per-Class Metrics (Train):\n",
      " Class     F1  Precision  Recall\n",
      "     1 0.0000     0.0000  0.0000\n",
      "     2 0.0000     0.0000  0.0000\n",
      "     3 0.0000     0.0000  0.0000\n",
      "     4 0.0000     0.0000  0.0000\n",
      "     5 0.0000     0.0000  0.0000\n",
      "     6 0.0000     0.0000  0.0000\n",
      "     7 0.0000     0.0000  0.0000\n",
      "     8 0.0000     0.0000  0.0000\n",
      "     9 0.0000     0.0000  0.0000\n",
      "    10 0.0000     0.0000  0.0000\n",
      "    11 0.0000     0.0000  0.0000\n",
      "    12 0.0617     0.0318  1.0000\n",
      "    13 0.0000     0.0000  0.0000\n",
      "    14 0.0000     0.0000  0.0000\n",
      "    15 0.0000     0.0000  0.0000\n",
      "    16 0.0000     0.0000  0.0000\n",
      "    17 0.0000     0.0000  0.0000\n",
      "    18 0.0000     0.0000  0.0000\n",
      "    19 0.0000     0.0000  0.0000\n",
      "    20 0.0000     0.0000  0.0000\n",
      "    21 0.0000     0.0000  0.0000\n",
      "    22 0.0000     0.0000  0.0000\n",
      "    23 0.0000     0.0000  0.0000\n",
      "    24 0.0000     0.0000  0.0000\n",
      "    25 0.0605     0.0369  0.1683\n",
      "    26 0.0000     0.0000  0.0000\n",
      "    27 0.0000     0.0000  0.0000\n",
      "    28 0.0000     0.0000  0.0000\n",
      "    29 0.0000     0.0000  0.0000\n",
      "    30 0.0000     0.0000  0.0000\n",
      "    31 0.0000     0.0000  0.0000\n",
      "    32 0.0000     0.0000  0.0000\n",
      "    33 0.0000     0.0000  0.0000\n",
      "    34 0.0000     0.0000  0.0000\n",
      "    35 0.0000     0.0000  0.0000\n",
      "    36 0.0000     0.0000  0.0000\n",
      "\n",
      "Average (Macro) Metrics (Train):\n",
      "F1: 0.0034, Precision: 0.0019, Recall: 0.0325\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "hidden_layer_length = [\n",
    "    [512],\n",
    "    [512, 256],\n",
    "    [512, 256, 128],\n",
    "    [512, 256, 128, 64]\n",
    "]\n",
    "\n",
    "for h_len in hidden_layer_length:\n",
    "    print(f\"\\n=== Hidden layer architecture: {h_len} ===\")\n",
    "\n",
    "    model = NeuralNetwork(M=32, n=3072, HiddenLayer=h_len, target_class=36, lr=0.01)\n",
    "    model.fit(X_train, y_train, epochs=20)\n",
    "\n",
    "    # Predictions\n",
    "    y_test_prediction = model.predict(X_test)\n",
    "    y_train_prediction = model.predict(X_train)\n",
    "\n",
    "    # Accuracy\n",
    "    print(\"\\nAccuracy:\")\n",
    "    print(f\"Train Accuracy: {np.mean(y_train_prediction == np.argmax(y_train, axis=1)) * 100:.2f}%\")\n",
    "    print(f\"Test Accuracy: {np.mean(y_test_prediction == np.argmax(y_test, axis=1)) * 100:.2f}%\")\n",
    "\n",
    "    # ---------- Metric Calculation ----------\n",
    "    def results_metrics(y_true, y_pred):\n",
    "        y_true_labels = np.argmax(y_true, axis=1)\n",
    "        f1 = f1_score(y_true_labels, y_pred, average=None)\n",
    "        precision = precision_score(y_true_labels, y_pred, average=None, zero_division=0)\n",
    "        recall = recall_score(y_true_labels, y_pred, average=None, zero_division=0)\n",
    "        return f1, precision, recall\n",
    "\n",
    "    # ---------- TEST SET ----------\n",
    "    f1_test, precision_test, recall_test = results_metrics(y_test, y_test_prediction)\n",
    "    results_test = pd.DataFrame({\n",
    "        \"Class\": np.arange(1, len(f1_test) + 1),\n",
    "        \"F1\": f1_test,\n",
    "        \"Precision\": precision_test,\n",
    "        \"Recall\": recall_test\n",
    "    })\n",
    "\n",
    "    print(\"\\nPer-Class Metrics (Test):\")\n",
    "    print(results_test.round(4).to_string(index=False))\n",
    "\n",
    "    print(\"\\nAverage (Macro) Metrics (Test):\")\n",
    "    print(f\"F1: {np.mean(f1_test):.4f}, Precision: {np.mean(precision_test):.4f}, Recall: {np.mean(recall_test):.4f}\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "    # ---------- TRAIN SET ----------\n",
    "    f1_train, precision_train, recall_train = results_metrics(y_train, y_train_prediction)\n",
    "    results_train = pd.DataFrame({\n",
    "        \"Class\": np.arange(1, len(f1_train) + 1),\n",
    "        \"F1\": f1_train,\n",
    "        \"Precision\": precision_train,\n",
    "        \"Recall\": recall_train\n",
    "    })\n",
    "\n",
    "    print(\"\\nPer-Class Metrics (Train):\")\n",
    "    print(results_train.round(4).to_string(index=False))\n",
    "\n",
    "    print(\"\\nAverage (Macro) Metrics (Train):\")\n",
    "    print(f\"F1: {np.mean(f1_train):.4f}, Precision: {np.mean(precision_train):.4f}, Recall: {np.mean(recall_train):.4f}\")\n",
    "    print(\"===================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74043af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Hidden layer architecture: [512] ===\n",
      "Epoch 1 / 200 , Loss : 2165.4585596611087\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Hidden layer architecture: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralNetwork(M\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3072\u001b[39m, HiddenLayer\u001b[38;5;241m=\u001b[39mh_len, target_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m36\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[0;32m      8\u001b[0m y_test_prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "Cell \u001b[1;32mIn[5], line 128\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[1;34m(self, X, Y, epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m X_shuffled[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBatch_size]\n\u001b[0;32m    127\u001b[0m Y_batch \u001b[38;5;241m=\u001b[39m Y_shuffled[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBatch_size]\n\u001b[1;32m--> 128\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorss_entropy_loss(Y_batch , predictions)\n\u001b[0;32m    130\u001b[0m J \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[1;32mIn[5], line 110\u001b[0m, in \u001b[0;36mNeuralNetwork._prev\u001b[1;34m(self, Y, X)\u001b[0m\n\u001b[0;32m    108\u001b[0m do_down \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer\u001b[38;5;241m.\u001b[39m_prev(Y , stored_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m , \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m , \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 110\u001b[0m     do_down \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\u001b[38;5;241m.\u001b[39m_prev(do_down , stored_outputs[i])\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "h_len = [512]\n",
    "print(f\"\\n=== Hidden layer architecture: {h_len} ===\")\n",
    "\n",
    "model = NeuralNetwork(M=32, n=3072, HiddenLayer=h_len, target_class=36, lr=0.01)\n",
    "model.fit(X_train, y_train, epochs=200)\n",
    "\n",
    "# Predictions\n",
    "y_test_prediction = model.predict(X_test)\n",
    "y_train_prediction = model.predict(X_train)\n",
    "\n",
    "# Accuracy\n",
    "print(\"\\nAccuracy:\")\n",
    "print(f\"Train Accuracy: {np.mean(y_train_prediction == np.argmax(y_train, axis=1)) * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {np.mean(y_test_prediction == np.argmax(y_test, axis=1)) * 100:.2f}%\")\n",
    "\n",
    "# ---------- Metric Calculation ----------\n",
    "def results_metrics(y_true, y_pred):\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    f1 = f1_score(y_true_labels, y_pred, average=None)\n",
    "    precision = precision_score(y_true_labels, y_pred, average=None, zero_division=0)\n",
    "    recall = recall_score(y_true_labels, y_pred, average=None, zero_division=0)\n",
    "    return f1, precision, recall\n",
    "\n",
    "# ---------- TEST SET ----------\n",
    "f1_test, precision_test, recall_test = results_metrics(y_test, y_test_prediction)\n",
    "results_test = pd.DataFrame({\n",
    "    \"Class\": np.arange(1, len(f1_test) + 1),\n",
    "    \"F1\": f1_test,\n",
    "    \"Precision\": precision_test,\n",
    "    \"Recall\": recall_test\n",
    "})\n",
    "\n",
    "print(\"\\nPer-Class Metrics (Test):\")\n",
    "print(results_test.round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\nAverage (Macro) Metrics (Test):\")\n",
    "print(f\"F1: {np.mean(f1_test):.4f}, Precision: {np.mean(precision_test):.4f}, Recall: {np.mean(recall_test):.4f}\")\n",
    "print(\"---------------------------------------------------\")\n",
    "\n",
    "# ---------- TRAIN SET ----------\n",
    "f1_train, precision_train, recall_train = results_metrics(y_train, y_train_prediction)\n",
    "results_train = pd.DataFrame({\n",
    "    \"Class\": np.arange(1, len(f1_train) + 1),\n",
    "    \"F1\": f1_train,\n",
    "    \"Precision\": precision_train,\n",
    "    \"Recall\": recall_train\n",
    "})\n",
    "\n",
    "print(\"\\nPer-Class Metrics (Train):\")\n",
    "print(results_train.round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\nAverage (Macro) Metrics (Train):\")\n",
    "print(f\"F1: {np.mean(f1_train):.4f}, Precision: {np.mean(precision_train):.4f}, Recall: {np.mean(recall_train):.4f}\")\n",
    "print(\"===================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc2e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
