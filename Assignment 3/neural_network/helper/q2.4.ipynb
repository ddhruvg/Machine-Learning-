{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d3a6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be80307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_data(type=\"train\"):\n",
    "    data = []\n",
    "    label = []\n",
    "    for i in range(1, 37):\n",
    "        X = []\n",
    "        y = []\n",
    "        target = f\"{i:02d}\"  \n",
    "        train_data_path = f\"data/q2/{type}/{target}\"\n",
    "        \n",
    "        for img_name in os.listdir(train_data_path):\n",
    "            img_path = os.path.join(train_data_path, img_name)\n",
    "            \n",
    "            \n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            \n",
    "            img = cv2.resize(img, (32, 32))\n",
    "            img = img / 255.0 \n",
    "            img_flatten = img.flatten()\n",
    "            X.append(img_flatten)\n",
    "            y.append(i)\n",
    "\n",
    "        data.extend(X)\n",
    "        label.extend(y)\n",
    "\n",
    "    data = np.array(data)\n",
    "    label = np.array(label)\n",
    "    return data, label\n",
    "\n",
    "X_train , y_train = get_data(\"train\")\n",
    "X_test , y_test = get_data(\"test\")\n",
    "y_train  = np.eye(36)[y_train - 1].astype(np.float32)\n",
    "y_test  = np.eye(36)[y_test - 1].astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ef1dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self,in_dimension , out_dimension , learning_rate = 0.001 , is_sigmoid = True,is_output = False):\n",
    "        self.net_grad = None\n",
    "        self.lr = learning_rate\n",
    "        self.O = None\n",
    "        limit = np.sqrt(6 / (in_dimension + out_dimension))\n",
    "        self.w = np.random.uniform(-limit, limit, (in_dimension + 1, out_dimension)).astype(np.float32)\n",
    "\n",
    "        self.output_function = self.__sigmoid if is_sigmoid else self.__relu\n",
    "        self.output_function_grad = self.__sigmoid_grad if is_sigmoid else self.__relu_grad\n",
    "        self.is_output = is_output\n",
    "\n",
    "    def __relu(self,x):\n",
    "        return np.maximum(0,x)\n",
    "\n",
    "    def __relu_grad(self,x):\n",
    "        return np.where(x > 0 , 1 , 0)\n",
    "\n",
    "    def __sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __softmax(self,x):\n",
    "        exp_x = np.exp(x - np.max(x , axis =1 , keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis = 1 , keepdims=True)\n",
    "\n",
    "    def __sigmoid_grad(self,x):\n",
    "        return x * (1-x)    \n",
    "\n",
    "    def next(self,X ):\n",
    "        X = np.hstack([np.ones((X.shape[0] , 1)),X])\n",
    "        self.net_grad = np.dot(X, self.w)\n",
    "        self.O = self.output_function(self.net_grad)\n",
    "        return self.O\n",
    "\n",
    "    def _prev(self, dO , O_down):\n",
    "        if not self.is_output:\n",
    "            new_grad = dO * self.output_function_grad(self.O)\n",
    "            dw = np.dot(np.hstack([np.ones((O_down.shape[0], 1)), O_down]).T , new_grad) / O_down.shape[0]\n",
    "            d_O_down = np.dot(new_grad , self.w.T)\n",
    "            dO_prev =   d_O_down[:,1:]\n",
    "            self.w = self.w - self.lr * dw\n",
    "            return  dO_prev\n",
    "        \n",
    "            \n",
    "    \n",
    "\n",
    "class FinalLayer :\n",
    "    def __init__(self,in_dim , out_dim , lr = 0.001 ,is_softmax = True) :\n",
    "        self.lr = lr\n",
    "        limit = np.sqrt(6 / (in_dim + out_dim))\n",
    "        self.w = np.random.uniform(-limit, limit, (in_dim + 1, out_dim)).astype(np.float32)\n",
    "        self.net_grad = None\n",
    "        self.O = None\n",
    "        self.output_function = self.__softmax if is_softmax else self.__sigmoid\n",
    "\n",
    "    \n",
    "           \n",
    "\n",
    "    def __sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __softmax(self,x):\n",
    "        exp_x = np.exp(x - np.max(x , axis =1 , keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis = 1 , keepdims=True)\n",
    "\n",
    "    def __sigmoid_grad(self,x):\n",
    "        return x * (1-x)  \n",
    "\n",
    "    def next(self , X) :\n",
    "        X = np.hstack([np.ones((X.shape[0] , 1)),X ])    \n",
    "        self.net_grad = np.dot(X, self.w)\n",
    "        self.O = self.output_function(self.net_grad)\n",
    "        return self.O\n",
    "    \n",
    "    def _prev(self, y_true , O_down)  :\n",
    "        new_grad = self.O - y_true\n",
    "        dw = np.dot(np.hstack([np.ones((O_down.shape[0], 1)), O_down]).T , new_grad) / y_true.shape[0]\n",
    "        d_O_down = np.dot(new_grad , self.w.T)\n",
    "        dO_prev =   d_O_down[:,1:]\n",
    "        self.w = self.w - self.lr * dw\n",
    "        return dO_prev\n",
    "    \n",
    "class NeuralNetwork:\n",
    "    def __init__(self, M = 32 , n = 3072 , HiddenLayer = [512 , 265 ]  , target_class = 36 , lr = 0.01,is_sigmoid = True):\n",
    "        self.lr = lr\n",
    "        self.layers = []\n",
    "        self.Batch_size = M\n",
    "        prev_dimn = n\n",
    "        for h in HiddenLayer:\n",
    "            layer = Layer(prev_dimn , h , is_sigmoid = is_sigmoid , learning_rate = self.lr)\n",
    "            self.layers.append(layer)\n",
    "            prev_dimn = h\n",
    "        self.output_layer = FinalLayer(prev_dimn , target_class , is_softmax = True , lr = self.lr)\n",
    "\n",
    "    def next(self,X) : \n",
    "        O_down = X \n",
    "        for layer in self.layers :\n",
    "            O_down = layer.next(O_down)\n",
    "\n",
    "        predictions = self.output_layer.next(O_down)\n",
    "        return predictions\n",
    "    \n",
    "    def corss_entropy_loss(self, y_true , y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        return - np.sum(y_true * np.log(y_pred + 1e-10 )) / m\n",
    "     \n",
    "    def _prev(self,Y , X) : \n",
    "        O = X \n",
    "        stored_outputs = [X]\n",
    "        for layer in self.layers :\n",
    "            O = layer.next(O)\n",
    "            stored_outputs.append(O)\n",
    "        predictions = self.output_layer.next(O)\n",
    "\n",
    "       \n",
    "\n",
    "        do_down = self.output_layer._prev(Y , stored_outputs[-1])\n",
    "        for i in range(len(self.layers)-1 , -1 , -1):\n",
    "            do_down = self.layers[i]._prev(do_down , stored_outputs[i])\n",
    "\n",
    "        return predictions  \n",
    "\n",
    "    def predict(self,X) :\n",
    "        predictions = self.next(X)\n",
    "        return np.argmax(predictions , axis = 1)   \n",
    "    \n",
    "    def fit(self , X , Y , epochs = 10 ):\n",
    "        for epoch in range(epochs) : \n",
    "            comb = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[comb]\n",
    "            Y_shuffled = Y[comb]\n",
    "            J = 0\n",
    "\n",
    "            for i in range(0, X.shape[0] , self.Batch_size):\n",
    "                X_batch = X_shuffled[i : i + self.Batch_size]\n",
    "                Y_batch = Y_shuffled[i : i + self.Batch_size]\n",
    "                predictions = self._prev(Y_batch , X_batch)\n",
    "                loss = self.corss_entropy_loss(Y_batch , predictions)\n",
    "                J += loss\n",
    "           \n",
    "            print(f\"Epoch {epoch + 1} / {epochs} , Loss : {J}\")\n",
    "\n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e04767bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 20 , Loss : 1410.3795818505146\n",
      "Epoch 2 / 20 , Loss : 887.3334228372789\n",
      "Epoch 3 / 20 , Loss : 766.7980125820359\n",
      "Epoch 4 / 20 , Loss : 694.8222979784409\n",
      "Epoch 5 / 20 , Loss : 635.6503266572336\n",
      "Epoch 6 / 20 , Loss : 585.2483438553969\n",
      "Epoch 7 / 20 , Loss : 539.1828790157534\n",
      "Epoch 8 / 20 , Loss : 497.0867322085973\n",
      "Epoch 9 / 20 , Loss : 458.83680171435776\n",
      "Epoch 10 / 20 , Loss : 424.28964728745217\n",
      "Epoch 11 / 20 , Loss : 393.043294739849\n",
      "Epoch 12 / 20 , Loss : 364.4231414223733\n",
      "Epoch 13 / 20 , Loss : 339.3621528425476\n",
      "Epoch 14 / 20 , Loss : 315.3869720643986\n",
      "Epoch 15 / 20 , Loss : 293.45137232770793\n",
      "Epoch 16 / 20 , Loss : 274.6647893148358\n",
      "Epoch 17 / 20 , Loss : 256.78075100842386\n",
      "Epoch 18 / 20 , Loss : 240.91047587167682\n",
      "Epoch 19 / 20 , Loss : 225.60601375560924\n",
      "Epoch 20 / 20 , Loss : 211.7659058591298\n",
      "Train Accuracy:  94.27777777777779\n",
      "Test Accuracy:  84.93518518518518\n",
      "f1 Score for hidden layer size [512] :  0.849187369703186\n",
      "Precision for hidden layer size [512] :  0.849187369703186\n",
      "Recall for hidden layer size [512] :  0.849187369703186\n",
      "---------------------------------------------------\n",
      "Train f1 Score for hidden layer size [512] :  0.9427750745170239\n",
      "Train Precision for hidden layer size [512] :  0.9427750745170239\n",
      "Train Recall for hidden layer size [512] :  0.9427750745170239\n",
      "Epoch 1 / 20 , Loss : 1482.7430294436545\n",
      "Epoch 2 / 20 , Loss : 858.8297616119031\n",
      "Epoch 3 / 20 , Loss : 717.1942358629684\n",
      "Epoch 4 / 20 , Loss : 624.4373813402316\n",
      "Epoch 5 / 20 , Loss : 547.0611869551294\n",
      "Epoch 6 / 20 , Loss : 479.00610631521135\n",
      "Epoch 7 / 20 , Loss : 423.15828357707784\n",
      "Epoch 8 / 20 , Loss : 372.8450757396688\n",
      "Epoch 9 / 20 , Loss : 328.7699937894798\n",
      "Epoch 10 / 20 , Loss : 293.0301566420024\n",
      "Epoch 11 / 20 , Loss : 261.3582386818267\n",
      "Epoch 12 / 20 , Loss : 231.81056172306396\n",
      "Epoch 13 / 20 , Loss : 208.25438441162845\n",
      "Epoch 14 / 20 , Loss : 186.46178283489488\n",
      "Epoch 15 / 20 , Loss : 167.1216371785374\n",
      "Epoch 16 / 20 , Loss : 150.07844933811867\n",
      "Epoch 17 / 20 , Loss : 135.30791936745575\n",
      "Epoch 18 / 20 , Loss : 121.80488441876963\n",
      "Epoch 19 / 20 , Loss : 109.72502197078731\n",
      "Epoch 20 / 20 , Loss : 98.69618496556471\n",
      "Train Accuracy:  98.12962962962963\n",
      "Test Accuracy:  87.78703703703704\n",
      "f1 Score for hidden layer size [512, 256] :  0.8779510674150167\n",
      "Precision for hidden layer size [512, 256] :  0.8779510674150167\n",
      "Recall for hidden layer size [512, 256] :  0.8779510674150167\n",
      "---------------------------------------------------\n",
      "Train f1 Score for hidden layer size [512, 256] :  0.9812981083787835\n",
      "Train Precision for hidden layer size [512, 256] :  0.9812981083787835\n",
      "Train Recall for hidden layer size [512, 256] :  0.9812981083787835\n",
      "Epoch 1 / 20 , Loss : 1573.9711844104245\n",
      "Epoch 2 / 20 , Loss : 874.8661119127343\n",
      "Epoch 3 / 20 , Loss : 711.5453273071375\n",
      "Epoch 4 / 20 , Loss : 596.728619679428\n",
      "Epoch 5 / 20 , Loss : 504.3743516455389\n",
      "Epoch 6 / 20 , Loss : 427.1740888465041\n",
      "Epoch 7 / 20 , Loss : 364.8474996103706\n",
      "Epoch 8 / 20 , Loss : 312.26611866968614\n",
      "Epoch 9 / 20 , Loss : 267.53723889464266\n",
      "Epoch 10 / 20 , Loss : 232.60589972161137\n",
      "Epoch 11 / 20 , Loss : 201.71363847964952\n",
      "Epoch 12 / 20 , Loss : 174.03837215585384\n",
      "Epoch 13 / 20 , Loss : 150.07522958290158\n",
      "Epoch 14 / 20 , Loss : 128.3500813901065\n",
      "Epoch 15 / 20 , Loss : 110.23732786879091\n",
      "Epoch 16 / 20 , Loss : 93.56320950878471\n",
      "Epoch 17 / 20 , Loss : 80.36863678866827\n",
      "Epoch 18 / 20 , Loss : 68.37255510926681\n",
      "Epoch 19 / 20 , Loss : 57.798355123659455\n",
      "Epoch 20 / 20 , Loss : 48.24960182142839\n",
      "Train Accuracy:  99.3888888888889\n",
      "Test Accuracy:  88.17592592592592\n",
      "f1 Score for hidden layer size [512, 256, 128] :  0.8818264342417821\n",
      "Precision for hidden layer size [512, 256, 128] :  0.8818264342417821\n",
      "Recall for hidden layer size [512, 256, 128] :  0.8818264342417821\n",
      "---------------------------------------------------\n",
      "Train f1 Score for hidden layer size [512, 256, 128] :  0.9938809902886467\n",
      "Train Precision for hidden layer size [512, 256, 128] :  0.9938809902886467\n",
      "Train Recall for hidden layer size [512, 256, 128] :  0.9938809902886467\n",
      "Epoch 1 / 20 , Loss : 1848.3595492809031\n",
      "Epoch 2 / 20 , Loss : 981.4126412028418\n",
      "Epoch 3 / 20 , Loss : 744.5043554096501\n",
      "Epoch 4 / 20 , Loss : 600.9829811367883\n",
      "Epoch 5 / 20 , Loss : 492.6027884057174\n",
      "Epoch 6 / 20 , Loss : 405.8296071382328\n",
      "Epoch 7 / 20 , Loss : 336.07836376739175\n",
      "Epoch 8 / 20 , Loss : 282.5308467860986\n",
      "Epoch 9 / 20 , Loss : 237.11681291833594\n",
      "Epoch 10 / 20 , Loss : 197.15067572916104\n",
      "Epoch 11 / 20 , Loss : 167.4251599243649\n",
      "Epoch 12 / 20 , Loss : 139.15343024049665\n",
      "Epoch 13 / 20 , Loss : 112.84863761713036\n",
      "Epoch 14 / 20 , Loss : 93.27198993432555\n",
      "Epoch 15 / 20 , Loss : 76.78277508789441\n",
      "Epoch 16 / 20 , Loss : 61.53074395253072\n",
      "Epoch 17 / 20 , Loss : 48.03193760283483\n",
      "Epoch 18 / 20 , Loss : 38.27224433571431\n",
      "Epoch 19 / 20 , Loss : 29.29353042914364\n",
      "Epoch 20 / 20 , Loss : 22.002727477669236\n",
      "Train Accuracy:  99.92129629629629\n",
      "Test Accuracy:  88.5925925925926\n",
      "f1 Score for hidden layer size [512, 256, 128, 64] :  0.88602399142009\n",
      "Precision for hidden layer size [512, 256, 128, 64] :  0.88602399142009\n",
      "Recall for hidden layer size [512, 256, 128, 64] :  0.88602399142009\n",
      "---------------------------------------------------\n",
      "Train f1 Score for hidden layer size [512, 256, 128, 64] :  0.9992134564600667\n",
      "Train Precision for hidden layer size [512, 256, 128, 64] :  0.9992134564600667\n",
      "Train Recall for hidden layer size [512, 256, 128, 64] :  0.9992134564600667\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_length = [[512],[512,256],[512,256,128],[512,256,128,64],]\n",
    "\n",
    "for h_len in hidden_layer_length:\n",
    "    model = NeuralNetwork(M = 32 , n = 3072 , HiddenLayer = h_len  , target_class = 36 , is_sigmoid = False , lr = 0.01) \n",
    "    model.fit(X_train , y_train , epochs = 20)\n",
    "    y_test_prediction = model.predict(X_test)\n",
    "    y_train_prediction = model.predict(X_train)\n",
    "    print(\"Train Accuracy: \", np.mean(y_train_prediction == np.argmax(y_train , axis = 1)) * 100)\n",
    "    print(\"Test Accuracy: \", np.mean(y_test_prediction == np.argmax(y_test , axis = 1)) * 100)\n",
    "    \n",
    "    def results_metrics(y_true, y_pred):\n",
    "        f1 = f1_score(np.argmax(y_true , axis = 1) , y_pred , average='macro')\n",
    "        precision = f1_score(np.argmax(y_true , axis = 1) , y_pred , average='macro', zero_division=0)\n",
    "        recall = f1_score(np.argmax(y_true , axis = 1) , y_pred , average='macro', zero_division=0)\n",
    "        return f1, precision, recall\n",
    "\n",
    "    f1, precision, recall = results_metrics(y_test, y_test_prediction)\n",
    "    print(f\"f1 Score for hidden layer size {h_len} : \", f1)\n",
    "    print(f\"Precision for hidden layer size {h_len} : \", precision)\n",
    "    print(f\"Recall for hidden layer size {h_len} : \", recall)\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "    f1, precision, recall = results_metrics(y_train, y_train_prediction)\n",
    "    print(f\"Train f1 Score for hidden layer size {h_len} : \", f1)\n",
    "    print(f\"Train Precision for hidden layer size {h_len} : \", precision)\n",
    "    print(f\"Train Recall for hidden layer size {h_len} : \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf2655df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Training Model with Hidden Layers: [512] =========\n",
      "Epoch 1 / 20 , Loss : 1417.0166322760367\n",
      "Epoch 2 / 20 , Loss : 880.0839094791822\n",
      "Epoch 3 / 20 , Loss : 760.4211851574343\n",
      "Epoch 4 / 20 , Loss : 687.5992048771793\n",
      "Epoch 5 / 20 , Loss : 629.7357836509797\n",
      "Epoch 6 / 20 , Loss : 578.6031068963745\n",
      "Epoch 7 / 20 , Loss : 532.5367287154924\n",
      "Epoch 8 / 20 , Loss : 490.17649100649487\n",
      "Epoch 9 / 20 , Loss : 452.2570125952336\n",
      "Epoch 10 / 20 , Loss : 417.2496015668978\n",
      "Epoch 11 / 20 , Loss : 386.157857433306\n",
      "Epoch 12 / 20 , Loss : 357.68938576596383\n",
      "Epoch 13 / 20 , Loss : 332.50662490900936\n",
      "Epoch 14 / 20 , Loss : 309.0680204723646\n",
      "Epoch 15 / 20 , Loss : 287.720769218596\n",
      "Epoch 16 / 20 , Loss : 269.31456291957306\n",
      "Epoch 17 / 20 , Loss : 251.3456894027004\n",
      "Epoch 18 / 20 , Loss : 235.03880321957462\n",
      "Epoch 19 / 20 , Loss : 220.51272564859062\n",
      "Epoch 20 / 20 , Loss : 207.0537379465492\n",
      "Train Accuracy: 94.24%\n",
      "Test Accuracy: 85.19%\n",
      "\n",
      "üìä Test Macro Metrics:\n",
      "F1 Score: 0.8517\n",
      "Precision: 0.8548\n",
      "Recall: 0.8519\n",
      "\n",
      "üìä Train Macro Metrics:\n",
      "F1 Score: 0.9424\n",
      "Precision: 0.9434\n",
      "Recall: 0.9424\n",
      "\n",
      "üîç Detailed Per-Class Metrics (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9516    0.9167    0.9338       300\n",
      "           1     0.7720    0.8467    0.8076       300\n",
      "           2     0.8439    0.8467    0.8453       300\n",
      "           3     0.7470    0.8267    0.7848       300\n",
      "           4     0.8423    0.8367    0.8395       300\n",
      "           5     0.8235    0.8867    0.8539       300\n",
      "           6     0.8810    0.7900    0.8330       300\n",
      "           7     0.8955    0.8567    0.8756       300\n",
      "           8     0.9386    0.9167    0.9275       300\n",
      "           9     0.9397    0.8833    0.9107       300\n",
      "          10     0.9754    0.9267    0.9504       300\n",
      "          11     0.9153    0.9000    0.9076       300\n",
      "          12     0.7764    0.8567    0.8146       300\n",
      "          13     0.8313    0.9200    0.8734       300\n",
      "          14     0.8581    0.8867    0.8721       300\n",
      "          15     0.8675    0.9167    0.8914       300\n",
      "          16     0.8014    0.7800    0.7905       300\n",
      "          17     0.8938    0.8133    0.8517       300\n",
      "          18     0.8410    0.7933    0.8165       300\n",
      "          19     0.8433    0.7533    0.7958       300\n",
      "          20     0.8111    0.8733    0.8411       300\n",
      "          21     0.9228    0.9167    0.9197       300\n",
      "          22     0.8421    0.7467    0.7915       300\n",
      "          23     0.8493    0.8267    0.8378       300\n",
      "          24     0.7543    0.8700    0.8080       300\n",
      "          25     0.8315    0.7567    0.7923       300\n",
      "          26     0.9231    0.9200    0.9215       300\n",
      "          27     0.9199    0.8800    0.8995       300\n",
      "          28     0.7493    0.8367    0.7906       300\n",
      "          29     0.8716    0.8600    0.8658       300\n",
      "          30     0.7705    0.9067    0.8331       300\n",
      "          31     0.8761    0.6367    0.7375       300\n",
      "          32     0.8576    0.8633    0.8605       300\n",
      "          33     0.8517    0.9000    0.8752       300\n",
      "          34     0.8459    0.8600    0.8529       300\n",
      "          35     0.8571    0.8600    0.8586       300\n",
      "\n",
      "    accuracy                         0.8519     10800\n",
      "   macro avg     0.8548    0.8519    0.8517     10800\n",
      "weighted avg     0.8548    0.8519    0.8517     10800\n",
      "\n",
      "\n",
      "üîç Detailed Per-Class Metrics (Train Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9898    0.9667    0.9781       600\n",
      "           1     0.8988    0.9617    0.9291       600\n",
      "           2     0.9566    0.9550    0.9558       600\n",
      "           3     0.9013    0.9283    0.9146       600\n",
      "           4     0.9369    0.9150    0.9258       600\n",
      "           5     0.9233    0.9633    0.9429       600\n",
      "           6     0.9699    0.9117    0.9399       600\n",
      "           7     0.9571    0.9300    0.9434       600\n",
      "           8     0.9883    0.9817    0.9849       600\n",
      "           9     0.9544    0.9417    0.9480       600\n",
      "          10     0.9813    0.9600    0.9705       600\n",
      "          11     0.9681    0.9600    0.9640       600\n",
      "          12     0.9060    0.9317    0.9187       600\n",
      "          13     0.9184    0.9750    0.9458       600\n",
      "          14     0.9570    0.9650    0.9610       600\n",
      "          15     0.9570    0.9633    0.9601       600\n",
      "          16     0.9448    0.9133    0.9288       600\n",
      "          17     0.9545    0.9083    0.9308       600\n",
      "          18     0.9410    0.9300    0.9355       600\n",
      "          19     0.9634    0.9217    0.9421       600\n",
      "          20     0.9088    0.9633    0.9353       600\n",
      "          21     0.9816    0.9767    0.9791       600\n",
      "          22     0.9468    0.8900    0.9175       600\n",
      "          23     0.9410    0.9567    0.9488       600\n",
      "          24     0.8873    0.9583    0.9215       600\n",
      "          25     0.9488    0.8950    0.9211       600\n",
      "          26     0.9639    0.9800    0.9719       600\n",
      "          27     0.9775    0.9417    0.9593       600\n",
      "          28     0.8799    0.9283    0.9035       600\n",
      "          29     0.9579    0.9483    0.9531       600\n",
      "          30     0.8848    0.9733    0.9270       600\n",
      "          31     0.9725    0.8267    0.8937       600\n",
      "          32     0.9277    0.9617    0.9444       600\n",
      "          33     0.9642    0.9417    0.9528       600\n",
      "          34     0.9179    0.9317    0.9247       600\n",
      "          35     0.9357    0.9700    0.9525       600\n",
      "\n",
      "    accuracy                         0.9424     21600\n",
      "   macro avg     0.9434    0.9424    0.9424     21600\n",
      "weighted avg     0.9434    0.9424    0.9424     21600\n",
      "\n",
      "===================================================\n",
      "\n",
      "========= Training Model with Hidden Layers: [512, 256] =========\n",
      "Epoch 1 / 20 , Loss : 1444.322584030121\n",
      "Epoch 2 / 20 , Loss : 842.7938252354987\n",
      "Epoch 3 / 20 , Loss : 707.5667401509621\n",
      "Epoch 4 / 20 , Loss : 614.1853386059255\n",
      "Epoch 5 / 20 , Loss : 535.025558446318\n",
      "Epoch 6 / 20 , Loss : 468.7352501393043\n",
      "Epoch 7 / 20 , Loss : 411.3459971713993\n",
      "Epoch 8 / 20 , Loss : 362.13776388771174\n",
      "Epoch 9 / 20 , Loss : 321.1031795718658\n",
      "Epoch 10 / 20 , Loss : 285.9517469529513\n",
      "Epoch 11 / 20 , Loss : 253.5498398822018\n",
      "Epoch 12 / 20 , Loss : 225.83714632423042\n",
      "Epoch 13 / 20 , Loss : 202.73807257501\n",
      "Epoch 14 / 20 , Loss : 182.15816855994495\n",
      "Epoch 15 / 20 , Loss : 163.24314470578793\n",
      "Epoch 16 / 20 , Loss : 146.58221183279016\n",
      "Epoch 17 / 20 , Loss : 131.47036113086202\n",
      "Epoch 18 / 20 , Loss : 117.54905968796236\n",
      "Epoch 19 / 20 , Loss : 105.12872850685918\n",
      "Epoch 20 / 20 , Loss : 94.60833700395347\n",
      "Train Accuracy: 98.25%\n",
      "Test Accuracy: 87.47%\n",
      "\n",
      "üìä Test Macro Metrics:\n",
      "F1 Score: 0.8746\n",
      "Precision: 0.8767\n",
      "Recall: 0.8747\n",
      "\n",
      "üìä Train Macro Metrics:\n",
      "F1 Score: 0.9825\n",
      "Precision: 0.9827\n",
      "Recall: 0.9825\n",
      "\n",
      "üîç Detailed Per-Class Metrics (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9398    0.9367    0.9382       300\n",
      "           1     0.8328    0.8633    0.8478       300\n",
      "           2     0.9161    0.8733    0.8942       300\n",
      "           3     0.7932    0.8567    0.8237       300\n",
      "           4     0.8547    0.8433    0.8490       300\n",
      "           5     0.9075    0.8833    0.8953       300\n",
      "           6     0.8431    0.8600    0.8515       300\n",
      "           7     0.8442    0.9033    0.8728       300\n",
      "           8     0.9191    0.9467    0.9327       300\n",
      "           9     0.9100    0.9433    0.9264       300\n",
      "          10     0.9720    0.9267    0.9488       300\n",
      "          11     0.8743    0.9733    0.9211       300\n",
      "          12     0.8416    0.8500    0.8458       300\n",
      "          13     0.8673    0.9367    0.9006       300\n",
      "          14     0.9331    0.8833    0.9075       300\n",
      "          15     0.9094    0.9033    0.9064       300\n",
      "          16     0.7880    0.8300    0.8084       300\n",
      "          17     0.8851    0.8733    0.8792       300\n",
      "          18     0.8893    0.7233    0.7978       300\n",
      "          19     0.7903    0.8667    0.8267       300\n",
      "          20     0.8119    0.9067    0.8567       300\n",
      "          21     0.9094    0.9367    0.9228       300\n",
      "          22     0.8381    0.7767    0.8062       300\n",
      "          23     0.8836    0.8600    0.8716       300\n",
      "          24     0.8600    0.8600    0.8600       300\n",
      "          25     0.8505    0.7967    0.8227       300\n",
      "          26     0.9137    0.9533    0.9331       300\n",
      "          27     0.9455    0.8667    0.9043       300\n",
      "          28     0.7435    0.8600    0.7975       300\n",
      "          29     0.8829    0.9300    0.9058       300\n",
      "          30     0.9158    0.8700    0.8923       300\n",
      "          31     0.8364    0.7667    0.8000       300\n",
      "          32     0.9275    0.8100    0.8648       300\n",
      "          33     0.9076    0.9167    0.9121       300\n",
      "          34     0.9206    0.8500    0.8839       300\n",
      "          35     0.9046    0.8533    0.8782       300\n",
      "\n",
      "    accuracy                         0.8747     10800\n",
      "   macro avg     0.8767    0.8747    0.8746     10800\n",
      "weighted avg     0.8767    0.8747    0.8746     10800\n",
      "\n",
      "\n",
      "üîç Detailed Per-Class Metrics (Train Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9900    0.9900    0.9900       600\n",
      "           1     0.9818    0.9917    0.9867       600\n",
      "           2     0.9917    0.9900    0.9908       600\n",
      "           3     0.9686    0.9783    0.9735       600\n",
      "           4     0.9751    0.9783    0.9767       600\n",
      "           5     0.9949    0.9850    0.9899       600\n",
      "           6     0.9689    0.9867    0.9777       600\n",
      "           7     0.9674    0.9900    0.9786       600\n",
      "           8     0.9934    0.9967    0.9950       600\n",
      "           9     0.9787    0.9950    0.9868       600\n",
      "          10     1.0000    0.9800    0.9899       600\n",
      "          11     0.9706    0.9917    0.9810       600\n",
      "          12     0.9799    0.9750    0.9774       600\n",
      "          13     0.9642    0.9867    0.9753       600\n",
      "          14     0.9966    0.9900    0.9933       600\n",
      "          15     0.9866    0.9850    0.9858       600\n",
      "          16     0.9611    0.9883    0.9745       600\n",
      "          17     0.9882    0.9733    0.9807       600\n",
      "          18     0.9982    0.9483    0.9726       600\n",
      "          19     0.9735    0.9783    0.9759       600\n",
      "          20     0.9692    0.9967    0.9827       600\n",
      "          21     0.9852    0.9983    0.9917       600\n",
      "          22     0.9812    0.9567    0.9688       600\n",
      "          23     0.9916    0.9817    0.9866       600\n",
      "          24     0.9785    0.9883    0.9834       600\n",
      "          25     0.9814    0.9683    0.9748       600\n",
      "          26     0.9820    1.0000    0.9909       600\n",
      "          27     0.9983    0.9617    0.9796       600\n",
      "          28     0.9281    0.9900    0.9581       600\n",
      "          29     0.9868    0.9967    0.9917       600\n",
      "          30     1.0000    0.9733    0.9865       600\n",
      "          31     0.9898    0.9733    0.9815       600\n",
      "          32     0.9932    0.9783    0.9857       600\n",
      "          33     0.9933    0.9900    0.9917       600\n",
      "          34     0.9966    0.9650    0.9805       600\n",
      "          35     0.9932    0.9717    0.9823       600\n",
      "\n",
      "    accuracy                         0.9825     21600\n",
      "   macro avg     0.9827    0.9825    0.9825     21600\n",
      "weighted avg     0.9827    0.9825    0.9825     21600\n",
      "\n",
      "===================================================\n",
      "\n",
      "========= Training Model with Hidden Layers: [512, 256, 128] =========\n",
      "Epoch 1 / 20 , Loss : 1570.6070509456638\n",
      "Epoch 2 / 20 , Loss : 859.3107956399284\n",
      "Epoch 3 / 20 , Loss : 692.9914618113394\n",
      "Epoch 4 / 20 , Loss : 580.4449506749587\n",
      "Epoch 5 / 20 , Loss : 487.124729040432\n",
      "Epoch 6 / 20 , Loss : 410.1344231318232\n",
      "Epoch 7 / 20 , Loss : 347.42705317873066\n",
      "Epoch 8 / 20 , Loss : 297.53663942832264\n",
      "Epoch 9 / 20 , Loss : 254.75473826549467\n",
      "Epoch 10 / 20 , Loss : 220.4994896782617\n",
      "Epoch 11 / 20 , Loss : 190.19921614673405\n",
      "Epoch 12 / 20 , Loss : 162.93682122343407\n",
      "Epoch 13 / 20 , Loss : 139.7734402299698\n",
      "Epoch 14 / 20 , Loss : 120.08629706483002\n",
      "Epoch 15 / 20 , Loss : 101.09214205391903\n",
      "Epoch 16 / 20 , Loss : 87.85061283699342\n",
      "Epoch 17 / 20 , Loss : 73.24330699014354\n",
      "Epoch 18 / 20 , Loss : 62.17859882340712\n",
      "Epoch 19 / 20 , Loss : 52.27218385196781\n",
      "Epoch 20 / 20 , Loss : 43.64744178285469\n",
      "Train Accuracy: 99.08%\n",
      "Test Accuracy: 87.66%\n",
      "\n",
      "üìä Test Macro Metrics:\n",
      "F1 Score: 0.8768\n",
      "Precision: 0.8813\n",
      "Recall: 0.8766\n",
      "\n",
      "üìä Train Macro Metrics:\n",
      "F1 Score: 0.9909\n",
      "Precision: 0.9917\n",
      "Recall: 0.9908\n",
      "\n",
      "üîç Detailed Per-Class Metrics (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9191    0.9467    0.9327       300\n",
      "           1     0.8543    0.8600    0.8571       300\n",
      "           2     0.8307    0.8833    0.8562       300\n",
      "           3     0.9015    0.7933    0.8440       300\n",
      "           4     0.8815    0.7933    0.8351       300\n",
      "           5     0.8787    0.8933    0.8860       300\n",
      "           6     0.9148    0.8233    0.8667       300\n",
      "           7     0.8833    0.8833    0.8833       300\n",
      "           8     0.9530    0.9467    0.9498       300\n",
      "           9     0.9329    0.9267    0.9298       300\n",
      "          10     0.9661    0.9500    0.9580       300\n",
      "          11     0.9293    0.9200    0.9246       300\n",
      "          12     0.8397    0.8733    0.8562       300\n",
      "          13     0.9072    0.8800    0.8934       300\n",
      "          14     0.9249    0.9033    0.9140       300\n",
      "          15     0.9246    0.9400    0.9322       300\n",
      "          16     0.6303    0.8867    0.7368       300\n",
      "          17     0.8198    0.9100    0.8626       300\n",
      "          18     0.8704    0.7167    0.7861       300\n",
      "          19     0.8174    0.9100    0.8612       300\n",
      "          20     0.8447    0.9067    0.8746       300\n",
      "          21     0.9621    0.9300    0.9458       300\n",
      "          22     0.8571    0.7600    0.8057       300\n",
      "          23     0.8721    0.8633    0.8677       300\n",
      "          24     0.8185    0.8867    0.8512       300\n",
      "          25     0.8333    0.8333    0.8333       300\n",
      "          26     0.9406    0.9500    0.9453       300\n",
      "          27     0.9228    0.9167    0.9197       300\n",
      "          28     0.7701    0.8933    0.8272       300\n",
      "          29     0.8854    0.8500    0.8673       300\n",
      "          30     0.8874    0.8933    0.8904       300\n",
      "          31     0.9352    0.6733    0.7829       300\n",
      "          32     0.9062    0.8700    0.8878       300\n",
      "          33     0.8977    0.9067    0.9022       300\n",
      "          34     0.9125    0.9033    0.9079       300\n",
      "          35     0.9010    0.8800    0.8904       300\n",
      "\n",
      "    accuracy                         0.8766     10800\n",
      "   macro avg     0.8813    0.8766    0.8768     10800\n",
      "weighted avg     0.8813    0.8766    0.8768     10800\n",
      "\n",
      "\n",
      "üîç Detailed Per-Class Metrics (Train Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9983    0.9992       600\n",
      "           1     0.9934    0.9983    0.9958       600\n",
      "           2     0.9917    0.9967    0.9942       600\n",
      "           3     1.0000    0.9617    0.9805       600\n",
      "           4     1.0000    0.9933    0.9967       600\n",
      "           5     0.9967    0.9983    0.9975       600\n",
      "           6     1.0000    0.9950    0.9975       600\n",
      "           7     0.9983    0.9933    0.9958       600\n",
      "           8     1.0000    1.0000    1.0000       600\n",
      "           9     0.9950    0.9983    0.9967       600\n",
      "          10     1.0000    0.9983    0.9992       600\n",
      "          11     1.0000    0.9950    0.9975       600\n",
      "          12     0.9934    0.9983    0.9958       600\n",
      "          13     0.9983    0.9767    0.9874       600\n",
      "          14     0.9967    0.9933    0.9950       600\n",
      "          15     0.9967    1.0000    0.9983       600\n",
      "          16     0.8403    1.0000    0.9132       600\n",
      "          17     0.9756    0.9983    0.9868       600\n",
      "          18     0.9962    0.8700    0.9288       600\n",
      "          19     0.9884    0.9983    0.9934       600\n",
      "          20     0.9917    0.9967    0.9942       600\n",
      "          21     0.9983    0.9983    0.9983       600\n",
      "          22     0.9966    0.9917    0.9942       600\n",
      "          23     0.9983    0.9950    0.9967       600\n",
      "          24     0.9917    0.9983    0.9950       600\n",
      "          25     0.9916    0.9883    0.9900       600\n",
      "          26     0.9983    1.0000    0.9992       600\n",
      "          27     0.9983    0.9883    0.9933       600\n",
      "          28     0.9788    0.9983    0.9884       600\n",
      "          29     1.0000    0.9983    0.9992       600\n",
      "          30     0.9983    0.9883    0.9933       600\n",
      "          31     1.0000    0.9733    0.9865       600\n",
      "          32     1.0000    1.0000    1.0000       600\n",
      "          33     1.0000    0.9967    0.9983       600\n",
      "          34     1.0000    0.9950    0.9975       600\n",
      "          35     0.9967    1.0000    0.9983       600\n",
      "\n",
      "    accuracy                         0.9908     21600\n",
      "   macro avg     0.9917    0.9908    0.9909     21600\n",
      "weighted avg     0.9917    0.9908    0.9909     21600\n",
      "\n",
      "===================================================\n",
      "\n",
      "========= Training Model with Hidden Layers: [512, 256, 128, 64] =========\n",
      "Epoch 1 / 20 , Loss : 1851.901571491418\n",
      "Epoch 2 / 20 , Loss : 1006.8688471252505\n",
      "Epoch 3 / 20 , Loss : 754.5579224502619\n",
      "Epoch 4 / 20 , Loss : 603.6797612958932\n",
      "Epoch 5 / 20 , Loss : 490.45203244548605\n",
      "Epoch 6 / 20 , Loss : 400.7554552927075\n",
      "Epoch 7 / 20 , Loss : 330.8115096257123\n",
      "Epoch 8 / 20 , Loss : 273.32950136342026\n",
      "Epoch 9 / 20 , Loss : 224.5636101242709\n",
      "Epoch 10 / 20 , Loss : 190.02431277557147\n",
      "Epoch 11 / 20 , Loss : 156.8171547528175\n",
      "Epoch 12 / 20 , Loss : 128.88711330936016\n",
      "Epoch 13 / 20 , Loss : 106.24066978430069\n",
      "Epoch 14 / 20 , Loss : 85.91511646977757\n",
      "Epoch 15 / 20 , Loss : 69.34738973588338\n",
      "Epoch 16 / 20 , Loss : 53.66539118636287\n",
      "Epoch 17 / 20 , Loss : 41.21602907020947\n",
      "Epoch 18 / 20 , Loss : 32.72490007290981\n",
      "Epoch 19 / 20 , Loss : 25.325266431900502\n",
      "Epoch 20 / 20 , Loss : 18.317229245341696\n",
      "Train Accuracy: 99.92%\n",
      "Test Accuracy: 88.95%\n",
      "\n",
      "üìä Test Macro Metrics:\n",
      "F1 Score: 0.8895\n",
      "Precision: 0.8910\n",
      "Recall: 0.8895\n",
      "\n",
      "üìä Train Macro Metrics:\n",
      "F1 Score: 0.9992\n",
      "Precision: 0.9992\n",
      "Recall: 0.9992\n",
      "\n",
      "üîç Detailed Per-Class Metrics (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9156    0.9400    0.9276       300\n",
      "           1     0.8138    0.9033    0.8562       300\n",
      "           2     0.8750    0.9100    0.8922       300\n",
      "           3     0.7859    0.8567    0.8198       300\n",
      "           4     0.8733    0.8500    0.8615       300\n",
      "           5     0.9033    0.9033    0.9033       300\n",
      "           6     0.9149    0.8600    0.8866       300\n",
      "           7     0.8822    0.9233    0.9023       300\n",
      "           8     0.9685    0.9233    0.9454       300\n",
      "           9     0.9248    0.9433    0.9340       300\n",
      "          10     0.9600    0.9600    0.9600       300\n",
      "          11     0.9426    0.9300    0.9362       300\n",
      "          12     0.8682    0.9000    0.8838       300\n",
      "          13     0.9307    0.9400    0.9353       300\n",
      "          14     0.8958    0.9167    0.9061       300\n",
      "          15     0.9307    0.9400    0.9353       300\n",
      "          16     0.8782    0.7933    0.8336       300\n",
      "          17     0.8767    0.8767    0.8767       300\n",
      "          18     0.8147    0.8500    0.8320       300\n",
      "          19     0.8553    0.8667    0.8609       300\n",
      "          20     0.8795    0.9000    0.8896       300\n",
      "          21     0.9532    0.9500    0.9516       300\n",
      "          22     0.9087    0.7300    0.8096       300\n",
      "          23     0.9046    0.8533    0.8782       300\n",
      "          24     0.8935    0.7833    0.8348       300\n",
      "          25     0.8630    0.8400    0.8514       300\n",
      "          26     0.9461    0.9367    0.9414       300\n",
      "          27     0.9490    0.9300    0.9394       300\n",
      "          28     0.8280    0.8667    0.8469       300\n",
      "          29     0.9016    0.9167    0.9091       300\n",
      "          30     0.8626    0.9000    0.8809       300\n",
      "          31     0.7781    0.8767    0.8245       300\n",
      "          32     0.8750    0.8867    0.8808       300\n",
      "          33     0.8937    0.8967    0.8952       300\n",
      "          34     0.9144    0.8900    0.9020       300\n",
      "          35     0.9135    0.8800    0.8964       300\n",
      "\n",
      "    accuracy                         0.8895     10800\n",
      "   macro avg     0.8910    0.8895    0.8895     10800\n",
      "weighted avg     0.8910    0.8895    0.8895     10800\n",
      "\n",
      "\n",
      "üîç Detailed Per-Class Metrics (Train Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000       600\n",
      "           1     0.9983    1.0000    0.9992       600\n",
      "           2     0.9983    1.0000    0.9992       600\n",
      "           3     1.0000    0.9983    0.9992       600\n",
      "           4     1.0000    1.0000    1.0000       600\n",
      "           5     1.0000    1.0000    1.0000       600\n",
      "           6     1.0000    1.0000    1.0000       600\n",
      "           7     0.9950    1.0000    0.9975       600\n",
      "           8     1.0000    1.0000    1.0000       600\n",
      "           9     1.0000    1.0000    1.0000       600\n",
      "          10     1.0000    1.0000    1.0000       600\n",
      "          11     1.0000    1.0000    1.0000       600\n",
      "          12     1.0000    1.0000    1.0000       600\n",
      "          13     0.9983    0.9983    0.9983       600\n",
      "          14     0.9983    0.9983    0.9983       600\n",
      "          15     1.0000    1.0000    1.0000       600\n",
      "          16     1.0000    1.0000    1.0000       600\n",
      "          17     0.9983    0.9983    0.9983       600\n",
      "          18     1.0000    1.0000    1.0000       600\n",
      "          19     0.9983    0.9983    0.9983       600\n",
      "          20     0.9983    1.0000    0.9992       600\n",
      "          21     1.0000    1.0000    1.0000       600\n",
      "          22     1.0000    0.9967    0.9983       600\n",
      "          23     1.0000    1.0000    1.0000       600\n",
      "          24     1.0000    0.9900    0.9950       600\n",
      "          25     1.0000    1.0000    1.0000       600\n",
      "          26     1.0000    1.0000    1.0000       600\n",
      "          27     1.0000    0.9933    0.9967       600\n",
      "          28     0.9967    1.0000    0.9983       600\n",
      "          29     0.9983    1.0000    0.9992       600\n",
      "          30     1.0000    0.9983    0.9992       600\n",
      "          31     0.9917    1.0000    0.9959       600\n",
      "          32     1.0000    1.0000    1.0000       600\n",
      "          33     1.0000    1.0000    1.0000       600\n",
      "          34     1.0000    1.0000    1.0000       600\n",
      "          35     1.0000    1.0000    1.0000       600\n",
      "\n",
      "    accuracy                         0.9992     21600\n",
      "   macro avg     0.9992    0.9992    0.9992     21600\n",
      "weighted avg     0.9992    0.9992    0.9992     21600\n",
      "\n",
      "===================================================\n",
      "\n",
      "============== AVERAGE PERFORMANCE ACROSS ALL MODELS ==============\n",
      "Average Train Accuracy: 97.87%\n",
      "Average Test Accuracy: 87.32%\n",
      "Average Test F1 Score: 0.8731\n",
      "Average Test Precision: 0.8759\n",
      "Average Test Recall: 0.8732\n",
      "===================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "hidden_layer_length = [\n",
    "    [512],\n",
    "    [512, 256],\n",
    "    [512, 256, 128],\n",
    "    [512, 256, 128, 64],\n",
    "]\n",
    "\n",
    "overall_results = []\n",
    "\n",
    "for h_len in hidden_layer_length:\n",
    "    print(f\"\\n========= Training Model with Hidden Layers: {h_len} =========\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = NeuralNetwork(M=32, n=3072, HiddenLayer=h_len, target_class=36, is_sigmoid=False, lr=0.01)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=20)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Accuracy\n",
    "    y_train_labels = np.argmax(y_train, axis=1)\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    train_acc = np.mean(y_train_pred == y_train_labels) * 100\n",
    "    test_acc = np.mean(y_test_pred == y_test_labels) * 100\n",
    "    \n",
    "    print(f\"Train Accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    # Macro metrics\n",
    "    def results_metrics(y_true_labels, y_pred):\n",
    "        f1 = f1_score(y_true_labels, y_pred, average='macro', zero_division=0)\n",
    "        precision = precision_score(y_true_labels, y_pred, average='macro', zero_division=0)\n",
    "        recall = recall_score(y_true_labels, y_pred, average='macro', zero_division=0)\n",
    "        return f1, precision, recall\n",
    "    \n",
    "    # Test metrics\n",
    "    f1_test, precision_test, recall_test = results_metrics(y_test_labels, y_test_pred)\n",
    "    print(f\"\\nüìä Test Macro Metrics:\")\n",
    "    print(f\"F1 Score: {f1_test:.4f}\")\n",
    "    print(f\"Precision: {precision_test:.4f}\")\n",
    "    print(f\"Recall: {recall_test:.4f}\")\n",
    "    \n",
    "    # Train metrics\n",
    "    f1_train, precision_train, recall_train = results_metrics(y_train_labels, y_train_pred)\n",
    "    print(f\"\\nüìä Train Macro Metrics:\")\n",
    "    print(f\"F1 Score: {f1_train:.4f}\")\n",
    "    print(f\"Precision: {precision_train:.4f}\")\n",
    "    print(f\"Recall: {recall_train:.4f}\")\n",
    "\n",
    "    # ‚úÖ Detailed per-class metrics for Test Data\n",
    "    print(\"\\nüîç Detailed Per-Class Metrics (Test Data):\")\n",
    "    print(classification_report(y_test_labels, y_test_pred, digits=4, zero_division=0))\n",
    "\n",
    "    # ‚úÖ Detailed per-class metrics for Train Data\n",
    "    print(\"\\nüîç Detailed Per-Class Metrics (Train Data):\")\n",
    "    print(classification_report(y_train_labels, y_train_pred, digits=4, zero_division=0))\n",
    "    \n",
    "    print(\"===================================================\")\n",
    "    \n",
    "    # Append results for averaging\n",
    "    overall_results.append({\n",
    "        \"layers\": h_len,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"f1_train\": f1_train,\n",
    "        \"precision_train\": precision_train,\n",
    "        \"recall_train\": recall_train,\n",
    "        \"f1_test\": f1_test,\n",
    "        \"precision_test\": precision_test,\n",
    "        \"recall_test\": recall_test\n",
    "    })\n",
    "\n",
    "# Compute average metrics across all models\n",
    "avg_train_acc = np.mean([res[\"train_acc\"] for res in overall_results])\n",
    "avg_test_acc = np.mean([res[\"test_acc\"] for res in overall_results])\n",
    "avg_f1 = np.mean([res[\"f1_test\"] for res in overall_results])\n",
    "avg_precision = np.mean([res[\"precision_test\"] for res in overall_results])\n",
    "avg_recall = np.mean([res[\"recall_test\"] for res in overall_results])\n",
    "\n",
    "print(\"\\n============== AVERAGE PERFORMANCE ACROSS ALL MODELS ==============\")\n",
    "print(f\"Average Train Accuracy: {avg_train_acc:.2f}%\")\n",
    "print(f\"Average Test Accuracy: {avg_test_acc:.2f}%\")\n",
    "print(f\"Average Test F1 Score: {avg_f1:.4f}\")\n",
    "print(f\"Average Test Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Test Recall: {avg_recall:.4f}\")\n",
    "print(\"===================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d4f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
