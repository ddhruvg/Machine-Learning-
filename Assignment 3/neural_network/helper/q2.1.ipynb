{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3181b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896bf505",
   "metadata": {},
   "source": [
    "Data Handling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af2ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def get_data(type=\"train\"):\n",
    "    data = []\n",
    "    label = []\n",
    "    for i in range(1, 37):\n",
    "        X = []\n",
    "        y = []\n",
    "        target = f\"{i:02d}\"  \n",
    "        train_data_path = f\"data/q2/{type}/{target}\"\n",
    "        \n",
    "        for img_name in os.listdir(train_data_path):\n",
    "            img_path = os.path.join(train_data_path, img_name)\n",
    "            \n",
    "            \n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            \n",
    "            img = cv2.resize(img, (32, 32))\n",
    "            img = img / 255.0 \n",
    "            img_flatten = img.flatten()\n",
    "            X.append(img_flatten)\n",
    "            y.append(i)\n",
    "\n",
    "        data.extend(X)\n",
    "        label.extend(y)\n",
    "\n",
    "    data = np.array(data)\n",
    "    label = np.array(label)\n",
    "    return data, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ec1cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , y_train = get_data(\"train\")\n",
    "X_test , y_test = get_data(\"test\")\n",
    "y_train  = np.eye(36)[y_train - 1]\n",
    "y_test  = np.eye(36)[y_test - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72d606",
   "metadata": {},
   "source": [
    "Perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dad609b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self,in_dimension , out_dimension , learning_rate = 0.001 , is_sigmoid = True,is_output = False):\n",
    "        self.net_grad = None\n",
    "        self.lr = learning_rate\n",
    "        self.O = None\n",
    "        limit = np.sqrt(6 / (in_dimension + out_dimension))\n",
    "        self.w = np.random.uniform(-limit, limit, (in_dimension + 1, out_dimension))\n",
    "\n",
    "        self.output_function = self.__sigmoid if is_sigmoid else self.__softmax\n",
    "        self.is_output = is_output\n",
    "\n",
    "    def __sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __softmax(self,x):\n",
    "        exp_x = np.exp(x - np.max(x , axis =1 , keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis = 1 , keepdims=True)\n",
    "\n",
    "    def __sigmoid_grad(self,x):\n",
    "        return x * (1-x)    \n",
    "\n",
    "    def next(self,X ):\n",
    "        X = np.hstack([np.ones((X.shape[0] , 1)),X])\n",
    "        self.net_grad = np.dot(X, self.w)\n",
    "        self.O = self.output_function(self.net_grad)\n",
    "        return self.O\n",
    "\n",
    "    def _prev(self, dO , O_down):\n",
    "        if not self.is_output:\n",
    "            new_grad = dO * self.__sigmoid_grad(self.O)\n",
    "            dw = np.dot(np.hstack([np.ones((O_down.shape[0], 1)), O_down]).T , new_grad) / O_down.shape[0]\n",
    "            d_O_down = np.dot(new_grad , self.w.T)\n",
    "            dO_prev =   d_O_down[:,1:]\n",
    "            self.w = self.w - self.lr * dw\n",
    "            return  dO_prev\n",
    "        \n",
    "            \n",
    "    \n",
    "\n",
    "class FinalLayer :\n",
    "    def __init__(self,in_dim , out_dim , lr = 0.001 ,is_softmax = True) :\n",
    "        self.lr = lr\n",
    "        limit = np.sqrt(6 / (in_dim + out_dim))\n",
    "        self.w = np.random.uniform(-limit, limit, (in_dim + 1, out_dim))\n",
    "\n",
    "        self.net_grad = None\n",
    "        self.O = None\n",
    "        self.output_function = self.__softmax if is_softmax else self.__sigmoid\n",
    "\n",
    "    def __sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __softmax(self,x):\n",
    "        exp_x = np.exp(x - np.max(x , axis =1 , keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis = 1 , keepdims=True)\n",
    "\n",
    "    def __sigmoid_grad(self,x):\n",
    "        return x * (1-x)  \n",
    "\n",
    "    def next(self , X) :\n",
    "        X = np.hstack([np.ones((X.shape[0] , 1)),X ])    \n",
    "        self.net_grad = np.dot(X, self.w)\n",
    "        self.O = self.output_function(self.net_grad)\n",
    "        return self.O\n",
    "    \n",
    "    def _prev(self, y_true , O_down)  :\n",
    "        new_grad = self.O - y_true\n",
    "        dw = np.dot(np.hstack([np.ones((O_down.shape[0], 1)), O_down]).T , new_grad) / y_true.shape[0]\n",
    "        d_O_down = np.dot(new_grad , self.w.T)\n",
    "        dO_prev =   d_O_down[:,1:]\n",
    "        self.w = self.w - self.lr * dw\n",
    "        return dO_prev\n",
    "    \n",
    "class NeuralNetwork:\n",
    "    def __init__(self, M = 32 , n = 3072 , HiddenLayer = [512 , 265 ]  , target_class = 36 , lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.layers = []\n",
    "        self.Batch_size = M\n",
    "        prev_dimn = n\n",
    "        for h in HiddenLayer:\n",
    "            layer = Layer(prev_dimn , h , is_sigmoid = True , learning_rate = self.lr)\n",
    "            self.layers.append(layer)\n",
    "            prev_dimn = h\n",
    "        self.output_layer = FinalLayer(prev_dimn , target_class , is_softmax = True , lr = self.lr)\n",
    "\n",
    "    def next(self,X) : \n",
    "        O_down = X \n",
    "        for layer in self.layers :\n",
    "            O_down = layer.next(O_down)\n",
    "\n",
    "        predictions = self.output_layer.next(O_down)\n",
    "        return predictions\n",
    "    \n",
    "    def corss_entropy_loss(self, y_true , y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        return - np.sum(y_true * np.log(y_pred + 1e-10 )) / m\n",
    "     \n",
    "    def _prev(self,Y , X) : \n",
    "        O = X \n",
    "        stored_outputs = [X]\n",
    "        for layer in self.layers :\n",
    "            O = layer.next(O)\n",
    "            stored_outputs.append(O)\n",
    "        predictions = self.output_layer.next(O)\n",
    "\n",
    "       \n",
    "\n",
    "        do_down = self.output_layer._prev(Y , stored_outputs[-1])\n",
    "        for i in range(len(self.layers)-1 , -1 , -1):\n",
    "            do_down = self.layers[i]._prev(do_down , stored_outputs[i])\n",
    "\n",
    "        return predictions  \n",
    "\n",
    "    def predict(self,X) :\n",
    "        predictions = self.next(X)\n",
    "        return np.argmax(predictions , axis = 1)   \n",
    "    \n",
    "    def fit(self , X , Y , epochs = 10 ):\n",
    "        for epoch in range(epochs) : \n",
    "            comb = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[comb]\n",
    "            Y_shuffled = Y[comb]\n",
    "            J = 0\n",
    "\n",
    "            for i in range(0, X.shape[0] , self.Batch_size):\n",
    "                X_batch = X_shuffled[i : i + self.Batch_size]\n",
    "                Y_batch = Y_shuffled[i : i + self.Batch_size]\n",
    "                predictions = self._prev(Y_batch , X_batch)\n",
    "                loss = self.corss_entropy_loss(Y_batch , predictions)\n",
    "                J += loss\n",
    "           \n",
    "            print(f\"Epoch {epoch + 1} / {epochs} , Loss : {J}\")\n",
    "\n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1f244fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 40 , Loss : 2409.2260649951395\n",
      "Epoch 2 / 40 , Loss : 2352.690445946514\n",
      "Epoch 3 / 40 , Loss : 2284.5034127556746\n",
      "Epoch 4 / 40 , Loss : 2185.0189945715047\n",
      "Epoch 5 / 40 , Loss : 2050.343502056293\n",
      "Epoch 6 / 40 , Loss : 1895.3162494674878\n",
      "Epoch 7 / 40 , Loss : 1745.271929570783\n",
      "Epoch 8 / 40 , Loss : 1610.2060360356575\n",
      "Epoch 9 / 40 , Loss : 1493.668258879202\n",
      "Epoch 10 / 40 , Loss : 1394.0021078690725\n",
      "Epoch 11 / 40 , Loss : 1308.76394627451\n",
      "Epoch 12 / 40 , Loss : 1237.9929297539309\n",
      "Epoch 13 / 40 , Loss : 1178.94935483754\n",
      "Epoch 14 / 40 , Loss : 1129.728854652895\n",
      "Epoch 15 / 40 , Loss : 1088.6353222242217\n",
      "Epoch 16 / 40 , Loss : 1054.090937004183\n",
      "Epoch 17 / 40 , Loss : 1024.0412185376429\n",
      "Epoch 18 / 40 , Loss : 998.4419160644235\n",
      "Epoch 19 / 40 , Loss : 975.9303788167264\n",
      "Epoch 20 / 40 , Loss : 955.9037674220816\n",
      "Epoch 21 / 40 , Loss : 937.6590348950805\n",
      "Epoch 22 / 40 , Loss : 921.4122868125509\n",
      "Epoch 23 / 40 , Loss : 906.1927614164491\n",
      "Epoch 24 / 40 , Loss : 892.5320170162543\n",
      "Epoch 25 / 40 , Loss : 879.0992952827745\n",
      "Epoch 26 / 40 , Loss : 866.9382582276542\n",
      "Epoch 27 / 40 , Loss : 855.6695290259605\n",
      "Epoch 28 / 40 , Loss : 844.6387791439325\n",
      "Epoch 29 / 40 , Loss : 834.4142637250872\n",
      "Epoch 30 / 40 , Loss : 824.4599084366979\n",
      "Epoch 31 / 40 , Loss : 814.6341359835711\n",
      "Epoch 32 / 40 , Loss : 805.2189483890536\n",
      "Epoch 33 / 40 , Loss : 796.0210998663199\n",
      "Epoch 34 / 40 , Loss : 787.5223085446254\n",
      "Epoch 35 / 40 , Loss : 778.9610867251664\n",
      "Epoch 36 / 40 , Loss : 771.1283074520902\n",
      "Epoch 37 / 40 , Loss : 762.6725226343062\n",
      "Epoch 38 / 40 , Loss : 754.644434292879\n",
      "Epoch 39 / 40 , Loss : 747.4969602080349\n",
      "Epoch 40 / 40 , Loss : 739.4737256533288\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(M = 32 , n = 3072 , HiddenLayer = [512 , 265 ]  , target_class = 36) \n",
    "model.fit(X_train , y_train , epochs = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b38f1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Hidden layer size: 1 ===\n",
      "Epoch 1 / 200 , Loss : 2426.437714335293\n",
      "Epoch 2 / 200 , Loss : 2410.071257007787\n",
      "Epoch 3 / 200 , Loss : 2398.2718749550168\n",
      "Epoch 4 / 200 , Loss : 2388.2231032576406\n",
      "Epoch 5 / 200 , Loss : 2379.2867552347543\n",
      "Epoch 6 / 200 , Loss : 2371.1355506258483\n",
      "Epoch 7 / 200 , Loss : 2363.558830812137\n",
      "Epoch 8 / 200 , Loss : 2356.4375748886564\n",
      "Epoch 9 / 200 , Loss : 2349.700468851641\n",
      "Epoch 10 / 200 , Loss : 2343.2710583316225\n",
      "Epoch 11 / 200 , Loss : 2337.1181527663543\n",
      "Epoch 12 / 200 , Loss : 2331.2683094492563\n",
      "Epoch 13 / 200 , Loss : 2325.667756978265\n",
      "Epoch 14 / 200 , Loss : 2320.269253991931\n",
      "Epoch 15 / 200 , Loss : 2315.1111950771046\n",
      "Epoch 16 / 200 , Loss : 2310.1977803668224\n",
      "Epoch 17 / 200 , Loss : 2305.4472060990092\n",
      "Epoch 18 / 200 , Loss : 2300.9288172219076\n",
      "Epoch 19 / 200 , Loss : 2296.596022632107\n",
      "Epoch 20 / 200 , Loss : 2292.439050285678\n",
      "Epoch 21 / 200 , Loss : 2288.5090266907105\n",
      "Epoch 22 / 200 , Loss : 2284.6751249569293\n",
      "Epoch 23 / 200 , Loss : 2281.033505861923\n",
      "Epoch 24 / 200 , Loss : 2277.559355368292\n",
      "Epoch 25 / 200 , Loss : 2274.2227446172583\n",
      "Epoch 26 / 200 , Loss : 2270.9868671698723\n",
      "Epoch 27 / 200 , Loss : 2267.93995675397\n",
      "Epoch 28 / 200 , Loss : 2264.9889175399917\n",
      "Epoch 29 / 200 , Loss : 2262.1350975428777\n",
      "Epoch 30 / 200 , Loss : 2259.4028470963003\n",
      "Epoch 31 / 200 , Loss : 2256.856371314005\n",
      "Epoch 32 / 200 , Loss : 2254.330539118454\n",
      "Epoch 33 / 200 , Loss : 2251.7855996546778\n",
      "Epoch 34 / 200 , Loss : 2249.5224601902864\n",
      "Epoch 35 / 200 , Loss : 2247.248143917703\n",
      "Epoch 36 / 200 , Loss : 2245.096684970784\n",
      "Epoch 37 / 200 , Loss : 2242.978435651758\n",
      "Epoch 38 / 200 , Loss : 2240.9630589215485\n",
      "Epoch 39 / 200 , Loss : 2238.9557699492925\n",
      "Epoch 40 / 200 , Loss : 2237.1260415601055\n",
      "Epoch 41 / 200 , Loss : 2235.2725039846996\n",
      "Epoch 42 / 200 , Loss : 2233.518254829728\n",
      "Epoch 43 / 200 , Loss : 2231.7221365184705\n",
      "Epoch 44 / 200 , Loss : 2230.1576676072\n",
      "Epoch 45 / 200 , Loss : 2228.5003690420344\n",
      "Epoch 46 / 200 , Loss : 2226.9912210815914\n",
      "Epoch 47 / 200 , Loss : 2225.4485369252343\n",
      "Epoch 48 / 200 , Loss : 2223.9510049375367\n",
      "Epoch 49 / 200 , Loss : 2222.5862723847918\n",
      "Epoch 50 / 200 , Loss : 2221.1086792480387\n",
      "Epoch 51 / 200 , Loss : 2219.78947690008\n",
      "Epoch 52 / 200 , Loss : 2218.391155281667\n",
      "Epoch 53 / 200 , Loss : 2217.0962236119353\n",
      "Epoch 54 / 200 , Loss : 2215.720815810986\n",
      "Epoch 55 / 200 , Loss : 2214.6198069469733\n",
      "Epoch 56 / 200 , Loss : 2213.4579166432377\n",
      "Epoch 57 / 200 , Loss : 2212.2452048199048\n",
      "Epoch 58 / 200 , Loss : 2211.084598279486\n",
      "Epoch 59 / 200 , Loss : 2210.005633292969\n",
      "Epoch 60 / 200 , Loss : 2208.9072121939253\n",
      "Epoch 61 / 200 , Loss : 2207.8001775866865\n",
      "Epoch 62 / 200 , Loss : 2206.7608029733656\n",
      "Epoch 63 / 200 , Loss : 2205.638532845673\n",
      "Epoch 64 / 200 , Loss : 2204.626224874878\n",
      "Epoch 65 / 200 , Loss : 2203.614897454335\n",
      "Epoch 66 / 200 , Loss : 2202.723703737005\n",
      "Epoch 67 / 200 , Loss : 2201.7582508692553\n",
      "Epoch 68 / 200 , Loss : 2200.663590023198\n",
      "Epoch 69 / 200 , Loss : 2199.8922888875213\n",
      "Epoch 70 / 200 , Loss : 2198.864634368037\n",
      "Epoch 71 / 200 , Loss : 2197.9844971767147\n",
      "Epoch 72 / 200 , Loss : 2197.140180327386\n",
      "Epoch 73 / 200 , Loss : 2196.2568307223796\n",
      "Epoch 74 / 200 , Loss : 2195.4062472654314\n",
      "Epoch 75 / 200 , Loss : 2194.489936413299\n",
      "Epoch 76 / 200 , Loss : 2193.6942357880935\n",
      "Epoch 77 / 200 , Loss : 2192.9711193304456\n",
      "Epoch 78 / 200 , Loss : 2192.0672429659685\n",
      "Epoch 79 / 200 , Loss : 2191.3210419525885\n",
      "Epoch 80 / 200 , Loss : 2190.490786550293\n",
      "Epoch 81 / 200 , Loss : 2189.743200476874\n",
      "Epoch 82 / 200 , Loss : 2188.8686181735866\n",
      "Epoch 83 / 200 , Loss : 2188.0740163551045\n",
      "Epoch 84 / 200 , Loss : 2187.345532725389\n",
      "Epoch 85 / 200 , Loss : 2186.6701975189917\n",
      "Epoch 86 / 200 , Loss : 2186.001264443672\n",
      "Epoch 87 / 200 , Loss : 2185.0904935800486\n",
      "Epoch 88 / 200 , Loss : 2184.4790010509796\n",
      "Epoch 89 / 200 , Loss : 2183.9226051736005\n",
      "Epoch 90 / 200 , Loss : 2183.152407499103\n",
      "Epoch 91 / 200 , Loss : 2182.4494492317763\n",
      "Epoch 92 / 200 , Loss : 2181.8157144988545\n",
      "Epoch 93 / 200 , Loss : 2181.0961556527377\n",
      "Epoch 94 / 200 , Loss : 2180.501586574645\n",
      "Epoch 95 / 200 , Loss : 2179.9597642976582\n",
      "Epoch 96 / 200 , Loss : 2179.087801290637\n",
      "Epoch 97 / 200 , Loss : 2178.57255696691\n",
      "Epoch 98 / 200 , Loss : 2177.9424973177165\n",
      "Epoch 99 / 200 , Loss : 2177.337438697586\n",
      "Epoch 100 / 200 , Loss : 2176.632768029767\n",
      "Epoch 101 / 200 , Loss : 2176.1354936174025\n",
      "Epoch 102 / 200 , Loss : 2175.5768432109912\n",
      "Epoch 103 / 200 , Loss : 2175.001476404332\n",
      "Epoch 104 / 200 , Loss : 2174.2578320935727\n",
      "Epoch 105 / 200 , Loss : 2173.6980845596704\n",
      "Epoch 106 / 200 , Loss : 2173.1122064287265\n",
      "Epoch 107 / 200 , Loss : 2172.6656882978177\n",
      "Epoch 108 / 200 , Loss : 2172.1315993523813\n",
      "Epoch 109 / 200 , Loss : 2171.5151920594744\n",
      "Epoch 110 / 200 , Loss : 2171.1402391077318\n",
      "Epoch 111 / 200 , Loss : 2170.468190842857\n",
      "Epoch 112 / 200 , Loss : 2169.805603616784\n",
      "Epoch 113 / 200 , Loss : 2169.4288307172396\n",
      "Epoch 114 / 200 , Loss : 2168.886753703247\n",
      "Epoch 115 / 200 , Loss : 2168.4001901833117\n",
      "Epoch 116 / 200 , Loss : 2167.948082027398\n",
      "Epoch 117 / 200 , Loss : 2167.4918435670015\n",
      "Epoch 118 / 200 , Loss : 2166.8755727349885\n",
      "Epoch 119 / 200 , Loss : 2166.4894653220163\n",
      "Epoch 120 / 200 , Loss : 2165.9224874455444\n",
      "Epoch 121 / 200 , Loss : 2165.493817218746\n",
      "Epoch 122 / 200 , Loss : 2165.1490484426004\n",
      "Epoch 123 / 200 , Loss : 2164.715697558306\n",
      "Epoch 124 / 200 , Loss : 2164.277568646677\n",
      "Epoch 125 / 200 , Loss : 2163.885184547377\n",
      "Epoch 126 / 200 , Loss : 2163.412358352189\n",
      "Epoch 127 / 200 , Loss : 2162.931381318901\n",
      "Epoch 128 / 200 , Loss : 2162.380664699179\n",
      "Epoch 129 / 200 , Loss : 2161.9384989699115\n",
      "Epoch 130 / 200 , Loss : 2161.599784485365\n",
      "Epoch 131 / 200 , Loss : 2161.0653860573\n",
      "Epoch 132 / 200 , Loss : 2160.816380047057\n",
      "Epoch 133 / 200 , Loss : 2160.5240611501376\n",
      "Epoch 134 / 200 , Loss : 2160.1391675600116\n",
      "Epoch 135 / 200 , Loss : 2159.529907388935\n",
      "Epoch 136 / 200 , Loss : 2159.280238643401\n",
      "Epoch 137 / 200 , Loss : 2158.7874768600964\n",
      "Epoch 138 / 200 , Loss : 2158.471950841517\n",
      "Epoch 139 / 200 , Loss : 2158.217471654172\n",
      "Epoch 140 / 200 , Loss : 2157.941761195476\n",
      "Epoch 141 / 200 , Loss : 2157.5034852903627\n",
      "Epoch 142 / 200 , Loss : 2156.9613830874837\n",
      "Epoch 143 / 200 , Loss : 2156.6766570132045\n",
      "Epoch 144 / 200 , Loss : 2156.3812671797405\n",
      "Epoch 145 / 200 , Loss : 2156.1837352686016\n",
      "Epoch 146 / 200 , Loss : 2155.795334282671\n",
      "Epoch 147 / 200 , Loss : 2155.288496606723\n",
      "Epoch 148 / 200 , Loss : 2155.11322498528\n",
      "Epoch 149 / 200 , Loss : 2154.807420962762\n",
      "Epoch 150 / 200 , Loss : 2154.3484814436447\n",
      "Epoch 151 / 200 , Loss : 2154.0045399484425\n",
      "Epoch 152 / 200 , Loss : 2153.795563358213\n",
      "Epoch 153 / 200 , Loss : 2153.428548700401\n",
      "Epoch 154 / 200 , Loss : 2153.0827562327736\n",
      "Epoch 155 / 200 , Loss : 2152.6902489809945\n",
      "Epoch 156 / 200 , Loss : 2152.5124210402023\n",
      "Epoch 157 / 200 , Loss : 2151.9838713560507\n",
      "Epoch 158 / 200 , Loss : 2151.731066707359\n",
      "Epoch 159 / 200 , Loss : 2151.732263903165\n",
      "Epoch 160 / 200 , Loss : 2151.4813611784793\n",
      "Epoch 161 / 200 , Loss : 2151.1372199455905\n",
      "Epoch 162 / 200 , Loss : 2150.60600711868\n",
      "Epoch 163 / 200 , Loss : 2150.6472435395453\n",
      "Epoch 164 / 200 , Loss : 2150.2415427936876\n",
      "Epoch 165 / 200 , Loss : 2149.8601946590147\n",
      "Epoch 166 / 200 , Loss : 2149.566566031294\n",
      "Epoch 167 / 200 , Loss : 2149.376292194134\n",
      "Epoch 168 / 200 , Loss : 2149.032370840658\n",
      "Epoch 169 / 200 , Loss : 2148.847284610738\n",
      "Epoch 170 / 200 , Loss : 2148.636891253896\n",
      "Epoch 171 / 200 , Loss : 2148.3140807898353\n",
      "Epoch 172 / 200 , Loss : 2148.2039847245624\n",
      "Epoch 173 / 200 , Loss : 2147.953008170326\n",
      "Epoch 174 / 200 , Loss : 2147.55723230103\n",
      "Epoch 175 / 200 , Loss : 2147.19076283038\n",
      "Epoch 176 / 200 , Loss : 2147.0888684701995\n",
      "Epoch 177 / 200 , Loss : 2146.999724685083\n",
      "Epoch 178 / 200 , Loss : 2146.5149719496585\n",
      "Epoch 179 / 200 , Loss : 2146.3417390671175\n",
      "Epoch 180 / 200 , Loss : 2146.112251256107\n",
      "Epoch 181 / 200 , Loss : 2146.091063980785\n",
      "Epoch 182 / 200 , Loss : 2145.521018197946\n",
      "Epoch 183 / 200 , Loss : 2145.5478191945626\n",
      "Epoch 184 / 200 , Loss : 2145.2221544978506\n",
      "Epoch 185 / 200 , Loss : 2145.015689641433\n",
      "Epoch 186 / 200 , Loss : 2144.696878319021\n",
      "Epoch 187 / 200 , Loss : 2144.450581459875\n",
      "Epoch 188 / 200 , Loss : 2144.2577245315165\n",
      "Epoch 189 / 200 , Loss : 2144.157123194601\n",
      "Epoch 190 / 200 , Loss : 2143.842104829178\n",
      "Epoch 191 / 200 , Loss : 2143.622690078192\n",
      "Epoch 192 / 200 , Loss : 2143.426395695099\n",
      "Epoch 193 / 200 , Loss : 2143.1784899251043\n",
      "Epoch 194 / 200 , Loss : 2142.9570486829043\n",
      "Epoch 195 / 200 , Loss : 2142.694154028589\n",
      "Epoch 196 / 200 , Loss : 2142.5072296338017\n",
      "Epoch 197 / 200 , Loss : 2142.403554122031\n",
      "Epoch 198 / 200 , Loss : 2142.432697849826\n",
      "Epoch 199 / 200 , Loss : 2141.8482433654076\n",
      "Epoch 200 / 200 , Loss : 2141.861112916352\n",
      "\n",
      "Accuracy:\n",
      "Train Accuracy: 6.88%\n",
      "Test Accuracy: 6.76%\n",
      "\n",
      "Per-Class Metrics (Test):\n",
      " Class     F1  Precision  Recall\n",
      "     1 0.0639     0.1215  0.0433\n",
      "     2 0.0000     0.0000  0.0000\n",
      "     3 0.0000     0.0000  0.0000\n",
      "     4 0.0000     0.0000  0.0000\n",
      "     5 0.0000     0.0000  0.0000\n",
      "     6 0.0746     0.0637  0.0900\n",
      "     7 0.0000     0.0000  0.0000\n",
      "     8 0.0404     0.0379  0.0433\n",
      "     9 0.0000     0.0000  0.0000\n",
      "    10 0.0079     0.0065  0.0100\n",
      "    11 0.0136     0.0211  0.0100\n",
      "    12 0.0125     0.0952  0.0067\n",
      "    13 0.0211     0.0286  0.0167\n",
      "    14 0.2050     0.1147  0.9600\n",
      "    15 0.0000     0.0000  0.0000\n",
      "    16 0.0000     0.0000  0.0000\n",
      "    17 0.0000     0.0000  0.0000\n",
      "    18 0.0494     0.0759  0.0367\n",
      "    19 0.0000     0.0000  0.0000\n",
      "    20 0.0352     0.0516  0.0267\n",
      "    21 0.0000     0.0000  0.0000\n",
      "    22 0.0000     0.0000  0.0000\n",
      "    23 0.0354     0.0431  0.0300\n",
      "    24 0.1112     0.0592  0.9200\n",
      "    25 0.0000     0.0000  0.0000\n",
      "    26 0.0000     0.0000  0.0000\n",
      "    27 0.1000     0.1438  0.0767\n",
      "    28 0.0412     0.0368  0.0467\n",
      "    29 0.0565     0.0436  0.0800\n",
      "    30 0.0000     0.0000  0.0000\n",
      "    31 0.0000     0.0000  0.0000\n",
      "    32 0.0000     0.0000  0.0000\n",
      "    33 0.0000     0.0000  0.0000\n",
      "    34 0.0000     0.0000  0.0000\n",
      "    35 0.0000     0.0000  0.0000\n",
      "    36 0.0351     0.0336  0.0367\n",
      "\n",
      "Average (Macro) Metrics (Test):\n",
      "F1: 0.0251, Precision: 0.0271, Recall: 0.0676\n",
      "---------------------------------------------------\n",
      "\n",
      "Per-Class Metrics (Train):\n",
      " Class     F1  Precision  Recall\n",
      "     1 0.0323     0.0631  0.0217\n",
      "     2 0.0000     0.0000  0.0000\n",
      "     3 0.0000     0.0000  0.0000\n",
      "     4 0.0000     0.0000  0.0000\n",
      "     5 0.0000     0.0000  0.0000\n",
      "     6 0.0761     0.0651  0.0917\n",
      "     7 0.0000     0.0000  0.0000\n",
      "     8 0.0511     0.0464  0.0567\n",
      "     9 0.0000     0.0000  0.0000\n",
      "    10 0.0191     0.0161  0.0233\n",
      "    11 0.0180     0.0277  0.0133\n",
      "    12 0.0032     0.0323  0.0017\n",
      "    13 0.0183     0.0236  0.0150\n",
      "    14 0.2091     0.1171  0.9717\n",
      "    15 0.0000     0.0000  0.0000\n",
      "    16 0.0000     0.0000  0.0000\n",
      "    17 0.0000     0.0000  0.0000\n",
      "    18 0.0337     0.0515  0.0250\n",
      "    19 0.0000     0.0000  0.0000\n",
      "    20 0.0368     0.0525  0.0283\n",
      "    21 0.0000     0.0000  0.0000\n",
      "    22 0.0000     0.0000  0.0000\n",
      "    23 0.0350     0.0458  0.0283\n",
      "    24 0.1121     0.0597  0.9200\n",
      "    25 0.0000     0.0000  0.0000\n",
      "    26 0.0000     0.0000  0.0000\n",
      "    27 0.0477     0.0681  0.0367\n",
      "    28 0.0652     0.0558  0.0783\n",
      "    29 0.0757     0.0586  0.1067\n",
      "    30 0.0000     0.0000  0.0000\n",
      "    31 0.0000     0.0000  0.0000\n",
      "    32 0.0000     0.0000  0.0000\n",
      "    33 0.0000     0.0000  0.0000\n",
      "    34 0.0000     0.0000  0.0000\n",
      "    35 0.0095     0.0882  0.0050\n",
      "    36 0.0465     0.0423  0.0517\n",
      "\n",
      "Average (Macro) Metrics (Train):\n",
      "F1: 0.0247, Precision: 0.0254, Recall: 0.0688\n",
      "===================================================\n",
      "\n",
      "=== Hidden layer size: 5 ===\n",
      "Epoch 1 / 200 , Loss : 2429.562045286988\n",
      "Epoch 2 / 200 , Loss : 2393.404251745345\n",
      "Epoch 3 / 200 , Loss : 2362.18767751428\n",
      "Epoch 4 / 200 , Loss : 2335.1261923464376\n",
      "Epoch 5 / 200 , Loss : 2309.8080319924816\n",
      "Epoch 6 / 200 , Loss : 2284.8612933508425\n",
      "Epoch 7 / 200 , Loss : 2259.8325094525453\n",
      "Epoch 8 / 200 , Loss : 2234.681251306525\n",
      "Epoch 9 / 200 , Loss : 2209.43780158042\n",
      "Epoch 10 / 200 , Loss : 2184.281593052439\n",
      "Epoch 11 / 200 , Loss : 2159.3756771117387\n",
      "Epoch 12 / 200 , Loss : 2134.8042362623987\n",
      "Epoch 13 / 200 , Loss : 2110.7790904417097\n",
      "Epoch 14 / 200 , Loss : 2087.2012536278116\n",
      "Epoch 15 / 200 , Loss : 2064.24780169633\n",
      "Epoch 16 / 200 , Loss : 2041.9812771130928\n",
      "Epoch 17 / 200 , Loss : 2020.2665441540787\n",
      "Epoch 18 / 200 , Loss : 1999.1956345459032\n",
      "Epoch 19 / 200 , Loss : 1978.8767396090366\n",
      "Epoch 20 / 200 , Loss : 1959.131943186297\n",
      "Epoch 21 / 200 , Loss : 1940.161259518708\n",
      "Epoch 22 / 200 , Loss : 1921.7412201035077\n",
      "Epoch 23 / 200 , Loss : 1904.0384878333389\n",
      "Epoch 24 / 200 , Loss : 1887.0440886931729\n",
      "Epoch 25 / 200 , Loss : 1870.690400746406\n",
      "Epoch 26 / 200 , Loss : 1854.8437835834989\n",
      "Epoch 27 / 200 , Loss : 1839.7281853219408\n",
      "Epoch 28 / 200 , Loss : 1825.2187572609348\n",
      "Epoch 29 / 200 , Loss : 1811.3127129733855\n",
      "Epoch 30 / 200 , Loss : 1798.0255191663477\n",
      "Epoch 31 / 200 , Loss : 1785.1206012272505\n",
      "Epoch 32 / 200 , Loss : 1772.8097922775837\n",
      "Epoch 33 / 200 , Loss : 1760.9823272042865\n",
      "Epoch 34 / 200 , Loss : 1749.467138894932\n",
      "Epoch 35 / 200 , Loss : 1738.6268485044084\n",
      "Epoch 36 / 200 , Loss : 1728.1812075681244\n",
      "Epoch 37 / 200 , Loss : 1718.0908386127758\n",
      "Epoch 38 / 200 , Loss : 1708.3892164471026\n",
      "Epoch 39 / 200 , Loss : 1699.047946921092\n",
      "Epoch 40 / 200 , Loss : 1690.3057610252283\n",
      "Epoch 41 / 200 , Loss : 1681.4563912293233\n",
      "Epoch 42 / 200 , Loss : 1673.1063929585052\n",
      "Epoch 43 / 200 , Loss : 1665.229327614953\n",
      "Epoch 44 / 200 , Loss : 1657.51545119035\n",
      "Epoch 45 / 200 , Loss : 1649.9782728444177\n",
      "Epoch 46 / 200 , Loss : 1642.9222476330258\n",
      "Epoch 47 / 200 , Loss : 1635.99256473265\n",
      "Epoch 48 / 200 , Loss : 1629.3094041305349\n",
      "Epoch 49 / 200 , Loss : 1622.7687113854538\n",
      "Epoch 50 / 200 , Loss : 1616.5646443400287\n",
      "Epoch 51 / 200 , Loss : 1610.635324426024\n",
      "Epoch 52 / 200 , Loss : 1604.6818637599995\n",
      "Epoch 53 / 200 , Loss : 1599.0800677517473\n",
      "Epoch 54 / 200 , Loss : 1593.7302523789922\n",
      "Epoch 55 / 200 , Loss : 1588.394322106448\n",
      "Epoch 56 / 200 , Loss : 1583.3282546278363\n",
      "Epoch 57 / 200 , Loss : 1578.3976571099108\n",
      "Epoch 58 / 200 , Loss : 1573.394329058766\n",
      "Epoch 59 / 200 , Loss : 1568.7930028408296\n",
      "Epoch 60 / 200 , Loss : 1564.3867507288448\n",
      "Epoch 61 / 200 , Loss : 1560.000516344098\n",
      "Epoch 62 / 200 , Loss : 1555.7157350775\n",
      "Epoch 63 / 200 , Loss : 1551.5753229960158\n",
      "Epoch 64 / 200 , Loss : 1547.3948906544174\n",
      "Epoch 65 / 200 , Loss : 1543.450412095877\n",
      "Epoch 66 / 200 , Loss : 1539.7679724449636\n",
      "Epoch 67 / 200 , Loss : 1535.882204434642\n",
      "Epoch 68 / 200 , Loss : 1532.2870125849806\n",
      "Epoch 69 / 200 , Loss : 1528.7518571216633\n",
      "Epoch 70 / 200 , Loss : 1525.3658155323653\n",
      "Epoch 71 / 200 , Loss : 1521.9899353655824\n",
      "Epoch 72 / 200 , Loss : 1518.5630884020347\n",
      "Epoch 73 / 200 , Loss : 1515.4615022517758\n",
      "Epoch 74 / 200 , Loss : 1512.4098409543162\n",
      "Epoch 75 / 200 , Loss : 1509.4354603490394\n",
      "Epoch 76 / 200 , Loss : 1506.5355533964942\n",
      "Epoch 77 / 200 , Loss : 1503.7915430734893\n",
      "Epoch 78 / 200 , Loss : 1500.8061168648098\n",
      "Epoch 79 / 200 , Loss : 1497.8613473620903\n",
      "Epoch 80 / 200 , Loss : 1495.4921357440908\n",
      "Epoch 81 / 200 , Loss : 1492.8050744345264\n",
      "Epoch 82 / 200 , Loss : 1489.953985212872\n",
      "Epoch 83 / 200 , Loss : 1487.3693560689462\n",
      "Epoch 84 / 200 , Loss : 1484.942538782362\n",
      "Epoch 85 / 200 , Loss : 1482.4115918610914\n",
      "Epoch 86 / 200 , Loss : 1479.9603681966232\n",
      "Epoch 87 / 200 , Loss : 1477.5602784603452\n",
      "Epoch 88 / 200 , Loss : 1475.3265408083228\n",
      "Epoch 89 / 200 , Loss : 1473.1942753325557\n",
      "Epoch 90 / 200 , Loss : 1471.0936662196898\n",
      "Epoch 91 / 200 , Loss : 1468.887005974048\n",
      "Epoch 92 / 200 , Loss : 1466.7948692091734\n",
      "Epoch 93 / 200 , Loss : 1464.583724250522\n",
      "Epoch 94 / 200 , Loss : 1462.3659815397782\n",
      "Epoch 95 / 200 , Loss : 1460.4238053526549\n",
      "Epoch 96 / 200 , Loss : 1458.3717961391671\n",
      "Epoch 97 / 200 , Loss : 1456.3672014033582\n",
      "Epoch 98 / 200 , Loss : 1454.5804978879028\n",
      "Epoch 99 / 200 , Loss : 1452.4053952915099\n",
      "Epoch 100 / 200 , Loss : 1450.7484762986073\n",
      "Epoch 101 / 200 , Loss : 1448.907684909039\n",
      "Epoch 102 / 200 , Loss : 1447.231598956773\n",
      "Epoch 103 / 200 , Loss : 1445.336753022336\n",
      "Epoch 104 / 200 , Loss : 1443.7776240097305\n",
      "Epoch 105 / 200 , Loss : 1441.7258111053286\n",
      "Epoch 106 / 200 , Loss : 1440.028686536024\n",
      "Epoch 107 / 200 , Loss : 1438.5053380268473\n",
      "Epoch 108 / 200 , Loss : 1436.69615711463\n",
      "Epoch 109 / 200 , Loss : 1435.211685332415\n",
      "Epoch 110 / 200 , Loss : 1433.5455262137752\n",
      "Epoch 111 / 200 , Loss : 1431.93895461211\n",
      "Epoch 112 / 200 , Loss : 1430.249329577595\n",
      "Epoch 113 / 200 , Loss : 1428.9826655087622\n",
      "Epoch 114 / 200 , Loss : 1427.6361390092566\n",
      "Epoch 115 / 200 , Loss : 1426.1255388729958\n",
      "Epoch 116 / 200 , Loss : 1424.5342052785409\n",
      "Epoch 117 / 200 , Loss : 1423.0609125403967\n",
      "Epoch 118 / 200 , Loss : 1421.596727180504\n",
      "Epoch 119 / 200 , Loss : 1420.505446125619\n",
      "Epoch 120 / 200 , Loss : 1419.1674242624622\n",
      "Epoch 121 / 200 , Loss : 1417.7830541856288\n",
      "Epoch 122 / 200 , Loss : 1416.0190906013856\n",
      "Epoch 123 / 200 , Loss : 1414.6665784965644\n",
      "Epoch 124 / 200 , Loss : 1413.4284005484137\n",
      "Epoch 125 / 200 , Loss : 1412.0437617859607\n",
      "Epoch 126 / 200 , Loss : 1410.8176814843048\n",
      "Epoch 127 / 200 , Loss : 1409.99689803123\n",
      "Epoch 128 / 200 , Loss : 1408.4161257987726\n",
      "Epoch 129 / 200 , Loss : 1407.2347201783364\n",
      "Epoch 130 / 200 , Loss : 1406.0714326579628\n",
      "Epoch 131 / 200 , Loss : 1405.246431889078\n",
      "Epoch 132 / 200 , Loss : 1403.5056492919475\n",
      "Epoch 133 / 200 , Loss : 1402.7510096374535\n",
      "Epoch 134 / 200 , Loss : 1401.6270054697527\n",
      "Epoch 135 / 200 , Loss : 1400.3896133797864\n",
      "Epoch 136 / 200 , Loss : 1398.9123013055957\n",
      "Epoch 137 / 200 , Loss : 1398.3525272998604\n",
      "Epoch 138 / 200 , Loss : 1396.9240607927022\n",
      "Epoch 139 / 200 , Loss : 1395.4489574596682\n",
      "Epoch 140 / 200 , Loss : 1395.098444726814\n",
      "Epoch 141 / 200 , Loss : 1393.486846724991\n",
      "Epoch 142 / 200 , Loss : 1392.6472476544452\n",
      "Epoch 143 / 200 , Loss : 1391.7857661512712\n",
      "Epoch 144 / 200 , Loss : 1390.85971918407\n",
      "Epoch 145 / 200 , Loss : 1390.0324699569442\n",
      "Epoch 146 / 200 , Loss : 1388.7287104583272\n",
      "Epoch 147 / 200 , Loss : 1387.7744519931186\n",
      "Epoch 148 / 200 , Loss : 1386.5972535140477\n",
      "Epoch 149 / 200 , Loss : 1386.0476868212918\n",
      "Epoch 150 / 200 , Loss : 1384.6193589880943\n",
      "Epoch 151 / 200 , Loss : 1383.537870556431\n",
      "Epoch 152 / 200 , Loss : 1382.8315668302425\n",
      "Epoch 153 / 200 , Loss : 1381.7051332955757\n",
      "Epoch 154 / 200 , Loss : 1380.863991917622\n",
      "Epoch 155 / 200 , Loss : 1380.585662857475\n",
      "Epoch 156 / 200 , Loss : 1379.0046125181016\n",
      "Epoch 157 / 200 , Loss : 1378.3913767678318\n",
      "Epoch 158 / 200 , Loss : 1377.2885518234389\n",
      "Epoch 159 / 200 , Loss : 1377.008775110761\n",
      "Epoch 160 / 200 , Loss : 1375.7219737778971\n",
      "Epoch 161 / 200 , Loss : 1375.3043927121469\n",
      "Epoch 162 / 200 , Loss : 1373.9508720421281\n",
      "Epoch 163 / 200 , Loss : 1373.0506574079402\n",
      "Epoch 164 / 200 , Loss : 1372.6560350313764\n",
      "Epoch 165 / 200 , Loss : 1370.6298470266963\n",
      "Epoch 166 / 200 , Loss : 1370.5340290106315\n",
      "Epoch 167 / 200 , Loss : 1369.927950703842\n",
      "Epoch 168 / 200 , Loss : 1369.2278231190476\n",
      "Epoch 169 / 200 , Loss : 1368.2416534931963\n",
      "Epoch 170 / 200 , Loss : 1367.3660667074805\n",
      "Epoch 171 / 200 , Loss : 1367.1613181273497\n",
      "Epoch 172 / 200 , Loss : 1366.0440841519587\n",
      "Epoch 173 / 200 , Loss : 1365.1346579826702\n",
      "Epoch 174 / 200 , Loss : 1364.8834854831696\n",
      "Epoch 175 / 200 , Loss : 1363.9743150578229\n",
      "Epoch 176 / 200 , Loss : 1362.7488215541107\n",
      "Epoch 177 / 200 , Loss : 1362.040971521974\n",
      "Epoch 178 / 200 , Loss : 1361.5582722637275\n",
      "Epoch 179 / 200 , Loss : 1361.1380206094077\n",
      "Epoch 180 / 200 , Loss : 1359.7627904850453\n",
      "Epoch 181 / 200 , Loss : 1359.4920978702237\n",
      "Epoch 182 / 200 , Loss : 1358.9462859140613\n",
      "Epoch 183 / 200 , Loss : 1358.07816814923\n",
      "Epoch 184 / 200 , Loss : 1357.7828096630499\n",
      "Epoch 185 / 200 , Loss : 1356.2963069039054\n",
      "Epoch 186 / 200 , Loss : 1355.8759894065377\n",
      "Epoch 187 / 200 , Loss : 1355.0639423008608\n",
      "Epoch 188 / 200 , Loss : 1354.5531766940312\n",
      "Epoch 189 / 200 , Loss : 1354.0501808221725\n",
      "Epoch 190 / 200 , Loss : 1353.1076159776364\n",
      "Epoch 191 / 200 , Loss : 1352.9360039343335\n",
      "Epoch 192 / 200 , Loss : 1352.1666731241187\n",
      "Epoch 193 / 200 , Loss : 1351.0075653872327\n",
      "Epoch 194 / 200 , Loss : 1351.0681857945751\n",
      "Epoch 195 / 200 , Loss : 1350.014894786548\n",
      "Epoch 196 / 200 , Loss : 1350.0767241139288\n",
      "Epoch 197 / 200 , Loss : 1349.119642819413\n",
      "Epoch 198 / 200 , Loss : 1348.7483680953894\n",
      "Epoch 199 / 200 , Loss : 1348.08895836152\n",
      "Epoch 200 / 200 , Loss : 1346.9374180215673\n",
      "\n",
      "Accuracy:\n",
      "Train Accuracy: 41.42%\n",
      "Test Accuracy: 37.22%\n",
      "\n",
      "Per-Class Metrics (Test):\n",
      " Class     F1  Precision  Recall\n",
      "     1 0.6033     0.5935  0.6133\n",
      "     2 0.0173     0.0652  0.0100\n",
      "     3 0.2609     0.3013  0.2300\n",
      "     4 0.2157     0.2619  0.1833\n",
      "     5 0.3541     0.4467  0.2933\n",
      "     6 0.3888     0.3483  0.4400\n",
      "     7 0.4103     0.3981  0.4233\n",
      "     8 0.5065     0.4135  0.6533\n",
      "     9 0.6500     0.5816  0.7367\n",
      "    10 0.5594     0.4371  0.7767\n",
      "    11 0.6144     0.5054  0.7833\n",
      "    12 0.5753     0.5772  0.5733\n",
      "    13 0.4618     0.4132  0.5233\n",
      "    14 0.4567     0.3975  0.5367\n",
      "    15 0.2015     0.2236  0.1833\n",
      "    16 0.3084     0.3006  0.3167\n",
      "    17 0.3140     0.2495  0.4233\n",
      "    18 0.2562     0.3004  0.2233\n",
      "    19 0.1589     0.2656  0.1133\n",
      "    20 0.3636     0.3099  0.4400\n",
      "    21 0.3899     0.3366  0.4633\n",
      "    22 0.5910     0.5245  0.6767\n",
      "    23 0.2119     0.3137  0.1600\n",
      "    24 0.3205     0.4464  0.2500\n",
      "    25 0.3041     0.3016  0.3067\n",
      "    26 0.0674     0.1512  0.0433\n",
      "    27 0.3343     0.2944  0.3867\n",
      "    28 0.2506     0.3810  0.1867\n",
      "    29 0.1474     0.2804  0.1000\n",
      "    30 0.4418     0.3634  0.5633\n",
      "    31 0.4011     0.3480  0.4733\n",
      "    32 0.2211     0.2190  0.2233\n",
      "    33 0.3213     0.3504  0.2967\n",
      "    34 0.3376     0.3232  0.3533\n",
      "    35 0.1091     0.2471  0.0700\n",
      "    36 0.3575     0.3458  0.3700\n",
      "\n",
      "Average (Macro) Metrics (Test):\n",
      "F1: 0.3468, Precision: 0.3505, Recall: 0.3722\n",
      "---------------------------------------------------\n",
      "\n",
      "Per-Class Metrics (Train):\n",
      " Class     F1  Precision  Recall\n",
      "     1 0.6577     0.6605  0.6550\n",
      "     2 0.0184     0.1154  0.0100\n",
      "     3 0.2782     0.3060  0.2550\n",
      "     4 0.2318     0.2823  0.1967\n",
      "     5 0.3996     0.4915  0.3367\n",
      "     6 0.4417     0.3945  0.5017\n",
      "     7 0.5016     0.4816  0.5233\n",
      "     8 0.5690     0.4909  0.6767\n",
      "     9 0.6980     0.6172  0.8033\n",
      "    10 0.5552     0.4352  0.7667\n",
      "    11 0.6033     0.4867  0.7933\n",
      "    12 0.5987     0.5957  0.6017\n",
      "    13 0.5299     0.4755  0.5983\n",
      "    14 0.5071     0.4428  0.5933\n",
      "    15 0.2757     0.2894  0.2633\n",
      "    16 0.3508     0.3407  0.3617\n",
      "    17 0.3451     0.2780  0.4550\n",
      "    18 0.2644     0.2996  0.2367\n",
      "    19 0.1364     0.2534  0.0933\n",
      "    20 0.4225     0.3590  0.5133\n",
      "    21 0.4446     0.3803  0.5350\n",
      "    22 0.6822     0.6199  0.7583\n",
      "    23 0.2656     0.3891  0.2017\n",
      "    24 0.3785     0.4986  0.3050\n",
      "    25 0.3276     0.3236  0.3317\n",
      "    26 0.0780     0.1775  0.0500\n",
      "    27 0.3913     0.3567  0.4333\n",
      "    28 0.2568     0.4036  0.1883\n",
      "    29 0.2086     0.3953  0.1417\n",
      "    30 0.4246     0.3520  0.5350\n",
      "    31 0.4756     0.4219  0.5450\n",
      "    32 0.2790     0.2732  0.2850\n",
      "    33 0.3966     0.4547  0.3517\n",
      "    34 0.4048     0.3656  0.4533\n",
      "    35 0.1636     0.3706  0.1050\n",
      "    36 0.4111     0.3750  0.4550\n",
      "\n",
      "Average (Macro) Metrics (Train):\n",
      "F1: 0.3882, Precision: 0.3959, Recall: 0.4142\n",
      "===================================================\n",
      "\n",
      "=== Hidden layer size: 10 ===\n",
      "Epoch 1 / 200 , Loss : 2400.793538141322\n",
      "Epoch 2 / 200 , Loss : 2313.565113254409\n",
      "Epoch 3 / 200 , Loss : 2245.047127084424\n",
      "Epoch 4 / 200 , Loss : 2185.875083540324\n",
      "Epoch 5 / 200 , Loss : 2131.7283180742847\n",
      "Epoch 6 / 200 , Loss : 2080.891656680098\n",
      "Epoch 7 / 200 , Loss : 2032.833288051999\n",
      "Epoch 8 / 200 , Loss : 1987.411256482262\n",
      "Epoch 9 / 200 , Loss : 1944.564049719935\n",
      "Epoch 10 / 200 , Loss : 1904.157190019509\n",
      "Epoch 11 / 200 , Loss : 1866.0384407891813\n",
      "Epoch 12 / 200 , Loss : 1830.2588739733528\n",
      "Epoch 13 / 200 , Loss : 1796.6465377582253\n",
      "Epoch 14 / 200 , Loss : 1764.9931264171973\n",
      "Epoch 15 / 200 , Loss : 1735.0969083695438\n",
      "Epoch 16 / 200 , Loss : 1707.0400469081158\n",
      "Epoch 17 / 200 , Loss : 1680.5290267032892\n",
      "Epoch 18 / 200 , Loss : 1655.4932245853524\n",
      "Epoch 19 / 200 , Loss : 1631.867931757898\n",
      "Epoch 20 / 200 , Loss : 1609.3474655353027\n",
      "Epoch 21 / 200 , Loss : 1588.1679773978296\n",
      "Epoch 22 / 200 , Loss : 1567.7933255228243\n",
      "Epoch 23 / 200 , Loss : 1548.6727988583739\n",
      "Epoch 24 / 200 , Loss : 1530.4387217516462\n",
      "Epoch 25 / 200 , Loss : 1513.1505553547186\n",
      "Epoch 26 / 200 , Loss : 1496.3409197689452\n",
      "Epoch 27 / 200 , Loss : 1480.5604113769455\n",
      "Epoch 28 / 200 , Loss : 1465.2348253859952\n",
      "Epoch 29 / 200 , Loss : 1450.9251534294378\n",
      "Epoch 30 / 200 , Loss : 1437.0878582037544\n",
      "Epoch 31 / 200 , Loss : 1423.8539683451886\n",
      "Epoch 32 / 200 , Loss : 1410.978664432936\n",
      "Epoch 33 / 200 , Loss : 1398.822171897854\n",
      "Epoch 34 / 200 , Loss : 1387.189008187019\n",
      "Epoch 35 / 200 , Loss : 1375.8978369990407\n",
      "Epoch 36 / 200 , Loss : 1364.9579082822927\n",
      "Epoch 37 / 200 , Loss : 1354.6264123335243\n",
      "Epoch 38 / 200 , Loss : 1344.5178325749168\n",
      "Epoch 39 / 200 , Loss : 1334.9185430860975\n",
      "Epoch 40 / 200 , Loss : 1325.545678187228\n",
      "Epoch 41 / 200 , Loss : 1316.8057884256825\n",
      "Epoch 42 / 200 , Loss : 1308.1808382641216\n",
      "Epoch 43 / 200 , Loss : 1299.9422137520462\n",
      "Epoch 44 / 200 , Loss : 1291.8831161060327\n",
      "Epoch 45 / 200 , Loss : 1284.0807381830205\n",
      "Epoch 46 / 200 , Loss : 1276.562929694632\n",
      "Epoch 47 / 200 , Loss : 1269.3190107369717\n",
      "Epoch 48 / 200 , Loss : 1262.4220986207583\n",
      "Epoch 49 / 200 , Loss : 1255.65786302429\n",
      "Epoch 50 / 200 , Loss : 1249.2225883313286\n",
      "Epoch 51 / 200 , Loss : 1242.732957684009\n",
      "Epoch 52 / 200 , Loss : 1236.7040649880944\n",
      "Epoch 53 / 200 , Loss : 1230.6183507662295\n",
      "Epoch 54 / 200 , Loss : 1224.9959531002098\n",
      "Epoch 55 / 200 , Loss : 1219.4558761196995\n",
      "Epoch 56 / 200 , Loss : 1214.0479165050724\n",
      "Epoch 57 / 200 , Loss : 1208.6244127886453\n",
      "Epoch 58 / 200 , Loss : 1203.5218978116272\n",
      "Epoch 59 / 200 , Loss : 1198.4963445527378\n",
      "Epoch 60 / 200 , Loss : 1193.6671493058843\n",
      "Epoch 61 / 200 , Loss : 1188.8035864784833\n",
      "Epoch 62 / 200 , Loss : 1184.6519500531306\n",
      "Epoch 63 / 200 , Loss : 1180.004030644489\n",
      "Epoch 64 / 200 , Loss : 1175.8232954261402\n",
      "Epoch 65 / 200 , Loss : 1171.2582022807092\n",
      "Epoch 66 / 200 , Loss : 1167.2373719524555\n",
      "Epoch 67 / 200 , Loss : 1163.2497418423793\n",
      "Epoch 68 / 200 , Loss : 1159.2273821824158\n",
      "Epoch 69 / 200 , Loss : 1155.1914758764674\n",
      "Epoch 70 / 200 , Loss : 1151.6880215694257\n",
      "Epoch 71 / 200 , Loss : 1147.6531649174121\n",
      "Epoch 72 / 200 , Loss : 1144.3667594612052\n",
      "Epoch 73 / 200 , Loss : 1140.9234418091135\n",
      "Epoch 74 / 200 , Loss : 1137.225443662802\n",
      "Epoch 75 / 200 , Loss : 1134.0020654274754\n",
      "Epoch 76 / 200 , Loss : 1130.501760740606\n",
      "Epoch 77 / 200 , Loss : 1127.4901711416467\n",
      "Epoch 78 / 200 , Loss : 1124.2051067104994\n",
      "Epoch 79 / 200 , Loss : 1121.0420971549047\n",
      "Epoch 80 / 200 , Loss : 1118.2309905750406\n",
      "Epoch 81 / 200 , Loss : 1115.0648331945338\n",
      "Epoch 82 / 200 , Loss : 1112.0741012432547\n",
      "Epoch 83 / 200 , Loss : 1109.2074010750027\n",
      "Epoch 84 / 200 , Loss : 1106.5172075170594\n",
      "Epoch 85 / 200 , Loss : 1103.8053143898633\n",
      "Epoch 86 / 200 , Loss : 1100.7795652075754\n",
      "Epoch 87 / 200 , Loss : 1098.3024132238854\n",
      "Epoch 88 / 200 , Loss : 1095.6820932428054\n",
      "Epoch 89 / 200 , Loss : 1093.022094369403\n",
      "Epoch 90 / 200 , Loss : 1090.748231218293\n",
      "Epoch 91 / 200 , Loss : 1087.9280897083763\n",
      "Epoch 92 / 200 , Loss : 1085.7929515446897\n",
      "Epoch 93 / 200 , Loss : 1083.254862093339\n",
      "Epoch 94 / 200 , Loss : 1081.1279450024074\n",
      "Epoch 95 / 200 , Loss : 1078.5708162337983\n",
      "Epoch 96 / 200 , Loss : 1076.2698736126\n",
      "Epoch 97 / 200 , Loss : 1073.9312143721716\n",
      "Epoch 98 / 200 , Loss : 1072.1052452208182\n",
      "Epoch 99 / 200 , Loss : 1069.5978304851453\n",
      "Epoch 100 / 200 , Loss : 1067.3032644254163\n",
      "Epoch 101 / 200 , Loss : 1065.374633161732\n",
      "Epoch 102 / 200 , Loss : 1063.0625851019577\n",
      "Epoch 103 / 200 , Loss : 1060.9101927394136\n",
      "Epoch 104 / 200 , Loss : 1059.34604299526\n",
      "Epoch 105 / 200 , Loss : 1057.0267501534804\n",
      "Epoch 106 / 200 , Loss : 1055.5200579550876\n",
      "Epoch 107 / 200 , Loss : 1053.252585810605\n",
      "Epoch 108 / 200 , Loss : 1051.6127958637096\n",
      "Epoch 109 / 200 , Loss : 1049.337701138891\n",
      "Epoch 110 / 200 , Loss : 1047.8464920279796\n",
      "Epoch 111 / 200 , Loss : 1045.5404413777603\n",
      "Epoch 112 / 200 , Loss : 1043.8450100615155\n",
      "Epoch 113 / 200 , Loss : 1042.2479744138263\n",
      "Epoch 114 / 200 , Loss : 1040.2561085751636\n",
      "Epoch 115 / 200 , Loss : 1038.7802727340247\n",
      "Epoch 116 / 200 , Loss : 1037.1664030058766\n",
      "Epoch 117 / 200 , Loss : 1034.9959857443487\n",
      "Epoch 118 / 200 , Loss : 1033.7276276479763\n",
      "Epoch 119 / 200 , Loss : 1031.7711014855581\n",
      "Epoch 120 / 200 , Loss : 1030.3811526096501\n",
      "Epoch 121 / 200 , Loss : 1028.67204845246\n",
      "Epoch 122 / 200 , Loss : 1027.0578093125944\n",
      "Epoch 123 / 200 , Loss : 1025.4717905320451\n",
      "Epoch 124 / 200 , Loss : 1023.9079516157485\n",
      "Epoch 125 / 200 , Loss : 1022.1609707430796\n",
      "Epoch 126 / 200 , Loss : 1020.7113990050441\n",
      "Epoch 127 / 200 , Loss : 1019.5916884220385\n",
      "Epoch 128 / 200 , Loss : 1017.7176919394653\n",
      "Epoch 129 / 200 , Loss : 1016.754457913981\n",
      "Epoch 130 / 200 , Loss : 1015.1605773968923\n",
      "Epoch 131 / 200 , Loss : 1013.3635948435036\n",
      "Epoch 132 / 200 , Loss : 1012.5109901261793\n",
      "Epoch 133 / 200 , Loss : 1010.829493547896\n",
      "Epoch 134 / 200 , Loss : 1009.3988261704335\n",
      "Epoch 135 / 200 , Loss : 1008.1015353990032\n",
      "Epoch 136 / 200 , Loss : 1007.199995317856\n",
      "Epoch 137 / 200 , Loss : 1005.6774898057496\n",
      "Epoch 138 / 200 , Loss : 1004.4903983740303\n",
      "Epoch 139 / 200 , Loss : 1002.6690226362828\n",
      "Epoch 140 / 200 , Loss : 1001.7654666720836\n",
      "Epoch 141 / 200 , Loss : 1000.1861496105552\n",
      "Epoch 142 / 200 , Loss : 999.1805448906357\n",
      "Epoch 143 / 200 , Loss : 998.0079557201303\n",
      "Epoch 144 / 200 , Loss : 996.6434460942652\n",
      "Epoch 145 / 200 , Loss : 995.5466217049733\n",
      "Epoch 146 / 200 , Loss : 994.2196718687951\n",
      "Epoch 147 / 200 , Loss : 993.3054367644338\n",
      "Epoch 148 / 200 , Loss : 991.7836939362394\n",
      "Epoch 149 / 200 , Loss : 990.5967852691932\n",
      "Epoch 150 / 200 , Loss : 989.4744977774559\n",
      "Epoch 151 / 200 , Loss : 988.2942214320251\n",
      "Epoch 152 / 200 , Loss : 987.2744179078827\n",
      "Epoch 153 / 200 , Loss : 986.2944350188035\n",
      "Epoch 154 / 200 , Loss : 984.7811852455654\n",
      "Epoch 155 / 200 , Loss : 984.0426393542931\n",
      "Epoch 156 / 200 , Loss : 982.9852345014164\n",
      "Epoch 157 / 200 , Loss : 982.1221522481085\n",
      "Epoch 158 / 200 , Loss : 980.9668611671909\n",
      "Epoch 159 / 200 , Loss : 979.7981582534509\n",
      "Epoch 160 / 200 , Loss : 978.8314334655148\n",
      "Epoch 161 / 200 , Loss : 977.6353665209517\n",
      "Epoch 162 / 200 , Loss : 976.6286705555489\n",
      "Epoch 163 / 200 , Loss : 975.7836041216108\n",
      "Epoch 164 / 200 , Loss : 974.4845058079006\n",
      "Epoch 165 / 200 , Loss : 973.5639064551244\n",
      "Epoch 166 / 200 , Loss : 972.6209727406879\n",
      "Epoch 167 / 200 , Loss : 971.6007052131735\n",
      "Epoch 168 / 200 , Loss : 970.6760775298281\n",
      "Epoch 169 / 200 , Loss : 969.9612791076831\n",
      "Epoch 170 / 200 , Loss : 968.7138764667426\n",
      "Epoch 171 / 200 , Loss : 967.9621828650646\n",
      "Epoch 172 / 200 , Loss : 967.2310935345048\n",
      "Epoch 173 / 200 , Loss : 966.5582305528435\n",
      "Epoch 174 / 200 , Loss : 965.2931042001499\n",
      "Epoch 175 / 200 , Loss : 964.1013837496373\n",
      "Epoch 176 / 200 , Loss : 963.6284964940629\n",
      "Epoch 177 / 200 , Loss : 962.2929848535741\n",
      "Epoch 178 / 200 , Loss : 961.4345850325293\n",
      "Epoch 179 / 200 , Loss : 960.7808231606119\n",
      "Epoch 180 / 200 , Loss : 959.7706415334551\n",
      "Epoch 181 / 200 , Loss : 959.1028457791936\n",
      "Epoch 182 / 200 , Loss : 957.7399152090346\n",
      "Epoch 183 / 200 , Loss : 957.3739899975801\n",
      "Epoch 184 / 200 , Loss : 956.2294811104243\n",
      "Epoch 185 / 200 , Loss : 955.640686491512\n",
      "Epoch 186 / 200 , Loss : 954.8144241371849\n",
      "Epoch 187 / 200 , Loss : 954.1923828561187\n",
      "Epoch 188 / 200 , Loss : 952.8386876253642\n",
      "Epoch 189 / 200 , Loss : 952.2939025708577\n",
      "Epoch 190 / 200 , Loss : 951.4177128008794\n",
      "Epoch 191 / 200 , Loss : 950.5136563464179\n",
      "Epoch 192 / 200 , Loss : 949.9828940204698\n",
      "Epoch 193 / 200 , Loss : 949.2764443798856\n",
      "Epoch 194 / 200 , Loss : 948.508464623569\n",
      "Epoch 195 / 200 , Loss : 947.3484993295872\n",
      "Epoch 196 / 200 , Loss : 946.9203491766821\n",
      "Epoch 197 / 200 , Loss : 946.0283655047125\n",
      "Epoch 198 / 200 , Loss : 945.3837601732723\n",
      "Epoch 199 / 200 , Loss : 945.1983063358589\n",
      "Epoch 200 / 200 , Loss : 943.8034851473044\n",
      "\n",
      "Accuracy:\n",
      "Train Accuracy: 61.58%\n",
      "Test Accuracy: 54.92%\n",
      "\n",
      "Per-Class Metrics (Test):\n",
      " Class     F1  Precision  Recall\n",
      "     1 0.6820     0.6299  0.7433\n",
      "     2 0.3519     0.3958  0.3167\n",
      "     3 0.6091     0.5583  0.6700\n",
      "     4 0.4924     0.5017  0.4833\n",
      "     5 0.4391     0.4917  0.3967\n",
      "     6 0.5887     0.6518  0.5367\n",
      "     7 0.4721     0.4645  0.4800\n",
      "     8 0.6667     0.6601  0.6733\n",
      "     9 0.7852     0.7905  0.7800\n",
      "    10 0.6646     0.6325  0.7000\n",
      "    11 0.7532     0.7169  0.7933\n",
      "    12 0.6556     0.6545  0.6567\n",
      "    13 0.5427     0.5559  0.5300\n",
      "    14 0.6364     0.6006  0.6767\n",
      "    15 0.5657     0.5226  0.6167\n",
      "    16 0.5836     0.5074  0.6867\n",
      "    17 0.4065     0.3968  0.4167\n",
      "    18 0.4441     0.4591  0.4300\n",
      "    19 0.5870     0.6429  0.5400\n",
      "    20 0.5442     0.5668  0.5233\n",
      "    21 0.5167     0.4954  0.5400\n",
      "    22 0.7488     0.7200  0.7800\n",
      "    23 0.5166     0.5785  0.4667\n",
      "    24 0.5997     0.5749  0.6267\n",
      "    25 0.4485     0.4901  0.4133\n",
      "    26 0.2922     0.3392  0.2567\n",
      "    27 0.6730     0.6424  0.7067\n",
      "    28 0.5284     0.5878  0.4800\n",
      "    29 0.4031     0.4815  0.3467\n",
      "    30 0.4724     0.4478  0.5000\n",
      "    31 0.5629     0.4925  0.6567\n",
      "    32 0.2944     0.3724  0.2433\n",
      "    33 0.4832     0.4463  0.5267\n",
      "    34 0.5637     0.5396  0.5900\n",
      "    35 0.4473     0.4920  0.4100\n",
      "    36 0.5518     0.5291  0.5767\n",
      "\n",
      "Average (Macro) Metrics (Test):\n",
      "F1: 0.5437, Precision: 0.5453, Recall: 0.5492\n",
      "---------------------------------------------------\n",
      "\n",
      "Per-Class Metrics (Train):\n",
      " Class     F1  Precision  Recall\n",
      "     1 0.7486     0.7225  0.7767\n",
      "     2 0.4537     0.5104  0.4083\n",
      "     3 0.6840     0.6335  0.7433\n",
      "     4 0.5295     0.5273  0.5317\n",
      "     5 0.4688     0.5129  0.4317\n",
      "     6 0.6508     0.6930  0.6133\n",
      "     7 0.5483     0.5340  0.5633\n",
      "     8 0.7320     0.7357  0.7283\n",
      "     9 0.8122     0.8247  0.8000\n",
      "    10 0.7002     0.6939  0.7067\n",
      "    11 0.7885     0.7391  0.8450\n",
      "    12 0.7532     0.7615  0.7450\n",
      "    13 0.6289     0.6197  0.6383\n",
      "    14 0.7367     0.7254  0.7483\n",
      "    15 0.6533     0.6063  0.7083\n",
      "    16 0.6469     0.5716  0.7450\n",
      "    17 0.4996     0.4807  0.5200\n",
      "    18 0.4505     0.4902  0.4167\n",
      "    19 0.5973     0.6535  0.5500\n",
      "    20 0.6678     0.6723  0.6633\n",
      "    21 0.5828     0.5398  0.6333\n",
      "    22 0.7880     0.7686  0.8083\n",
      "    23 0.6013     0.6920  0.5317\n",
      "    24 0.6629     0.6365  0.6917\n",
      "    25 0.5507     0.5637  0.5383\n",
      "    26 0.3025     0.3584  0.2617\n",
      "    27 0.6997     0.6447  0.7650\n",
      "    28 0.6235     0.6764  0.5783\n",
      "    29 0.4814     0.5441  0.4317\n",
      "    30 0.5080     0.4671  0.5567\n",
      "    31 0.6347     0.5771  0.7050\n",
      "    32 0.3701     0.4704  0.3050\n",
      "    33 0.5731     0.5647  0.5817\n",
      "    34 0.6405     0.6053  0.6800\n",
      "    35 0.5657     0.6250  0.5167\n",
      "    36 0.6599     0.6228  0.7017\n",
      "\n",
      "Average (Macro) Metrics (Train):\n",
      "F1: 0.6110, Precision: 0.6129, Recall: 0.6158\n",
      "===================================================\n",
      "\n",
      "=== Hidden layer size: 50 ===\n",
      "Epoch 1 / 200 , Loss : 2290.8970495928675\n",
      "Epoch 2 / 200 , Loss : 2044.680804194268\n",
      "Epoch 3 / 200 , Loss : 1865.9356099687477\n",
      "Epoch 4 / 200 , Loss : 1720.7550599288702\n",
      "Epoch 5 / 200 , Loss : 1599.6926909945093\n",
      "Epoch 6 / 200 , Loss : 1497.9980335425912\n",
      "Epoch 7 / 200 , Loss : 1411.6259931846637\n",
      "Epoch 8 / 200 , Loss : 1338.5428788201414\n",
      "Epoch 9 / 200 , Loss : 1275.72011468605\n",
      "Epoch 10 / 200 , Loss : 1222.2599566459653\n",
      "Epoch 11 / 200 , Loss : 1175.5172514669168\n",
      "Epoch 12 / 200 , Loss : 1134.982526111262\n",
      "Epoch 13 / 200 , Loss : 1099.215401582079\n",
      "Epoch 14 / 200 , Loss : 1067.5270792400177\n",
      "Epoch 15 / 200 , Loss : 1039.1896067013292\n",
      "Epoch 16 / 200 , Loss : 1013.615977039326\n",
      "Epoch 17 / 200 , Loss : 990.6023579215765\n",
      "Epoch 18 / 200 , Loss : 969.5740355240221\n",
      "Epoch 19 / 200 , Loss : 950.409019739351\n",
      "Epoch 20 / 200 , Loss : 932.7955111787486\n",
      "Epoch 21 / 200 , Loss : 916.455292898538\n",
      "Epoch 22 / 200 , Loss : 901.2689106209208\n",
      "Epoch 23 / 200 , Loss : 887.3833946583864\n",
      "Epoch 24 / 200 , Loss : 873.9782456699036\n",
      "Epoch 25 / 200 , Loss : 861.526782430153\n",
      "Epoch 26 / 200 , Loss : 850.0553101484902\n",
      "Epoch 27 / 200 , Loss : 839.0500947061237\n",
      "Epoch 28 / 200 , Loss : 828.5390412781172\n",
      "Epoch 29 / 200 , Loss : 818.8471395916337\n",
      "Epoch 30 / 200 , Loss : 809.3123776145491\n",
      "Epoch 31 / 200 , Loss : 800.2585851061035\n",
      "Epoch 32 / 200 , Loss : 791.6513561262885\n",
      "Epoch 33 / 200 , Loss : 783.4433364276613\n",
      "Epoch 34 / 200 , Loss : 775.4072309465\n",
      "Epoch 35 / 200 , Loss : 768.0017149904952\n",
      "Epoch 36 / 200 , Loss : 760.5950358015056\n",
      "Epoch 37 / 200 , Loss : 753.6077263179185\n",
      "Epoch 38 / 200 , Loss : 746.6445003040486\n",
      "Epoch 39 / 200 , Loss : 740.2837234425384\n",
      "Epoch 40 / 200 , Loss : 733.699719037023\n",
      "Epoch 41 / 200 , Loss : 727.6347564460258\n",
      "Epoch 42 / 200 , Loss : 721.569124017378\n",
      "Epoch 43 / 200 , Loss : 715.9492221822449\n",
      "Epoch 44 / 200 , Loss : 710.3797916142859\n",
      "Epoch 45 / 200 , Loss : 704.7447208780643\n",
      "Epoch 46 / 200 , Loss : 699.4254000112818\n",
      "Epoch 47 / 200 , Loss : 694.244855160223\n",
      "Epoch 48 / 200 , Loss : 689.1448961448167\n",
      "Epoch 49 / 200 , Loss : 684.0401026616918\n",
      "Epoch 50 / 200 , Loss : 679.2307246666924\n",
      "Epoch 51 / 200 , Loss : 674.5399517259838\n",
      "Epoch 52 / 200 , Loss : 669.9782798458259\n",
      "Epoch 53 / 200 , Loss : 665.4909564509941\n",
      "Epoch 54 / 200 , Loss : 661.0713796302529\n",
      "Epoch 55 / 200 , Loss : 656.8920955281101\n",
      "Epoch 56 / 200 , Loss : 652.5807274140061\n",
      "Epoch 57 / 200 , Loss : 648.2644259338362\n",
      "Epoch 58 / 200 , Loss : 644.3429080231643\n",
      "Epoch 59 / 200 , Loss : 640.5821705723525\n",
      "Epoch 60 / 200 , Loss : 636.3528495272811\n",
      "Epoch 61 / 200 , Loss : 632.7632686835714\n",
      "Epoch 62 / 200 , Loss : 628.9691257447148\n",
      "Epoch 63 / 200 , Loss : 625.3245425433289\n",
      "Epoch 64 / 200 , Loss : 621.6623890664716\n",
      "Epoch 65 / 200 , Loss : 618.0752443504604\n",
      "Epoch 66 / 200 , Loss : 614.7009730517663\n",
      "Epoch 67 / 200 , Loss : 611.0875269848832\n",
      "Epoch 68 / 200 , Loss : 607.7984917713844\n",
      "Epoch 69 / 200 , Loss : 604.5335550622972\n",
      "Epoch 70 / 200 , Loss : 601.1679750281244\n",
      "Epoch 71 / 200 , Loss : 597.9338179362973\n",
      "Epoch 72 / 200 , Loss : 594.8588734595271\n",
      "Epoch 73 / 200 , Loss : 591.6687161273875\n",
      "Epoch 74 / 200 , Loss : 588.6805670277912\n",
      "Epoch 75 / 200 , Loss : 585.6278278315344\n",
      "Epoch 76 / 200 , Loss : 582.5934124674667\n",
      "Epoch 77 / 200 , Loss : 579.5930977297194\n",
      "Epoch 78 / 200 , Loss : 576.8675216088825\n",
      "Epoch 79 / 200 , Loss : 574.0212373478624\n",
      "Epoch 80 / 200 , Loss : 571.0018048976597\n",
      "Epoch 81 / 200 , Loss : 568.3897146345563\n",
      "Epoch 82 / 200 , Loss : 565.5136903305527\n",
      "Epoch 83 / 200 , Loss : 562.840209927158\n",
      "Epoch 84 / 200 , Loss : 560.0663125298356\n",
      "Epoch 85 / 200 , Loss : 557.7745412072517\n",
      "Epoch 86 / 200 , Loss : 555.1511839917588\n",
      "Epoch 87 / 200 , Loss : 552.4581720706592\n",
      "Epoch 88 / 200 , Loss : 549.895482408045\n",
      "Epoch 89 / 200 , Loss : 547.3916103211284\n",
      "Epoch 90 / 200 , Loss : 545.0590601740798\n",
      "Epoch 91 / 200 , Loss : 542.3971315242144\n",
      "Epoch 92 / 200 , Loss : 540.012918510465\n",
      "Epoch 93 / 200 , Loss : 537.7691107701341\n",
      "Epoch 94 / 200 , Loss : 535.3174897229418\n",
      "Epoch 95 / 200 , Loss : 532.8578759585324\n",
      "Epoch 96 / 200 , Loss : 530.6633919510222\n",
      "Epoch 97 / 200 , Loss : 528.3177973561003\n",
      "Epoch 98 / 200 , Loss : 526.262342679302\n",
      "Epoch 99 / 200 , Loss : 523.8614461921301\n",
      "Epoch 100 / 200 , Loss : 521.7184189921724\n",
      "Epoch 101 / 200 , Loss : 519.5090981861013\n",
      "Epoch 102 / 200 , Loss : 517.3571745237934\n",
      "Epoch 103 / 200 , Loss : 515.1399027386711\n",
      "Epoch 104 / 200 , Loss : 513.08078426901\n",
      "Epoch 105 / 200 , Loss : 511.0380143450083\n",
      "Epoch 106 / 200 , Loss : 508.9345099357926\n",
      "Epoch 107 / 200 , Loss : 506.8398665718256\n",
      "Epoch 108 / 200 , Loss : 504.8053412346365\n",
      "Epoch 109 / 200 , Loss : 502.74713610642357\n",
      "Epoch 110 / 200 , Loss : 500.82685222698234\n",
      "Epoch 111 / 200 , Loss : 498.8338728150906\n",
      "Epoch 112 / 200 , Loss : 496.8439754978169\n",
      "Epoch 113 / 200 , Loss : 494.9875850109902\n",
      "Epoch 114 / 200 , Loss : 492.9878044583373\n",
      "Epoch 115 / 200 , Loss : 490.84130905490207\n",
      "Epoch 116 / 200 , Loss : 489.19578018502534\n",
      "Epoch 117 / 200 , Loss : 487.1240793988782\n",
      "Epoch 118 / 200 , Loss : 485.26136547530405\n",
      "Epoch 119 / 200 , Loss : 483.4327341030758\n",
      "Epoch 120 / 200 , Loss : 481.77307600561244\n",
      "Epoch 121 / 200 , Loss : 479.8958267056268\n",
      "Epoch 122 / 200 , Loss : 477.9609098885541\n",
      "Epoch 123 / 200 , Loss : 476.3314838205602\n",
      "Epoch 124 / 200 , Loss : 474.4695490403006\n",
      "Epoch 125 / 200 , Loss : 472.67142458029383\n",
      "Epoch 126 / 200 , Loss : 471.01095287896266\n",
      "Epoch 127 / 200 , Loss : 469.22923767870157\n",
      "Epoch 128 / 200 , Loss : 467.5280824498127\n",
      "Epoch 129 / 200 , Loss : 465.74287172969673\n",
      "Epoch 130 / 200 , Loss : 464.0594934670143\n",
      "Epoch 131 / 200 , Loss : 462.4905035920005\n",
      "Epoch 132 / 200 , Loss : 460.7242868374491\n",
      "Epoch 133 / 200 , Loss : 459.2051945193303\n",
      "Epoch 134 / 200 , Loss : 457.52287892590016\n",
      "Epoch 135 / 200 , Loss : 455.8167116976625\n",
      "Epoch 136 / 200 , Loss : 454.4262804567356\n",
      "Epoch 137 / 200 , Loss : 452.68707312810164\n",
      "Epoch 138 / 200 , Loss : 451.12612806413335\n",
      "Epoch 139 / 200 , Loss : 449.5563610068693\n",
      "Epoch 140 / 200 , Loss : 448.00607383211894\n",
      "Epoch 141 / 200 , Loss : 446.48460071394334\n",
      "Epoch 142 / 200 , Loss : 444.94703029897977\n",
      "Epoch 143 / 200 , Loss : 443.3310147674926\n",
      "Epoch 144 / 200 , Loss : 441.8437591367692\n",
      "Epoch 145 / 200 , Loss : 440.2782569508814\n",
      "Epoch 146 / 200 , Loss : 438.8079918450096\n",
      "Epoch 147 / 200 , Loss : 437.43830085044704\n",
      "Epoch 148 / 200 , Loss : 435.89400346425236\n",
      "Epoch 149 / 200 , Loss : 434.3172350676419\n",
      "Epoch 150 / 200 , Loss : 433.0395796587413\n",
      "Epoch 151 / 200 , Loss : 431.60675927855056\n",
      "Epoch 152 / 200 , Loss : 430.263074675987\n",
      "Epoch 153 / 200 , Loss : 428.71321330451747\n",
      "Epoch 154 / 200 , Loss : 427.2844344868955\n",
      "Epoch 155 / 200 , Loss : 425.9231829169487\n",
      "Epoch 156 / 200 , Loss : 424.5800855996501\n",
      "Epoch 157 / 200 , Loss : 423.1562374443004\n",
      "Epoch 158 / 200 , Loss : 421.85278403985205\n",
      "Epoch 159 / 200 , Loss : 420.4099011467389\n",
      "Epoch 160 / 200 , Loss : 419.05908214637725\n",
      "Epoch 161 / 200 , Loss : 417.6402186982121\n",
      "Epoch 162 / 200 , Loss : 416.3504498600143\n",
      "Epoch 163 / 200 , Loss : 415.1141611218928\n",
      "Epoch 164 / 200 , Loss : 413.6981595232535\n",
      "Epoch 165 / 200 , Loss : 412.3974940804538\n",
      "Epoch 166 / 200 , Loss : 411.1451214427254\n",
      "Epoch 167 / 200 , Loss : 409.87020149951786\n",
      "Epoch 168 / 200 , Loss : 408.6224332836247\n",
      "Epoch 169 / 200 , Loss : 407.3007925509443\n",
      "Epoch 170 / 200 , Loss : 406.01818006951885\n",
      "Epoch 171 / 200 , Loss : 404.6819640285608\n",
      "Epoch 172 / 200 , Loss : 403.6021189073282\n",
      "Epoch 173 / 200 , Loss : 402.24462825252795\n",
      "Epoch 174 / 200 , Loss : 400.91220312367835\n",
      "Epoch 175 / 200 , Loss : 399.8068904652485\n",
      "Epoch 176 / 200 , Loss : 398.65612477048103\n",
      "Epoch 177 / 200 , Loss : 397.43371813151475\n",
      "Epoch 178 / 200 , Loss : 396.1885517407554\n",
      "Epoch 179 / 200 , Loss : 394.8251924836619\n",
      "Epoch 180 / 200 , Loss : 393.8548804923367\n",
      "Epoch 181 / 200 , Loss : 392.60117886906795\n",
      "Epoch 182 / 200 , Loss : 391.54053217152904\n",
      "Epoch 183 / 200 , Loss : 390.22821022174384\n",
      "Epoch 184 / 200 , Loss : 389.0944560419079\n",
      "Epoch 185 / 200 , Loss : 387.99567420599163\n",
      "Epoch 186 / 200 , Loss : 386.96883596572593\n",
      "Epoch 187 / 200 , Loss : 385.56233828728244\n",
      "Epoch 188 / 200 , Loss : 384.5047606423522\n",
      "Epoch 189 / 200 , Loss : 383.4577467972795\n",
      "Epoch 190 / 200 , Loss : 382.30230093536596\n",
      "Epoch 191 / 200 , Loss : 381.0282691150273\n",
      "Epoch 192 / 200 , Loss : 380.0871526833852\n",
      "Epoch 193 / 200 , Loss : 379.0054605080137\n",
      "Epoch 194 / 200 , Loss : 377.8860873781614\n",
      "Epoch 195 / 200 , Loss : 376.77416643988937\n",
      "Epoch 196 / 200 , Loss : 375.6559913123682\n",
      "Epoch 197 / 200 , Loss : 374.57203719615785\n",
      "Epoch 198 / 200 , Loss : 373.43146840268963\n",
      "Epoch 199 / 200 , Loss : 372.4376666348566\n",
      "Epoch 200 / 200 , Loss : 371.4295780367737\n",
      "\n",
      "Accuracy:\n",
      "Train Accuracy: 86.73%\n",
      "Test Accuracy: 75.50%\n",
      "\n",
      "Per-Class Metrics (Test):\n",
      " Class     F1  Precision  Recall\n",
      "     1 0.8221     0.8277  0.8167\n",
      "     2 0.6851     0.6677  0.7033\n",
      "     3 0.7871     0.8049  0.7700\n",
      "     4 0.6688     0.6550  0.6833\n",
      "     5 0.7255     0.7243  0.7267\n",
      "     6 0.7756     0.7680  0.7833\n",
      "     7 0.6966     0.7214  0.6733\n",
      "     8 0.8163     0.7968  0.8367\n",
      "     9 0.8803     0.8908  0.8700\n",
      "    10 0.8411     0.8355  0.8467\n",
      "    11 0.9151     0.9136  0.9167\n",
      "    12 0.8467     0.8467  0.8467\n",
      "    13 0.7496     0.7666  0.7333\n",
      "    14 0.8062     0.7536  0.8667\n",
      "    15 0.7902     0.7714  0.8100\n",
      "    16 0.8038     0.7706  0.8400\n",
      "    17 0.6644     0.6689  0.6600\n",
      "    18 0.7464     0.8038  0.6967\n",
      "    19 0.7038     0.6913  0.7167\n",
      "    20 0.7010     0.7234  0.6800\n",
      "    21 0.7273     0.7213  0.7333\n",
      "    22 0.8660     0.8494  0.8833\n",
      "    23 0.6839     0.7097  0.6600\n",
      "    24 0.7552     0.7821  0.7300\n",
      "    25 0.7155     0.7143  0.7167\n",
      "    26 0.5811     0.5890  0.5733\n",
      "    27 0.8286     0.8272  0.8300\n",
      "    28 0.8316     0.8582  0.8067\n",
      "    29 0.6387     0.6441  0.6333\n",
      "    30 0.7492     0.7517  0.7467\n",
      "    31 0.7649     0.7219  0.8133\n",
      "    32 0.6169     0.6276  0.6067\n",
      "    33 0.7182     0.7411  0.6967\n",
      "    34 0.7480     0.7302  0.7667\n",
      "    35 0.7533     0.7533  0.7533\n",
      "    36 0.7597     0.7661  0.7533\n",
      "\n",
      "Average (Macro) Metrics (Test):\n",
      "F1: 0.7545, Precision: 0.7553, Recall: 0.7550\n",
      "---------------------------------------------------\n",
      "\n",
      "Per-Class Metrics (Train):\n",
      " Class     F1  Precision  Recall\n",
      "     1 0.9075     0.9240  0.8917\n",
      "     2 0.8200     0.8088  0.8317\n",
      "     3 0.8708     0.8767  0.8650\n",
      "     4 0.8184     0.8071  0.8300\n",
      "     5 0.8356     0.8412  0.8300\n",
      "     6 0.8742     0.8574  0.8917\n",
      "     7 0.8569     0.8656  0.8483\n",
      "     8 0.8941     0.8948  0.8933\n",
      "     9 0.9415     0.9446  0.9383\n",
      "    10 0.8904     0.9010  0.8800\n",
      "    11 0.9423     0.9316  0.9533\n",
      "    12 0.9172     0.9210  0.9133\n",
      "    13 0.8540     0.8650  0.8433\n",
      "    14 0.9053     0.8943  0.9167\n",
      "    15 0.8835     0.8585  0.9100\n",
      "    16 0.9064     0.9012  0.9117\n",
      "    17 0.7980     0.7896  0.8067\n",
      "    18 0.8705     0.8902  0.8517\n",
      "    19 0.8232     0.8316  0.8150\n",
      "    20 0.8731     0.8746  0.8717\n",
      "    21 0.8576     0.8472  0.8683\n",
      "    22 0.9323     0.9232  0.9417\n",
      "    23 0.8176     0.8492  0.7883\n",
      "    24 0.8733     0.8733  0.8733\n",
      "    25 0.8461     0.8540  0.8383\n",
      "    26 0.7721     0.7793  0.7650\n",
      "    27 0.9181     0.9211  0.9150\n",
      "    28 0.9092     0.9085  0.9100\n",
      "    29 0.8156     0.8129  0.8183\n",
      "    30 0.8622     0.8643  0.8600\n",
      "    31 0.8801     0.8565  0.9050\n",
      "    32 0.7633     0.7717  0.7550\n",
      "    33 0.8689     0.8942  0.8450\n",
      "    34 0.8946     0.8977  0.8917\n",
      "    35 0.8348     0.8233  0.8467\n",
      "    36 0.8889     0.8718  0.9067\n",
      "\n",
      "Average (Macro) Metrics (Train):\n",
      "F1: 0.8672, Precision: 0.8674, Recall: 0.8673\n",
      "===================================================\n",
      "\n",
      "=== Hidden layer size: 100 ===\n",
      "Epoch 1 / 200 , Loss : 2268.850944302213\n",
      "Epoch 2 / 200 , Loss : 1956.373110747512\n",
      "Epoch 3 / 200 , Loss : 1719.9618521101931\n",
      "Epoch 4 / 200 , Loss : 1538.2775769560178\n",
      "Epoch 5 / 200 , Loss : 1399.3531123946939\n",
      "Epoch 6 / 200 , Loss : 1292.389499499993\n",
      "Epoch 7 / 200 , Loss : 1209.400606602056\n",
      "Epoch 8 / 200 , Loss : 1143.2887536854275\n",
      "Epoch 9 / 200 , Loss : 1089.788973442624\n",
      "Epoch 10 / 200 , Loss : 1045.4186160931931\n",
      "Epoch 11 / 200 , Loss : 1008.1520060018465\n",
      "Epoch 12 / 200 , Loss : 975.9961880125753\n",
      "Epoch 13 / 200 , Loss : 948.2932735765004\n",
      "Epoch 14 / 200 , Loss : 923.4961126739893\n",
      "Epoch 15 / 200 , Loss : 901.6635814276677\n",
      "Epoch 16 / 200 , Loss : 882.0970467466774\n",
      "Epoch 17 / 200 , Loss : 864.184277755579\n",
      "Epoch 18 / 200 , Loss : 847.7537038450239\n",
      "Epoch 19 / 200 , Loss : 832.5159863293331\n",
      "Epoch 20 / 200 , Loss : 818.599374588485\n",
      "Epoch 21 / 200 , Loss : 805.2089887645179\n",
      "Epoch 22 / 200 , Loss : 793.1236032672193\n",
      "Epoch 23 / 200 , Loss : 781.7083460132883\n",
      "Epoch 24 / 200 , Loss : 770.7134510084245\n",
      "Epoch 25 / 200 , Loss : 760.1072726463367\n",
      "Epoch 26 / 200 , Loss : 750.2235335910759\n",
      "Epoch 27 / 200 , Loss : 740.5319613268605\n",
      "Epoch 28 / 200 , Loss : 731.7048760174789\n",
      "Epoch 29 / 200 , Loss : 722.731975071301\n",
      "Epoch 30 / 200 , Loss : 714.6327589267239\n",
      "Epoch 31 / 200 , Loss : 706.3368522975646\n",
      "Epoch 32 / 200 , Loss : 698.4923885223084\n",
      "Epoch 33 / 200 , Loss : 691.1114144973067\n",
      "Epoch 34 / 200 , Loss : 683.6920735265636\n",
      "Epoch 35 / 200 , Loss : 676.4514482780534\n",
      "Epoch 36 / 200 , Loss : 669.4084053650224\n",
      "Epoch 37 / 200 , Loss : 662.9073181276407\n",
      "Epoch 38 / 200 , Loss : 656.2965237385783\n",
      "Epoch 39 / 200 , Loss : 649.9859040052628\n",
      "Epoch 40 / 200 , Loss : 643.4996232750455\n",
      "Epoch 41 / 200 , Loss : 637.6008828237151\n",
      "Epoch 42 / 200 , Loss : 631.7103352836269\n",
      "Epoch 43 / 200 , Loss : 625.9112710313705\n",
      "Epoch 44 / 200 , Loss : 620.4015205261958\n",
      "Epoch 45 / 200 , Loss : 614.6717588010424\n",
      "Epoch 46 / 200 , Loss : 609.2453491933811\n",
      "Epoch 47 / 200 , Loss : 604.0189445809132\n",
      "Epoch 48 / 200 , Loss : 598.6428080417969\n",
      "Epoch 49 / 200 , Loss : 593.4292004747633\n",
      "Epoch 50 / 200 , Loss : 588.5495066301419\n",
      "Epoch 51 / 200 , Loss : 583.5398290604014\n",
      "Epoch 52 / 200 , Loss : 578.7603167365871\n",
      "Epoch 53 / 200 , Loss : 573.9130218688518\n",
      "Epoch 54 / 200 , Loss : 569.3124300099406\n",
      "Epoch 55 / 200 , Loss : 564.4581585868641\n",
      "Epoch 56 / 200 , Loss : 560.0673751054333\n",
      "Epoch 57 / 200 , Loss : 555.6920432852587\n",
      "Epoch 58 / 200 , Loss : 551.2210407034977\n",
      "Epoch 59 / 200 , Loss : 546.8619348201131\n",
      "Epoch 60 / 200 , Loss : 542.6987228248295\n",
      "Epoch 61 / 200 , Loss : 538.4242444750092\n",
      "Epoch 62 / 200 , Loss : 534.0579625537171\n",
      "Epoch 63 / 200 , Loss : 530.2637406408708\n",
      "Epoch 64 / 200 , Loss : 526.0963831792949\n",
      "Epoch 65 / 200 , Loss : 522.187113013286\n",
      "Epoch 66 / 200 , Loss : 518.0973819403418\n",
      "Epoch 67 / 200 , Loss : 514.213228986612\n",
      "Epoch 68 / 200 , Loss : 510.4861070871302\n",
      "Epoch 69 / 200 , Loss : 506.79669571401126\n",
      "Epoch 70 / 200 , Loss : 503.0382810313631\n",
      "Epoch 71 / 200 , Loss : 499.2073055196084\n",
      "Epoch 72 / 200 , Loss : 495.6097703210261\n",
      "Epoch 73 / 200 , Loss : 492.1396462439544\n",
      "Epoch 74 / 200 , Loss : 488.5291501151465\n",
      "Epoch 75 / 200 , Loss : 484.9856870155808\n",
      "Epoch 76 / 200 , Loss : 481.51537322443335\n",
      "Epoch 77 / 200 , Loss : 478.20519878936244\n",
      "Epoch 78 / 200 , Loss : 474.63428205660256\n",
      "Epoch 79 / 200 , Loss : 471.3049260324656\n",
      "Epoch 80 / 200 , Loss : 468.0839178459753\n",
      "Epoch 81 / 200 , Loss : 464.8393519604591\n",
      "Epoch 82 / 200 , Loss : 461.63532596353946\n",
      "Epoch 83 / 200 , Loss : 458.33420444259633\n",
      "Epoch 84 / 200 , Loss : 455.22247729723716\n",
      "Epoch 85 / 200 , Loss : 452.08730073508707\n",
      "Epoch 86 / 200 , Loss : 449.0637842146094\n",
      "Epoch 87 / 200 , Loss : 445.84009812747234\n",
      "Epoch 88 / 200 , Loss : 442.83412301079403\n",
      "Epoch 89 / 200 , Loss : 439.89474566904397\n",
      "Epoch 90 / 200 , Loss : 436.9399427597027\n",
      "Epoch 91 / 200 , Loss : 433.74618758941335\n",
      "Epoch 92 / 200 , Loss : 431.09857895182756\n",
      "Epoch 93 / 200 , Loss : 428.16296200773894\n",
      "Epoch 94 / 200 , Loss : 425.57683017443196\n",
      "Epoch 95 / 200 , Loss : 422.6563256610062\n",
      "Epoch 96 / 200 , Loss : 419.9909771862714\n",
      "Epoch 97 / 200 , Loss : 416.9745056356011\n",
      "Epoch 98 / 200 , Loss : 414.3244054441778\n",
      "Epoch 99 / 200 , Loss : 411.66044190655333\n",
      "Epoch 100 / 200 , Loss : 408.915024473879\n",
      "Epoch 101 / 200 , Loss : 406.27118024564106\n",
      "Epoch 102 / 200 , Loss : 403.7469511912872\n",
      "Epoch 103 / 200 , Loss : 401.06379412438366\n",
      "Epoch 104 / 200 , Loss : 398.611738235893\n",
      "Epoch 105 / 200 , Loss : 395.898333470765\n",
      "Epoch 106 / 200 , Loss : 393.48815427188254\n",
      "Epoch 107 / 200 , Loss : 391.05996017539104\n",
      "Epoch 108 / 200 , Loss : 388.4532665500463\n",
      "Epoch 109 / 200 , Loss : 386.1191430804445\n",
      "Epoch 110 / 200 , Loss : 383.6393217945895\n",
      "Epoch 111 / 200 , Loss : 381.3994192572459\n",
      "Epoch 112 / 200 , Loss : 378.8074222313184\n",
      "Epoch 113 / 200 , Loss : 376.63735805631273\n",
      "Epoch 114 / 200 , Loss : 374.284972827479\n",
      "Epoch 115 / 200 , Loss : 371.89345726957305\n",
      "Epoch 116 / 200 , Loss : 369.69058428245745\n",
      "Epoch 117 / 200 , Loss : 367.5033562313869\n",
      "Epoch 118 / 200 , Loss : 365.2292796314544\n",
      "Epoch 119 / 200 , Loss : 362.9701763716686\n",
      "Epoch 120 / 200 , Loss : 360.78327588989964\n",
      "Epoch 121 / 200 , Loss : 358.6982021327683\n",
      "Epoch 122 / 200 , Loss : 356.39629610540214\n",
      "Epoch 123 / 200 , Loss : 354.3240094203657\n",
      "Epoch 124 / 200 , Loss : 352.24554177751463\n",
      "Epoch 125 / 200 , Loss : 350.14052801659807\n",
      "Epoch 126 / 200 , Loss : 348.08133641855335\n",
      "Epoch 127 / 200 , Loss : 345.8940808003286\n",
      "Epoch 128 / 200 , Loss : 343.97570359489845\n",
      "Epoch 129 / 200 , Loss : 341.9072719176935\n",
      "Epoch 130 / 200 , Loss : 339.82945222850697\n",
      "Epoch 131 / 200 , Loss : 337.914571731809\n",
      "Epoch 132 / 200 , Loss : 335.9292640718351\n",
      "Epoch 133 / 200 , Loss : 334.120166906025\n",
      "Epoch 134 / 200 , Loss : 331.98868470437196\n",
      "Epoch 135 / 200 , Loss : 330.044015914462\n",
      "Epoch 136 / 200 , Loss : 328.30025748434497\n",
      "Epoch 137 / 200 , Loss : 326.3642604015316\n",
      "Epoch 138 / 200 , Loss : 324.47785974918656\n",
      "Epoch 139 / 200 , Loss : 322.6336021208475\n",
      "Epoch 140 / 200 , Loss : 320.74273904834956\n",
      "Epoch 141 / 200 , Loss : 319.0356892317708\n",
      "Epoch 142 / 200 , Loss : 317.15709421877114\n",
      "Epoch 143 / 200 , Loss : 315.3788830529232\n",
      "Epoch 144 / 200 , Loss : 313.6377816859249\n",
      "Epoch 145 / 200 , Loss : 311.731285538225\n",
      "Epoch 146 / 200 , Loss : 310.2105503907397\n",
      "Epoch 147 / 200 , Loss : 308.37172319658043\n",
      "Epoch 148 / 200 , Loss : 306.69044701060045\n",
      "Epoch 149 / 200 , Loss : 304.9894146917987\n",
      "Epoch 150 / 200 , Loss : 303.3412577022906\n",
      "Epoch 151 / 200 , Loss : 301.5656208442636\n",
      "Epoch 152 / 200 , Loss : 299.968428276001\n",
      "Epoch 153 / 200 , Loss : 298.14883388239025\n",
      "Epoch 154 / 200 , Loss : 296.72447719949116\n",
      "Epoch 155 / 200 , Loss : 295.1280124835469\n",
      "Epoch 156 / 200 , Loss : 293.4849720770425\n",
      "Epoch 157 / 200 , Loss : 291.82471760744977\n",
      "Epoch 158 / 200 , Loss : 290.2187429872962\n",
      "Epoch 159 / 200 , Loss : 288.63770031970887\n",
      "Epoch 160 / 200 , Loss : 287.12440674830214\n",
      "Epoch 161 / 200 , Loss : 285.7149663179898\n",
      "Epoch 162 / 200 , Loss : 284.0428452598994\n",
      "Epoch 163 / 200 , Loss : 282.63690612858096\n",
      "Epoch 164 / 200 , Loss : 281.0657523643933\n",
      "Epoch 165 / 200 , Loss : 279.55789762121674\n",
      "Epoch 166 / 200 , Loss : 278.0920292737309\n",
      "Epoch 167 / 200 , Loss : 276.6747448838053\n",
      "Epoch 168 / 200 , Loss : 275.1472459487037\n",
      "Epoch 169 / 200 , Loss : 273.74549603205537\n",
      "Epoch 170 / 200 , Loss : 272.3333390969303\n",
      "Epoch 171 / 200 , Loss : 270.8295844133877\n",
      "Epoch 172 / 200 , Loss : 269.55095745800253\n",
      "Epoch 173 / 200 , Loss : 267.9499220560849\n",
      "Epoch 174 / 200 , Loss : 266.784827325389\n",
      "Epoch 175 / 200 , Loss : 265.2025710292617\n",
      "Epoch 176 / 200 , Loss : 263.8017909152438\n",
      "Epoch 177 / 200 , Loss : 262.6402262161152\n",
      "Epoch 178 / 200 , Loss : 261.2082226629595\n",
      "Epoch 179 / 200 , Loss : 259.8931896315839\n",
      "Epoch 180 / 200 , Loss : 258.42628768537577\n",
      "Epoch 181 / 200 , Loss : 257.2801254445284\n",
      "Epoch 182 / 200 , Loss : 255.87691507521072\n",
      "Epoch 183 / 200 , Loss : 254.51879245745675\n",
      "Epoch 184 / 200 , Loss : 253.226900345624\n",
      "Epoch 185 / 200 , Loss : 251.98043858185488\n",
      "Epoch 186 / 200 , Loss : 250.67083454409214\n",
      "Epoch 187 / 200 , Loss : 249.56257504595234\n",
      "Epoch 188 / 200 , Loss : 248.11359615275308\n",
      "Epoch 189 / 200 , Loss : 246.7955161937917\n",
      "Epoch 190 / 200 , Loss : 245.6848280273327\n",
      "Epoch 191 / 200 , Loss : 244.38521095050606\n",
      "Epoch 192 / 200 , Loss : 243.2050705157407\n",
      "Epoch 193 / 200 , Loss : 241.97661174748924\n",
      "Epoch 194 / 200 , Loss : 240.82385735413965\n",
      "Epoch 195 / 200 , Loss : 239.62991872846058\n",
      "Epoch 196 / 200 , Loss : 238.3614232954254\n",
      "Epoch 197 / 200 , Loss : 237.28777822690773\n",
      "Epoch 198 / 200 , Loss : 236.0827579051171\n",
      "Epoch 199 / 200 , Loss : 234.9224154304925\n",
      "Epoch 200 / 200 , Loss : 233.7136099828943\n",
      "\n",
      "Accuracy:\n",
      "Train Accuracy: 92.93%\n",
      "Test Accuracy: 81.36%\n",
      "\n",
      "Per-Class Metrics (Test):\n",
      " Class     F1  Precision  Recall\n",
      "     1 0.8893     0.8820  0.8967\n",
      "     2 0.7174     0.7115  0.7233\n",
      "     3 0.8112     0.8529  0.7733\n",
      "     4 0.7480     0.7302  0.7667\n",
      "     5 0.7812     0.8152  0.7500\n",
      "     6 0.8255     0.8311  0.8200\n",
      "     7 0.7805     0.8175  0.7467\n",
      "     8 0.8339     0.8153  0.8533\n",
      "     9 0.8919     0.9041  0.8800\n",
      "    10 0.8767     0.9014  0.8533\n",
      "    11 0.9264     0.9295  0.9233\n",
      "    12 0.8930     0.9100  0.8767\n",
      "    13 0.7888     0.7810  0.7967\n",
      "    14 0.8640     0.8308  0.9000\n",
      "    15 0.8355     0.8247  0.8467\n",
      "    16 0.9034     0.8875  0.9200\n",
      "    17 0.7379     0.7391  0.7367\n",
      "    18 0.7864     0.8000  0.7733\n",
      "    19 0.7440     0.7196  0.7700\n",
      "    20 0.7614     0.7732  0.7500\n",
      "    21 0.8134     0.7798  0.8500\n",
      "    22 0.9016     0.8871  0.9167\n",
      "    23 0.7579     0.7592  0.7567\n",
      "    24 0.8062     0.8304  0.7833\n",
      "    25 0.7844     0.7993  0.7700\n",
      "    26 0.7267     0.7267  0.7267\n",
      "    27 0.8832     0.8969  0.8700\n",
      "    28 0.8816     0.9081  0.8567\n",
      "    29 0.7645     0.7832  0.7467\n",
      "    30 0.8084     0.7880  0.8300\n",
      "    31 0.8208     0.7768  0.8700\n",
      "    32 0.6940     0.7443  0.6500\n",
      "    33 0.7778     0.7424  0.8167\n",
      "    34 0.8271     0.8025  0.8533\n",
      "    35 0.8040     0.8013  0.8067\n",
      "    36 0.8356     0.8412  0.8300\n",
      "\n",
      "Average (Macro) Metrics (Test):\n",
      "F1: 0.8134, Precision: 0.8146, Recall: 0.8136\n",
      "---------------------------------------------------\n",
      "\n",
      "Per-Class Metrics (Train):\n",
      " Class     F1  Precision  Recall\n",
      "     1 0.9502     0.9455  0.9550\n",
      "     2 0.9065     0.8998  0.9133\n",
      "     3 0.9266     0.9493  0.9050\n",
      "     4 0.8957     0.8965  0.8950\n",
      "     5 0.8923     0.9234  0.8633\n",
      "     6 0.9226     0.9121  0.9333\n",
      "     7 0.9347     0.9516  0.9183\n",
      "     8 0.9389     0.9297  0.9483\n",
      "     9 0.9716     0.9748  0.9683\n",
      "    10 0.9413     0.9476  0.9350\n",
      "    11 0.9624     0.9648  0.9600\n",
      "    12 0.9410     0.9386  0.9433\n",
      "    13 0.9019     0.8923  0.9117\n",
      "    14 0.9529     0.9458  0.9600\n",
      "    15 0.9395     0.9341  0.9450\n",
      "    16 0.9617     0.9617  0.9617\n",
      "    17 0.8953     0.8924  0.8983\n",
      "    18 0.9239     0.9381  0.9100\n",
      "    19 0.8979     0.8942  0.9017\n",
      "    20 0.9348     0.9501  0.9200\n",
      "    21 0.9237     0.8915  0.9583\n",
      "    22 0.9638     0.9528  0.9750\n",
      "    23 0.9067     0.9067  0.9067\n",
      "    24 0.9425     0.9432  0.9417\n",
      "    25 0.9147     0.9371  0.8933\n",
      "    26 0.8872     0.8828  0.8917\n",
      "    27 0.9593     0.9569  0.9617\n",
      "    28 0.9625     0.9617  0.9633\n",
      "    29 0.9086     0.9056  0.9117\n",
      "    30 0.9336     0.9192  0.9483\n",
      "    31 0.9299     0.9201  0.9400\n",
      "    32 0.8960     0.9091  0.8833\n",
      "    33 0.9350     0.9350  0.9350\n",
      "    34 0.9386     0.9340  0.9433\n",
      "    35 0.9146     0.9280  0.9017\n",
      "    36 0.9430     0.9345  0.9517\n",
      "\n",
      "Average (Macro) Metrics (Train):\n",
      "F1: 0.9292, Precision: 0.9295, Recall: 0.9293\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "hidden_layer_length = [1, 5, 10, 50, 100]\n",
    "\n",
    "for h_len in hidden_layer_length:\n",
    "    print(f\"\\n=== Hidden layer size: {h_len} ===\")\n",
    "\n",
    "    model = NeuralNetwork(M=32, n=3072, HiddenLayer=[h_len], target_class=36, lr=0.01)\n",
    "    model.fit(X_train, y_train, epochs=200)\n",
    "\n",
    "    y_test_prediction = model.predict(X_test)\n",
    "    y_train_prediction = model.predict(X_train)\n",
    "\n",
    "    print(\"\\nAccuracy:\")\n",
    "    print(f\"Train Accuracy: {np.mean(y_train_prediction == np.argmax(y_train, axis=1)) * 100:.2f}%\")\n",
    "    print(f\"Test Accuracy: {np.mean(y_test_prediction == np.argmax(y_test, axis=1)) * 100:.2f}%\")\n",
    "\n",
    "    # ---------- METRIC FUNCTION ----------\n",
    "    def results_metrics(y_true, y_pred):\n",
    "        y_true_labels = np.argmax(y_true, axis=1)\n",
    "        f1 = f1_score(y_true_labels, y_pred, average=None)\n",
    "        precision = precision_score(y_true_labels, y_pred, average=None, zero_division=0)\n",
    "        recall = recall_score(y_true_labels, y_pred, average=None, zero_division=0)\n",
    "        return f1, precision, recall\n",
    "\n",
    "    # ---------- TEST SET ----------\n",
    "    f1_test, precision_test, recall_test = results_metrics(y_test, y_test_prediction)\n",
    "    results_test = pd.DataFrame({\n",
    "        \"Class\": np.arange(1, len(f1_test) + 1),\n",
    "        \"F1\": f1_test,\n",
    "        \"Precision\": precision_test,\n",
    "        \"Recall\": recall_test\n",
    "    })\n",
    "\n",
    "    print(\"\\nPer-Class Metrics (Test):\")\n",
    "    print(results_test.round(4).to_string(index=False))\n",
    "\n",
    "    print(\"\\nAverage (Macro) Metrics (Test):\")\n",
    "    print(f\"F1: {np.mean(f1_test):.4f}, Precision: {np.mean(precision_test):.4f}, Recall: {np.mean(recall_test):.4f}\")\n",
    "\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "    # ---------- TRAIN SET ----------\n",
    "    f1_train, precision_train, recall_train = results_metrics(y_train, y_train_prediction)\n",
    "    results_train = pd.DataFrame({\n",
    "        \"Class\": np.arange(1, len(f1_train) + 1),\n",
    "        \"F1\": f1_train,\n",
    "        \"Precision\": precision_train,\n",
    "        \"Recall\": recall_train\n",
    "    })\n",
    "\n",
    "    print(\"\\nPer-Class Metrics (Train):\")\n",
    "    print(results_train.round(4).to_string(index=False))\n",
    "\n",
    "    print(\"\\nAverage (Macro) Metrics (Train):\")\n",
    "    print(f\"F1: {np.mean(f1_train):.4f}, Precision: {np.mean(precision_train):.4f}, Recall: {np.mean(recall_train):.4f}\")\n",
    "\n",
    "    print(\"===================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dbdc53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Average_Test_F1 = [0.9087,0.9164,0.8975,0.8831]\n",
    "Average_Train_F1 = [1,1,0.9940,0.9792]\n",
    "hidden_layer = [1,2,3,4]\n",
    "plt.plot(hidden_layer , Average_Train_F1 , label = \"Train F1 Score\")\n",
    "plt.plot(hidden_layer , Average_Test_F1 , label = \"Test F1 Score\")\n",
    "plt.xlabel(\"Hidden Layer (Depth)\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd4905f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
